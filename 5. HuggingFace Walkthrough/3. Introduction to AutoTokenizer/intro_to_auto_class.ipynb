{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08d6b8c-8884-42da-8bd1-4a627fc860a5",
   "metadata": {},
   "source": [
    "# **The Auto Class - Loading Tokenizers & Models Manually**  \n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Tokenizers\n",
    "    - What are Tokenizers?\n",
    "    - The Problem with Traditional Word-based Tokenizers\n",
    "    - The Solution - Subword Tokenization\n",
    "    - Common Tokenization Algorithms\n",
    "    - Special Tokens\n",
    "2. Introduction to Auto Classes\n",
    "    - Loading Models & Tokenizers Manually\n",
    "    - Why is it called \"auto\"?\n",
    "    - Why use Auto classes?\n",
    "    - Setting up a custom pipeline using Auto Classes\n",
    "    - Preprocessing Data using AutoTokenizer (i.e. Tokenize the input)\n",
    "    - Use the AutoModel to solve the Task (i.e. Pass the input to appropriate Model)\n",
    "3. AutoTokenizer\n",
    "    - Using .from_pretrained(\"model_name\")\n",
    "    - Configuration Files\n",
    "    - What are input_ids, attention_mask and token_type_ids?\n",
    "    - return_tensor Argument\n",
    "    - Essential Configuration & Special Token Properties\n",
    "4. Inspecting Tokenization Step by Step\n",
    "    - Step 1: Split input_text to tokens\n",
    "    - Step 2: Convert the tokens to numerical IDs\n",
    "    - Step 3: Append special tokens the model expects\n",
    "    - Step 4: Decoding input_ids back to words\n",
    "    - Example: AutoTokenizer for \"roberta-base\" model\n",
    "5. Batching, Padding and Truncation\n",
    "    - Batching\n",
    "    - Padding\n",
    "    - Truncation\n",
    "    - Fixing the Error: \"Unable to create tensor\"\n",
    "    - Most Common Practice: Using truncation and padding with max_length Argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84991c25-48c1-4e91-a47f-9666feec200f",
   "metadata": {},
   "source": [
    "## **Introduction to Tokenizers**\n",
    "\n",
    "### **What are Tokenizers?**\n",
    "At its core, a large language model (LLM) or any deep learning model understands numbers, not raw text. Tokenizers bridge this gap, converting human-readable text into a numerical format that models can process.\n",
    "\n",
    "### **The Problem with Traditional Word-based Tokenizers**\n",
    "1. **Vocabulary Size:** If we tried to make every unique word in the world a separate entry in a model's vocabulary, the vocabulary would be enormous (millions of words), making models inefficient and difficult to train.\n",
    "2. **Out-Of-Vocabulary (OOV) Words:** What happens when the model encounters a word it has never seen during training? Traditional word-based tokenizers would map this to an \"unknown\" token, losing all semantic information.\n",
    "3. **Morphology:** Words often share common roots or prefixes/suffixes (e.g., \"running,\" \"runs,\" \"ran\"). Word-level tokenization treats these as completely distinct, losing potential linguistic connections.\n",
    "\n",
    "### **The Solution - Subword Tokenization**\n",
    "Modern tokenizers, especially with Transformer models, use subword tokenization. Instead of splitting text into full words, they break it down into smaller, meaningful units (subwords) that can be combined to form words. This offers a fantastic balance:\n",
    "1. Smaller Vocabulary: Fewer unique subwords than unique words.\n",
    "2. Handles OOV: Any new word can be broken down into known subwords (e.g., \"unpredictable\" could become \"un\" + \"predict\" + \"able\").\n",
    "3. Morphological Information: Related words share common subwords.\n",
    "\n",
    "### **Common Tokenization Algorithms**\n",
    "\n",
    "Let's look at the most common subword algorithms:\n",
    "| Algorithm | Description | Example |\n",
    "| ------ | ------ | ------ |\n",
    "| **BPE (Byte Pair Encoding)** | Breaks rare words into common subwords | `huggingface ‚Üí hug + ging + face` |\n",
    "| **WordPiece** | Used in BERT; similar to BPE, adds special prefix (`##`) for subwords | `loving ‚Üí lov + ##ing` |\n",
    "| **SentencePiece** | Used in T5, ALBERT; language-agnostic, treats everything as a sequence of bytes | `‚ñÅI ‚ñÅlove ‚ñÅAI` |\n",
    "\n",
    "### **Special Tokens**\n",
    "\n",
    "Tokenizers also introduce special tokens that models use for specific purposes:\n",
    "\n",
    "1. `[CLS]` / `<s>` (Classification/Start): Often used at the beginning of a sequence. For classification tasks, the hidden state corresponding to this token is often used as the aggregated representation of the entire sequence.\n",
    "2. `[SEP]` / `</s>` (Separator/End): Used to separate two sequences (e.g., in question answering, where you have a question and a context) or to mark the end of a single sequence.\n",
    "3. `[PAD]` / `<pad>` (Padding): Used to fill shorter sequences to the same length as the longest sequence in a batch (more on this later).\n",
    "4. `[UNK]` / `<unk>` (Unknown): A fallback token for characters or subwords not found in the vocabulary (rare with subword tokenization, but possible).\n",
    "5. `[MASK]` / `<mask>` (Mask): Used in masked language modeling (e.g., BERT) where a token is randomly masked, and the model has to predict it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40f4b6-63d1-470f-bf48-bcb3cb4bf192",
   "metadata": {},
   "source": [
    "## **Introduction to Auto Classes**\n",
    "\n",
    "The Auto classes are Hugging Face's genius way of making it incredibly simple to load the correct tokenizer and model architecture for any pre-trained checkpoint from the Hub.\n",
    "\n",
    "<img width=\"800\" height=\"500\" src=\"data/images/hugging_face_transformers_pipeline.jpeg\">\n",
    "\n",
    "\n",
    "### **Loading Models & Tokenizers Manually**\n",
    "While **pipeline** is great for quick tasks, for more control (e.g., when you want to fine-tune a model, access intermediate layers, or customize generation parameters), you'll load the **model** and its corresponding **tokenizer** separately. This is where the **Auto classes** come in.\n",
    "\n",
    "### **Why is it called \"auto\"?**\n",
    "It's **\"auto\"** because it intelligently loads the correct tokenizer architecture and configuration for any pre-trained model you specify, ensuring compatibility.\n",
    "\n",
    "### **Why use Auto classes?**\n",
    "1. **Simplicity:** You don't need to know the exact class name (e.g., BertTokenizer, GPT2Model). Auto handles it.\n",
    "2. **Interoperability:** Easily swap models by just changing the model_name string.\n",
    "3. **Compatibility:** Ensures that the tokenizer and model are correctly matched for a given pre-trained checkpoint.\n",
    "\n",
    "### **Setting up a custom pipeline using Auto Classes**\n",
    "\n",
    "We need to perform the following two steps:\n",
    "1. Preprocessing Data using AutoTokenizer (i.e. Tokenize the input)\n",
    "2. Use the AutoModel to solve the Task (i.e. Pass the input to appropriate Model)\n",
    "\n",
    "### **Preprocessing Data using `AutoTokenizer` (i.e. Tokenize the input)**\n",
    "\n",
    "The main purpose of **AutoTokenizer** is to convert raw text into numerical representations (tokens) that a model can understand. This is achieved by:\n",
    "1. **Splitting text:** Breaking down text into words or sub-word units.\n",
    "2. **Converting to IDs:** Mapping those units to numerical IDs from the model's vocabulary.\n",
    "3. **Adding special tokens:** Adding tokens like [CLS] (start of sequence) or [SEP] (separator) required by certain models.\n",
    "4. **Padding and Truncation:** Making all input sequences the same length (padding) or cutting them down (truncation) to fit the model's maximum input size.\n",
    "\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. Note that:\n",
    "- **AutoTokenizer:** Text, use a `AutoTokenizer` to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.\n",
    "- **AutoFeatureExtractor:** Speech and audio, use a `AutoFeatureExtractor` to extract sequential features from audio waveforms and convert them into tensors.\n",
    "- **AutoImageProcessor:** Image inputs use a `AutoImageProcessor` to convert images into tensors.\n",
    "- **AutoProcessor:** Multimodal inputs, use a `AutoProcessor` to combine a tokenizer and a feature extractor or image processor.\n",
    "\n",
    "**Note: `AutoProcessor` always works and automatically chooses the correct class for the model you‚Äôre using, whether you‚Äôre using a tokenizer, image processor, feature extractor or processor.**\n",
    "\n",
    "\n",
    "### **Use the `AutoModel` to solve the Task (i.e. Pass the input to appropriate Model)**\n",
    "\n",
    "The purpose of **AutoModel** is to load the pre-trained model weights. \n",
    "\n",
    "After the data has been preprocessed and converted to vectors, we can use the following pre-trained AutoModel classes for solving:\n",
    "- [Natural Language Processing](https://huggingface.co/docs/transformers/model_doc/auto#natural-language-processing)\n",
    "- [Computer Vision](https://huggingface.co/docs/transformers/model_doc/auto#computer-vision)\n",
    "- [Audio](https://huggingface.co/docs/transformers/model_doc/auto#audio)\n",
    "- [Multimodal](https://huggingface.co/docs/transformers/model_doc/auto#multimodal)\n",
    "\n",
    "Similar to AutoTokenizer, it automatically infers the correct model architecture based on the model_name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0032a91-234d-41b8-86ed-ac58896f6b85",
   "metadata": {},
   "source": [
    "## **AutoTokenizer**\n",
    "\n",
    "A tokenizer takes text as input and outputs numbers the associated model can make sense of.\n",
    "\n",
    "Note that, the Auto classes are Hugging Face's genius way of making it incredibly simple to load the correct tokenizer and model architecture for any pre-trained checkpoint from the Hub.\n",
    "\n",
    "Let's learn the step by step process now.\n",
    "\n",
    "### **Using .from_pretrained(\"model_name\")**\n",
    "\n",
    "When you call `AutoTokenizer.from_pretrained(\"model_name\")` the library does several things:\n",
    "\n",
    "1. **Downloads Configuration:** It first downloads the **config.json** file associated with model_name from the Hugging Face Hub. This file contains metadata about the model (e.g., vocabulary size, number of layers, hidden dimension, the type of tokenizer it expects).\n",
    "2. **Instantiates Class:** Based on the config.json and the Auto class you used (e.g., AutoTokenizer knows to look for **tokenizer_config.json**), it instantiates the correct specific class (e.g., BertTokenizerFast, GPT2Tokenizer) behind the scenes.\n",
    "3. **Caches:** All downloaded files are cached locally on your system, so subsequent loads are much faster.\n",
    "\n",
    "### **Configuration Files** \n",
    "- tokenizer_config.json\n",
    "- config.json\n",
    "- vocab.json\n",
    "- merges.txt\n",
    "- tokenizer.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd6c504-cb8d-4280-9ea2-02fb75ef1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Let's try to tokenize!\n",
      "\n",
      "Tokenized Output:\n",
      "{'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Decoded Text Output:\n",
      "[CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print()\n",
    "\n",
    "# Tokenize input\n",
    "tokens = tokenizer(input_text)\n",
    "print(\"Tokenized Output:\")\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "# Decoding input_ids back to words\n",
    "print(\"Decoded Text Output:\")\n",
    "print(tokenizer.decode(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcc2e7-009a-4194-9235-482d20aaf3d5",
   "metadata": {},
   "source": [
    "### **What are input_ids, attention_mask and token_type_ids?**\n",
    "\n",
    "The tokenizer returns a dictionary with three important items:\n",
    "\n",
    "- **input_ids:** These are integer IDs of tokens from the tokenizer‚Äôs vocabulary. Each ID maps to a word or subword. The input ids are often the **only required parameters to be passed to the model as input**. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
    "- **attention_mask:** It tells the model which tokens to attend to (1) and which to ignore (0 for padding)\n",
    "- **token_type_ids:** identifies which segment a token belongs to when there is more than one sequence.\n",
    "\n",
    "**token_type_ids**  \n",
    "- Some models (like BERT) are designed to take two distinct sequences as input for tasks such as Question Answering (Question + Context) or Next Sentence Prediction. token_type_ids distinguish between these two segments.\n",
    "- token_type_ids helps the model understand which tokens belong to the first segment and which belong to the second.\n",
    "- These require two different sequences to be joined in a single ‚Äúinput_ids‚Äù entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:\n",
    "- Values:\n",
    "    - 0: For tokens belonging to the first sequence.\n",
    "    - 1: For tokens belonging to the second sequence.\n",
    "```python\n",
    "# [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]\n",
    "```\n",
    "\n",
    "We can use AutoTokenizer to automatically generate such a sentence by passing the two sequences to tokenizer as two arguments (and not a list, like before) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b6bdd0-83ee-4857-8be3-a612e7c826f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_type_ids:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Decoded Text Output:\n",
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "\n",
    "print(\"token_type_ids:\")\n",
    "print(encoded_dict[\"token_type_ids\"])\n",
    "print()\n",
    "\n",
    "# Decoding input_ids back to words\n",
    "print(\"Decoded Text Output:\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19272fe4-a1ed-4941-93e1-fc66fd27317e",
   "metadata": {},
   "source": [
    "### **return_tensor Argument**\n",
    "Finally, you want the tokenizer to return the actual tensors that get fed to the model.\n",
    "\n",
    "In order to do that, we can set the `return_tensors` parameter to either **'pt' for PyTorch**, or **'tf' for TensorFlow**.  \n",
    "\n",
    "`return_tensors`: Acceptable values are:\n",
    "- 'tf': Return TensorFlow tf.constant objects.\n",
    "- 'pt': Return PyTorch torch.Tensor objects.\n",
    "- 'np': Return Numpy np.ndarray objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e94f15-88af-4f80-9974-9ffaf81bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Let's try to tokenize!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\")\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411b13c7-3dfe-46b7-9f9c-d7e99378fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids\n",
      "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n",
      "\n",
      "Output Type:\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(input_text)\n",
    "\n",
    "print(\"Input Ids\")\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "print(\"Output Type:\")\n",
    "print(type(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9cbb22e-ed50-416f-bc5d-56c8076615d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids\n",
      "[[  101  2292  1005  1055  3046  2000 19204  4697   999   102]]\n",
      "\n",
      "Output Type:\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(input_text, return_tensors=\"np\")\n",
    "\n",
    "print(\"Input Ids\")\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "print(\"Output Type:\")\n",
    "print(type(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2789a202-cdae-4e64-90cf-16df910d220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids\n",
      "tensor([[  101,  2292,  1005,  1055,  3046,  2000, 19204,  4697,   999,   102]])\n",
      "\n",
      "Output Type:\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input Ids\")\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "print(\"Output Type:\")\n",
    "print(type(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11538cd2-3045-4c67-87a1-ca9ab26aac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids\n",
      "tf.Tensor([[  101  2292  1005  1055  3046  2000 19204  4697   999   102]], shape=(1, 10), dtype=int32)\n",
      "\n",
      "Output Type:\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(input_text, return_tensors=\"tf\")\n",
    "\n",
    "print(\"Input Ids\")\n",
    "print(encoded_input[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "print(\"Output Type:\")\n",
    "print(type(encoded_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9ea53-a6a3-4256-a53d-bebaebf31ba4",
   "metadata": {},
   "source": [
    "### **Essential Configuration & Special Token Properties**\n",
    "\n",
    "- **tokenizer.model_max_length:** The maximum length (in number of tokens) that the associated model is designed to handle. When loading a tokenizer with from_pretrained(), this value is often set based on the max_position_embeddings in the model's configuration. If no specific value is found, it might default to a very large integer (e.g., 1e30), indicating the tokenizer itself doesn't impose a hard limit, but the model still will.\n",
    "- **tokenizer.vocab_size:** The total number of unique tokens in the tokenizer's vocabulary. This includes all the subword tokens and any special tokens.\n",
    "- **tokenizer.is_fast:** A boolean indicating if this is a Rust-backed \"Fast\" tokenizer (which are generally recommended for speed and extra features like offset mapping).\n",
    "- **tokenizer.padding_side:** Indicates whether padding should be applied to the 'right' (default for most encoder models like BERT) or 'left' (common for decoder models like GPT-2, where the model generates token by token from left to right).\n",
    "- **tokenizer.truncation_side:** Indicates whether truncation should happen from the 'right' (default, cutting off the end) or 'left' (cutting off the beginning).\n",
    "- **tokenizer.model_input_names:** A list of the expected input names for the model's forward pass (e.g., ['input_ids', 'attention_mask', 'token_type_ids']). This is useful for understanding what keys the tokenizer() method will return.\n",
    "\n",
    "**Special Token Properties (and their IDs):**\n",
    "- Tokenizers automatically add special tokens (like [CLS], [SEP], [PAD]) during tokenization. These properties store the string representation of these tokens and their corresponding numerical IDs in the vocabulary.\n",
    "- **tokenizer.unk_token** and **tokenizer.unk_token_id:** Unknown token (for OOV words).\n",
    "- **tokenizer.pad_token** and **tokenizer.pad_token_id:** Padding token.\n",
    "- **tokenizer.cls_token** and **tokenizer.cls_token_id:** Classification token (e.g., BERT's start token).\n",
    "- **tokenizer.sep_token** and **tokenizer.sep_token_id:** Separator token.\n",
    "- **tokenizer.mask_token** and **tokenizer.mask_token_id:** Mask token (for masked language modeling).\n",
    "- **tokenizer.bos_token** and **tokenizer.bos_token_id:** Beginning of sentence token.\n",
    "- **tokenizer.eos_token** and **tokenizer.eos_token_id:** End of sentence token.\n",
    "- **tokenizer.additional_special_tokens** and **tokenizer.additional_special_tokens_ids:** Any other special tokens added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e115af6b-930b-42ad-92f8-283a81b25235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max Length: 512\n",
      "Vocabulary Size: 30522\n",
      "Is Fast Tokenizer: True\n",
      "Padding Side: right\n",
      "Truncation Side: right\n",
      "Model Input Names: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "\n",
      "--- Special Tokens ---\n",
      "[CLS] Token: '[CLS]' (ID: 101)\n",
      "[SEP] Token: '[SEP]' (ID: 102)\n",
      "[PAD] Token: '[PAD]' (ID: 0)\n",
      "[UNK] Token: '[UNK]' (ID: 100)\n",
      "[MASK] Token: '[MASK]' (ID: 103)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(f\"Model Max Length: {tokenizer.model_max_length}\")\n",
    "print(f\"Vocabulary Size: {tokenizer.vocab_size}\")\n",
    "print(f\"Is Fast Tokenizer: {tokenizer.is_fast}\")\n",
    "print(f\"Padding Side: {tokenizer.padding_side}\")\n",
    "print(f\"Truncation Side: {tokenizer.truncation_side}\")\n",
    "print(f\"Model Input Names: {tokenizer.model_input_names}\")\n",
    "\n",
    "print(\"\\n--- Special Tokens ---\")\n",
    "print(f\"[CLS] Token: '{tokenizer.cls_token}' (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"[SEP] Token: '{tokenizer.sep_token}' (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"[PAD] Token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"[UNK] Token: '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"[MASK] Token: '{tokenizer.mask_token}' (ID: {tokenizer.mask_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a182206-5eec-426f-82cd-c8a33acd3327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max Length: 512\n",
      "Vocabulary Size: 50265\n",
      "Is Fast Tokenizer: True\n",
      "Padding Side: right\n",
      "Truncation Side: right\n",
      "Model Input Names: ['input_ids', 'attention_mask']\n",
      "\n",
      "--- Special Tokens ---\n",
      "[CLS] Token: '<s>' (ID: 0)\n",
      "[SEP] Token: '</s>' (ID: 2)\n",
      "[PAD] Token: '<pad>' (ID: 1)\n",
      "[UNK] Token: '<unk>' (ID: 3)\n",
      "[MASK] Token: '<mask>' (ID: 50264)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "print(f\"Model Max Length: {tokenizer.model_max_length}\")\n",
    "print(f\"Vocabulary Size: {tokenizer.vocab_size}\")\n",
    "print(f\"Is Fast Tokenizer: {tokenizer.is_fast}\")\n",
    "print(f\"Padding Side: {tokenizer.padding_side}\")\n",
    "print(f\"Truncation Side: {tokenizer.truncation_side}\")\n",
    "print(f\"Model Input Names: {tokenizer.model_input_names}\")\n",
    "\n",
    "print(\"\\n--- Special Tokens ---\")\n",
    "print(f\"[CLS] Token: '{tokenizer.cls_token}' (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"[SEP] Token: '{tokenizer.sep_token}' (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"[PAD] Token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"[UNK] Token: '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"[MASK] Token: '{tokenizer.mask_token}' (ID: {tokenizer.mask_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af246d3-9f19-48f9-a198-25516865c254",
   "metadata": {},
   "source": [
    "## **Inspecting Tokenization Step by Step**\n",
    "<img style=\"float: right;\" width=\"400\" height=\"400\" src=\"data/images/tokenization.JPG\">\n",
    "\n",
    "- **Step 1: Split input_text to tokens**\n",
    "    - Create **tokens** using `tokenizer.tokinize(input_text)`. It helps to split input_text to tokens.\n",
    "- **Step 2: Convert the tokens to numerical IDs**\n",
    "    - Use `tokenizer.convert_tokens_to_ids(tokens)` to convert tokens to integer IDs. Each ID maps to a word or subword.\n",
    "- **Step 3: Append special tokens the model expects**\n",
    "    - Append special tokens the model expects using `tokenizer.prepare_for_model(input_ids)`.\n",
    "- **Step 4: Decoding input_ids back to words**\n",
    "    - Decode the final output using `tokenizer.decode(input_ids_with_special_tokens)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e511c-dde5-4222-9ade-032c84775524",
   "metadata": {},
   "source": [
    "### **Step 1: Split input_text to tokens**\n",
    "\n",
    "**Key Point**  \n",
    "- The ## prefix indicates a subword piece that belongs to the previous token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2541e8b-8a4a-4465-b3d1-d66467a73ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer's default max_length (model_max_length): 512\n",
      "\n",
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['let', \"'\", 's', 'try', 'to', 'token', '##ize', '!']\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Split input_text to tokens\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "print(f\"Tokenizer's default max_length (model_max_length): {tokenizer.model_max_length}\")\n",
    "print()\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034862b4-97a5-485f-8c24-0eb29303cc89",
   "metadata": {},
   "source": [
    "### **Step 2: Convert the tokens to numerical IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45abe8f4-422b-4ff8-9a3a-1552ca15e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Id: [2292, 1005, 1055, 3046, 2000, 19204, 4697, 999]\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Convert the tokens to numerical IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426f18c-3d29-4520-90ea-07ca160bebad",
   "metadata": {},
   "source": [
    "### **Step 3: Append special tokens the model expects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3850cff7-abac-4e75-b838-263c1b341c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Id with special tokens: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Lastly, the tokenizer adds special tokens the model expects\n",
    "out = tokenizer.prepare_for_model(input_ids)\n",
    "input_ids_with_special_tokens = out[\"input_ids\"]\n",
    "print(\"Tokens Id with special tokens:\", input_ids_with_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15d5f1-300f-4e90-baf4-c9e324b778a0",
   "metadata": {},
   "source": [
    "### **Step 4: Decoding input_ids back to words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6977ead1-2cb5-4d46-86c4-36c6f9874978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text Output: [CLS] let's try to tokenize! [SEP]\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(input_ids_with_special_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdc2f4-4edd-4c23-8fbc-41be04445c6b",
   "metadata": {},
   "source": [
    "### **Example: AutoTokenizer for \"roberta-base\" model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8334035-bab7-4ec7-9fe7-feaf79c97805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer's default max_length (model_max_length): 512\n",
      "\n",
      "Input Text: Let's try to tokenize!\n",
      "\n",
      "Tokens: ['Let', \"'s\", 'ƒ†try', 'ƒ†to', 'ƒ†token', 'ize', '!']\n",
      "\n",
      "Tokens Id: [7939, 18, 860, 7, 19233, 2072, 328]\n",
      "\n",
      "Tokens Id with special tokens: [0, 7939, 18, 860, 7, 19233, 2072, 328, 2]\n",
      "\n",
      "Decoded Text Output: <s>Let's try to tokenize!</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "print(f\"Tokenizer's default max_length (model_max_length): {tokenizer.model_max_length}\")\n",
    "print()\n",
    "\n",
    "input_text = \"Let's try to tokenize!\"\n",
    "print(\"Input Text:\", input_text)\n",
    "print()\n",
    "\n",
    "## The first step of the above pipeline is to split the text into tokens\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "## Convert the tokens to unique numerical number\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens Id:\", input_ids)\n",
    "print()\n",
    "\n",
    "## Lastly, the tokenizer adds special tokens the model expects\n",
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(\"Tokens Id with special tokens:\", final_inputs[\"input_ids\"])\n",
    "print()\n",
    "\n",
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\", tokenizer.decode(final_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a89b19-e268-43a3-9d43-7d259681c78e",
   "metadata": {},
   "source": [
    "**Important Note: with ƒ† indicating start of word**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16a9ff-ccea-45a9-8a94-8af4f636125f",
   "metadata": {},
   "source": [
    "## **Batching, Padding and Truncation**\n",
    "\n",
    "These concepts are critical for preparing data efficiently for deep learning models, especially when dealing with variable-length sequences like text.\n",
    "\n",
    "Reference: https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "\n",
    "### **Batching**\n",
    "- Deep learning models, particularly when running on GPUs, are highly optimized to process data in batches (collections of multiple input samples) rather than one sample at a time.\n",
    "- This improves efficiency and speed of computation.\n",
    "    - Efficiency: GPUs perform parallel computations very well. Processing multiple samples at once keeps the GPU busy and utilizes its power effectively.\n",
    "    - Speed: Reduces overhead from repeatedly transferring data between CPU and GPU.\n",
    "- Batched inputs are often different lengths, so they can‚Äôt be converted to fixed-size tensors. **Padding and truncation are strategies for** dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a **special padding token** to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by **truncating long sequences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b1b335a-155a-454b-aeb2-f37013f26796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['But what about second breakfast?',\n",
       " \"Don't think he knows about second breakfast, Pip.\",\n",
       " 'What about elevensies?',\n",
       " 'This is a much longer sentence that will definitely need to be truncated.',\n",
       " 'Another sentence.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "    \"This is a much longer sentence that will definitely need to be truncated.\",\n",
    "    \"Another sentence.\"\n",
    "]\n",
    "\n",
    "batched_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d4732-ce93-420c-bda9-8b3f88b2edd0",
   "metadata": {},
   "source": [
    "### **Padding**\n",
    "- When you batch multiple sequences of text, they almost always have different lengths. \n",
    "- However, neural networks require fixed-size input tensors. \n",
    "- Padding involves adding special [PAD] tokens to the shorter sequences in a batch to make them all the same length as the longest sequence in that batch.\n",
    "- The `padding` argument controls padding. It can be a boolean or a string::\n",
    "    - `padding=True or padding='longest':` Pads all sequences in the batch to the length of the longest sequence.\n",
    "    - `padding='max_length':` Pads all sequences to a specified maximum length (e.g., 512, which is common for BERT).\n",
    "    - `padding=False or padding='do_not_pad':` no padding is applied. This is the default behavior.\n",
    "- Crucial Role of attention_mask: The attention_mask becomes essential here. It tells the model to ignore the padded tokens so they don't influence the model's computations or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0491e9cb-2b83-49c0-8fbd-5a4a52f973c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer's default max_length (model_max_length): 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(f\"Tokenizer's default max_length (model_max_length): {tokenizer.model_max_length}\") \n",
    "# Often 512 or 1e30 (meaning no hard limit set by tokenizer itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "383b0ffb-9d94-4d5f-8fcc-82a09bca487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Inputs (Batch) ---\n",
      "Input IDs:\n",
      "tensor([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "          1010, 28315,  1012,   102,     0,     0],\n",
      "        [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  2097,  5791,\n",
      "          2342,  2000,  2022, 25449,  1012,   102],\n",
      "        [  101,  2178,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "Shape of Input IDs: torch.Size([5, 16])\n",
      "\n",
      "--- Decoded Sequences ---\n",
      "Sequence 1: [CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Sequence 2: [CLS] don't think he knows about second breakfast, pip. [SEP] [PAD] [PAD]\n",
      "Sequence 3: [CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Sequence 4: [CLS] this is a much longer sentence that will definitely need to be truncated. [SEP]\n",
      "Sequence 5: [CLS] another sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "--- Understanding the shapes ---\n",
      "Batch Size (Number of sentences): 5\n",
      "Sequence Length (after padding/truncation): 16\n"
     ]
    }
   ],
   "source": [
    "# Pad shorter sequences to the length of the longest in the batch\n",
    "tokenized_inputs = tokenizer(batched_sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Let's check the input ids\n",
    "print(\"\\n--- Processed Inputs (Batch) ---\")\n",
    "print(f\"Input IDs:\\n{tokenized_inputs['input_ids']}\")\n",
    "print(f\"Shape of Input IDs: {tokenized_inputs['input_ids'].shape}\") # Should be (batch_size, max_length)\n",
    "\n",
    "# Let's decode each sequence to see the effect\n",
    "print(\"\\n--- Decoded Sequences ---\")\n",
    "for i, input_ids in enumerate(tokenized_inputs['input_ids']):\n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print(f\"Sequence {i+1}: {decoded_text}\")\n",
    "\n",
    "print(\"\\n--- Understanding the shapes ---\")\n",
    "print(f\"Batch Size (Number of sentences): {tokenized_inputs['input_ids'].shape[0]}\")\n",
    "print(f\"Sequence Length (after padding/truncation): {tokenized_inputs['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29eac7-e2ac-4802-a676-5a0c29f7c41f",
   "metadata": {},
   "source": [
    "### **Truncation**\n",
    "- On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you‚Äôll need to truncate the sequence to a shorter length.\n",
    "- Models have a maximum input length they can handle (e.g., 512 tokens for bert-base-uncased).\n",
    "- If a sequence is longer than this maximum length, truncation involves cutting off the excess tokens from the end of the sequence.\n",
    "- The `truncation` argument controls truncation. It can be a boolean or a string:\n",
    "    - `truncation=True or truncation='longest_first':` Automatically truncates sequences that exceed the model's maximum length. If `max_length` is not specified, it defaults to the model's `model_max_length`\n",
    "    - `truncation='max_length':` You can explicitly set the maximum length to which sequences should be padded and truncated.\n",
    "    - `truncation=False or truncation='do_not_truncate':` no truncation is applied. This is the default behavior.\n",
    "\n",
    "In real scenarios, you'd often use `model.config.max_position_embeddings` to get the model's true maximum capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f9d8a6-eecf-45d2-85e2-d8894659c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n"
     ]
    }
   ],
   "source": [
    "# Pad shorter sequences to the length of the longest in the batch\n",
    "try:\n",
    "    tokenized_inputs = tokenizer(batched_sentences, truncation=True, return_tensors=\"pt\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204a4cb-bea4-4f7e-bfc1-2f16537e236b",
   "metadata": {},
   "source": [
    "### **Fixing the Error: \"Unable to create tensor\"**\n",
    "\n",
    "When batched_sentences is a list of multiple sentences, they will almost certainly have different lengths. For the tokenizer to combine them into a single, uniform tensor (a batch), all sequences must be of the same length.\n",
    "- `truncation=True` makes sure no sentence is too long.\n",
    "- `padding=True` makes sure no sentence is too short (it adds padding tokens).\n",
    "\n",
    "Without **padding=True**, if your sentences have different lengths, the tokenizer cannot create a single rectangular PyTorch tensor for the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1050e3e-81bb-4910-9d74-1926992c95b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processed Inputs (Batch) ---\n",
      "Input IDs:\n",
      "tensor([[  101,  2021,  2054,  2055,  2117,  6350,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "          1010, 28315,  1012,   102,     0,     0],\n",
      "        [  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  2097,  5791,\n",
      "          2342,  2000,  2022, 25449,  1012,   102],\n",
      "        [  101,  2178,  6251,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n",
      "Shape of Input IDs: torch.Size([5, 16])\n",
      "\n",
      "--- Decoded Sequences ---\n",
      "Sequence 1: [CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Sequence 2: [CLS] don't think he knows about second breakfast, pip. [SEP] [PAD] [PAD]\n",
      "Sequence 3: [CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Sequence 4: [CLS] this is a much longer sentence that will definitely need to be truncated. [SEP]\n",
      "Sequence 5: [CLS] another sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "--- Understanding the shapes ---\n",
      "Batch Size (Number of sentences): 5\n",
      "Sequence Length (after padding/truncation): 16\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(batched_sentences, \n",
    "                             truncation=True, \n",
    "                             padding=True, \n",
    "                             return_tensors=\"pt\")\n",
    "\n",
    "# Let's check the input ids\n",
    "print(\"\\n--- Processed Inputs (Batch) ---\")\n",
    "print(f\"Input IDs:\\n{tokenized_inputs['input_ids']}\")\n",
    "print(f\"Shape of Input IDs: {tokenized_inputs['input_ids'].shape}\") # Should be (batch_size, max_length)\n",
    "\n",
    "# Let's decode each sequence to see the effect\n",
    "print(\"\\n--- Decoded Sequences ---\")\n",
    "for i, input_ids in enumerate(tokenized_inputs['input_ids']):\n",
    "    decoded_text = tokenizer.decode(input_ids)\n",
    "    print(f\"Sequence {i+1}: {decoded_text}\")\n",
    "\n",
    "print(\"\\n--- Understanding the shapes ---\")\n",
    "print(f\"Batch Size (Number of sentences): {tokenized_inputs['input_ids'].shape[0]}\")\n",
    "print(f\"Sequence Length (after padding/truncation): {tokenized_inputs['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3891e-5aa9-4b24-86d1-18a9778b1262",
   "metadata": {},
   "source": [
    "### **Most Common Practice: Using truncation and padding with max_length Argument**\n",
    "- The `max_length` argument in the Hugging Face `tokenizer()` method is a crucial parameter that defines the maximum sequence length for both padding and truncation.\n",
    "- The `max_length` argument works in conjunction with both `padding` and `truncation` arguments to prepare your text data for the model.\n",
    "- padding=True (or 'longest' / 'max_length') + max_length specified:\n",
    "    - If a sequence is shorter than the max_length value, the tokenizer will add [PAD] tokens to the end of that sequence until it reaches the specified max_length.\n",
    "    - This ensures all sequences in your batch have the same length, which is a requirement for creating uniform tensors that can be fed into a deep learning model.\n",
    "- truncation=True + max_length specified:\n",
    "    - If a sequence is longer than the max_length value, the tokenizer will cut off (truncate) the excess tokens from the end of that sequence until it fits the specified max_length.\n",
    "    - This is essential because models have an inherent maximum context window or max_position_embeddings (e.g., 512 for bert-base-uncased). Passing a sequence longer than this limit would result in an error or undefined behavior.\n",
    "\n",
    "In most cases, padding your batch to the length of the longest sequence, and truncating to the maximum length a model can accept, works pretty well. However, the API supports more strategies if you need them. The three arguments you need to are: `padding`, `truncation` and `max_length`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d70049-df65-49eb-9674-3892cc8d1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer's default max_length (model_max_length): 512\n",
      "\n",
      "--- Padding & Truncation to max_length=20 (most common) ---\n",
      "\n",
      "Input IDs (combined - all same length):\n",
      "Seq 1 Length: 20, IDs: tensor([ 101, 2021, 2054, 2055, 2117, 6350, 1029,  102,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Decoded: [CLS] but what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Seq 2 Length: 20, IDs: tensor([  101,  2123,  1005,  1056,  2228,  2002,  4282,  2055,  2117,  6350,\n",
      "         1010, 28315,  1012,   102,     0,     0,     0,     0,     0,     0])\n",
      "Decoded: [CLS] don't think he knows about second breakfast, pip. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Seq 3 Length: 20, IDs: tensor([  101,  2054,  2055,  5408, 14625,  1029,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Decoded: [CLS] what about elevensies? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Seq 4 Length: 20, IDs: tensor([  101,  2023,  2003,  1037,  2172,  2936,  6251,  2008,  2097,  5791,\n",
      "         2342,  2000,  2022, 25449,  1012,   102,     0,     0,     0,     0])\n",
      "Decoded: [CLS] this is a much longer sentence that will definitely need to be truncated. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "Seq 5 Length: 20, IDs: tensor([ 101, 2178, 6251, 1012,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Decoded: [CLS] another sentence. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Final tensor shape: torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# We'll use a common BERT tokenizer for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "texts = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "    \"This is a much longer sentence that will definitely need to be truncated.\",\n",
    "    \"Another sentence.\"\n",
    "]\n",
    "\n",
    "print(f\"Tokenizer's default max_length (model_max_length): {tokenizer.model_max_length}\") \n",
    "\n",
    "# --- Padding and Truncation to a custom max_length (most common for fixed input size) ---\n",
    "print(\"\\n--- Padding & Truncation to max_length=20 (most common) ---\")\n",
    "inputs_combined = tokenizer(\n",
    "    texts,\n",
    "    padding='max_length', # Pad all sequences to max_length\n",
    "    truncation=True,      # Truncate sequences longer than max_length\n",
    "    max_length=20,        # The target fixed length\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"\\nInput IDs (combined - all same length):\")\n",
    "for i, ids in enumerate(inputs_combined['input_ids']):\n",
    "    print(f\"Seq {i+1} Length: {len(ids)}, IDs: {ids}\")\n",
    "    print(f\"Decoded: {tokenizer.decode(ids, skip_special_tokens=False)}\") # Show PAD tokens\n",
    "print(f\"\\nFinal tensor shape: {inputs_combined['input_ids'].shape}\") # All sequences are now (batch_size, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7419c-596c-46d7-8e6e-df84a3e69d21",
   "metadata": {},
   "source": [
    "## **AutoModel - The Universal Model Loader**\n",
    "\n",
    "Just as AutoTokenizer simplifies text preprocessing, AutoModel is your gateway to interacting with the pre-trained brain of a Transformer network.\n",
    "\n",
    "AutoModel is a class from the transformers library designed to automatically load the correct pre-trained model architecture and its weights based on a given checkpoint name (e.g., \"bert-base-uncased\", \"gpt2\", \"facebook/bart-large\").\n",
    "\n",
    "It loads:\n",
    "- The weights\n",
    "- The architecture class (like BertModel)\n",
    "- The configuration (like hidden_size, num_layers, etc.)\n",
    "\n",
    "But note that, AutoModel gives you the raw model without task-specific heads.\n",
    "\n",
    "\n",
    "### **Using .from_pretrained(\"model_name\")**\n",
    "\n",
    "When you call AutoModel.from_pretrained(\"model_name\") the library does several things:\n",
    "1. **Configuration Download (`config.json`):** When you call AutoModel.from_pretrained(\"model_name\"), the library first looks for and downloads the `config.json` file from the Hugging Face Hub for that model_name. This JSON file contains all the architectural blueprints and hyperparameters of the model (e.g., number of layers, hidden dimension, attention heads, vocabulary size, the task it's meant for).\n",
    "2. **Class Instantiation:** Based on the model_type specified in config.json (e.g., \"bert\"), AutoModel dynamically determines the correct Python class to instantiate (e.g., transformers.BertModel).\n",
    "3. **Weight Download:** Once the architecture is known, it downloads the actual pre-trained model weights (usually large binary files like **`pytorch_model.bin`** or **`tf_model.h5`**). These weights contain the knowledge the model acquired during its extensive pre-training on vast amounts of data.\n",
    "4. **Local Caching:** All downloaded files (config, weights) are stored in your local Hugging Face cache directory (usually `~/.cache/huggingface/transformers`), so subsequent loads of the same model are much faster.\n",
    "5. **Model Loading:** The model instance is created, and the downloaded weights are loaded into its layers.\n",
    "\n",
    "### **Configuration Files**\n",
    "- model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "907f9b48-5249-4fd3-893f-cbcbde293100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884feab-77c5-44b4-9fba-01f4444c1ab4",
   "metadata": {},
   "source": [
    "### **What are these warnings?**\n",
    "- These warnings are coming from the PyTorch backend, typically when a model is converted from a TensorFlow-style checkpoint (e.g., from .ckpt or .h5) into PyTorch, or during custom loading of model weights where the layer names do not fully align with PyTorch naming conventions.\n",
    "- In TensorFlow / Keras, the common names used in Batch Normalization or Layer Normalization layers are:\n",
    "    - gamma ‚Üí scale ‚Üí maps to weight in PyTorch\n",
    "    - beta ‚Üí offset ‚Üí maps to bias in PyTorch\n",
    "- When Hugging Face tries to load these weights into a PyTorch model, it renames them internally for compatibility.\n",
    "- No need to worry about these warnings. Your model will still load and run correctly. It won‚Äôt affect performance, outputs, or fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be07e4f3-b7e5-4989-9b5c-c1e2e1885146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"What will be the output of model?\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4d6ab-92aa-4b87-8fab-681df7fc640c",
   "metadata": {},
   "source": [
    "### **What is last_hidden_state and pooler_output?**\n",
    "\n",
    "**last_hidden_state (Embeddings for each token)**\n",
    "- It's the final output from the last encoder layer of the transformer. It contains token-level contextual embeddings.\n",
    "- Tensor of shape **(batch_size, seq_len, hidden_size)**\n",
    "- Used in NER, attention, sentence embeddings\n",
    "\n",
    "**pooler_output (Sentence-Level Embedding)**\n",
    "- It is a summary representation of the entire sentence, derived from the [CLS] token.\n",
    "- Specifically:\n",
    "    - Take the embedding of [CLS] from last_hidden_state\n",
    "    - Pass it through a dense (Linear) layer + Tanh activation\n",
    "- This gives you a fixed-size sentence vector.\n",
    "- Tensor of shape **(batch_size, hidden_size)**\n",
    "- Some models like DistilBERT, RoBERTa may not include pooler_output by default.\n",
    "\n",
    "**Example**  \n",
    "\n",
    "Let's say input is: \"I love GenAI!\"\n",
    "\n",
    "For the above input, last_hidden_state and pooler_output will be as follows:\n",
    "\n",
    "- last_hidden_state contains:\n",
    "    - [CLS] ‚Üí Vector 1\n",
    "    - I ‚Üí Vector 2\n",
    "    - love ‚Üí Vector 3\n",
    "    - GenAI ‚Üí Vector 4\n",
    "    - ! ‚Üí Vector 5\n",
    "    - [SEP] ‚Üí Vector 6\n",
    "- pooler_output = Tanh(Dense(Vector 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f04b38-0c41-421d-bf85-e13a888cdd30",
   "metadata": {},
   "source": [
    "### **Essential Model Configuration and Architecture**\n",
    "- You can access the model's configuration via its .config attribute. The config object contains all the architectural details and hyperparameters.\n",
    "- **model.config.model_type:** This defines the type of model architecture used, such as: \"bert\", \"roberta\", \"gpt2\", \"t5\", \"distilbert\", \"bloom\", etc. It helps Hugging Face determine:\n",
    "    - What tokenizer class to use\n",
    "    - Which model architecture to load\n",
    "    - How to handle special tokens like [CLS], [SEP], etc.\n",
    "- **model.config.vocab_size:** The number of unique tokens (words, subwords, or characters) the model knows. For eg:\n",
    "    - BERT: 30522\n",
    "    - GPT-2: 50257\n",
    "    - RoBERTa: 50265\n",
    "    - T5: 32128\n",
    "- **model.config.num_attention_heads:** Number of self-attention heads in each Transformer layer. For eg:\n",
    "    - BERT-base: 12 heads\n",
    "    - BERT-large: 16 heads\n",
    "    - GPT-2: 12, 24, or 32 depending on size\n",
    "- **model.config.num_hidden_layers:** The number of Transformer encoder (or decoder) layers in the model. Each layer contains Multi-head self-attention, Feed-forward neural network and Layer norm + residual connections. For eg:\n",
    "    - BERT-base: 12 layers\n",
    "    - BERT-large: 24 layers\n",
    "    - GPT-2 medium: 24 layers\n",
    "- **model.config.hidden_size:** The size of each hidden layer‚Äôs output vector and the embedding dimension. It controls: Size of token embeddings, Size of [CLS] vector, and Input/output shape of attention blocks. Bigger hidden_size = better learning capacity, but slower inference and training. For eg:\n",
    "    - BERT-base: 768\n",
    "    - BERT-large: 1024\n",
    "    - GPT-2: 768, 1024, or 1600\n",
    "- **model.config.max_position_embeddings:** This defines the maximum input sequence length the model can handle. Each token in the input gets a positional embedding based on its position (1st token, 2nd token, etc.). If your sequence is longer than this ‚Üí it will be truncated or need special handling like chunking or sliding window. Typical values:\n",
    "    - BERT: 512\n",
    "    - RoBERTa: 514\n",
    "    - GPT-2: 1024\n",
    "- **model.training:** This is a PyTorch flag that indicates whether the model is in training mode (True) or evaluation mode (False).\n",
    "    - Training mode: Dropout is enabled\n",
    "    - Evaluation mode: Dropout is disabled\n",
    "```python\n",
    "model.train()  # enables training mode\n",
    "model.eval()   # sets evaluation mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aa9e338-72c1-4146-8713-ca133615b843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Configuration ---\n",
      "  Model Type: bert\n",
      "  Vocabulary Size: 30522\n",
      "  Number of Attention Heads: 12\n",
      "  Number of Layers: 12\n",
      "  Hidden Size (Embedding Dimension): 768\n",
      "  Max Position Embeddings (max sequence length it can handle): 512\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Configuration ---\")\n",
    "print(f\"  Model Type: {model.config.model_type}\")\n",
    "print(f\"  Vocabulary Size: {model.config.vocab_size}\")\n",
    "print(f\"  Number of Attention Heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Number of Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden Size (Embedding Dimension): {model.config.hidden_size}\")\n",
    "print(f\"  Max Position Embeddings (max sequence length it can handle): {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da24ca0e-c2db-4f2c-94a8-638778147a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is in training mode (default): False\n",
      "Model is in evaluation mode: False\n"
     ]
    }
   ],
   "source": [
    "# You can also see if it's currently in training or evaluation mode\n",
    "print(f\"Model is in training mode (default): {model.training}\")\n",
    "\n",
    "# It's good practice to set the model to evaluation mode for inference\n",
    "model.eval()\n",
    "print(f\"Model is in evaluation mode: {model.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b16dd77f-b49e-4a9d-a2f4-0fde877a73f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Model Architecture----\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"----Model Architecture----\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6005222-f348-4741-85e4-737051c95835",
   "metadata": {},
   "source": [
    "## **Core AutoModel vs. Task-Specific AutoModelFor...**\n",
    "\n",
    "This is a critical distinction! The transformers library provides various AutoModelFor... classes, each tailored for a specific downstream task. They all share the same pre-trained backbone but differ in the \"head\" (the final layers) added on top.\n",
    "\n",
    "| Class                                | Use Case                         | Architecture                      |\n",
    "| ------------------------------------ | -------------------------------- | --------------------------------- |\n",
    "| `AutoModel`                          | Raw model for feature extraction | BERT, RoBERTa, GPT, etc.          |\n",
    "| `AutoModelForSequenceClassification` | Text classification              | Adds a classification head        |\n",
    "| `AutoModelForTokenClassification`    | NER, POS tagging                 | Token-wise classification         |\n",
    "| `AutoModelForQuestionAnswering`      | QnA tasks like SQuAD             | Outputs start/end logits          |\n",
    "| `AutoModelForCausalLM`               | Text generation (GPT)            | Decoder-only LM                   |\n",
    "| `AutoModelForMaskedLM`               | Fill-in-the-blank (BERT-style)   | Masked token prediction           |\n",
    "| `AutoModelForSeq2SeqLM`              | Translation, Summarization       | Encoder-decoder models (T5, BART) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d3dd7-409d-4084-8e05-b8090ec64a0a",
   "metadata": {},
   "source": [
    "## **AutoModelFor`*`, TFAutoModelFor`*` and FlaxAutoModelFor`*`**\n",
    "\n",
    "We will show how to use those briefly, following this pattern:\n",
    "\n",
    "* Given input articles.\n",
    "* Tokenize them (converting to token indices).\n",
    "* Apply the model on the tokenized data to generate summaries (represented as token indices).\n",
    "* Decode the summaries into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f941d28-921a-4827-8b5b-4fac0e9fed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the pre-trained model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42225c5f-78b3-408b-bce0-7e54fd3cea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For summarization, T5-small expects a prefix \"summarize: \", \n",
    "# so we prepend that to each article as a prompt.\n",
    "\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "\n",
    "pd.DataFrame(articles, columns=[\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9e28b-74b2-486d-afa7-4502d3a1eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "\n",
    "inputs = tokenizer(\n",
    "    articles, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    ")\n",
    "\n",
    "print(\"input_ids:\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\")\n",
    "print(inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd1c8f-f12a-43c5-8be7-a63cf04de00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries\n",
    "\n",
    "summary_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                num_beams=2,\n",
    "                min_length=0,\n",
    "                max_length=40,\n",
    ")\n",
    "\n",
    "print(summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7423df-df8f-4396-95d9-6aba641a03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated summaries\n",
    "\n",
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "pd.DataFrame(decoded_summaries, columns=[\"decoded_summaries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc83d17-cc85-4d92-aba9-2e249e021ad7",
   "metadata": {},
   "source": [
    "## **Fine-Tunning**\n",
    "\n",
    "https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266e326-1588-4286-b414-e32ab3335061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0dd13-753a-43e8-a241-b2248af084a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
