{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a8c407-31a1-4bf9-9c1d-76ecb381379d",
   "metadata": {},
   "source": [
    "# **AutoModel - The Universal Model Loader**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to AutoModel\n",
    "    - What is AutoModel?\n",
    "    - Using .from_pretrained(\"model_name\")\n",
    "    - Configuration Files\n",
    "    - What are these warnings?\n",
    "    - What is last_hidden_state and pooler_output?\n",
    "    - Essential Model Configuration and Architecture\n",
    "2. Core AutoModel vs. Task-Specific AutoModelFor...\n",
    "    - AutoModel Generate Embeddings\n",
    "    - Task-Specific AutoModelFor... (Ready for Downstream Tasks)\n",
    "3. AutoModelForSequenceClassification\n",
    "    - Sentiment Analysis (Step by Step)\n",
    "    - Step 1: Import and initialize the model and tokenizer\n",
    "    - Step 2: Preprocess the data batch\n",
    "    - Step 3: Pass your preprocessed batch of inputs directly to the model\n",
    "    - Step 4: Applying softmax activation on the output of model\n",
    "    - Step 5: Print the predicted labels\n",
    "    - Sentiment Analysis with pipeline()\n",
    "4. AutoModelForTokenClassification\n",
    "5. A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601227c9-a851-47d3-bae5-9e7cadd75be6",
   "metadata": {},
   "source": [
    "## **Introduction to AutoModel**\n",
    "\n",
    "### **What is AutoModel?**\n",
    "Just as AutoTokenizer simplifies text preprocessing, AutoModel is your gateway to interacting with the pre-trained brain of a Transformer network.\n",
    "\n",
    "AutoModel is a class from the transformers library designed to automatically load the correct pre-trained model architecture and its weights based on a given checkpoint name (e.g., \"bert-base-uncased\", \"gpt2\", \"facebook/bart-large\").\n",
    "\n",
    "It loads:\n",
    "- The weights\n",
    "- The architecture class (like BertModel)\n",
    "- The configuration (like hidden_size, num_layers, etc.)\n",
    "\n",
    "But note that, AutoModel gives you the raw model without task-specific heads. It is used for feature extraction from text data. (Discussed in detail later in this notebook)\n",
    "\n",
    "\n",
    "### **Using .from_pretrained(\"model_name\")**\n",
    "\n",
    "When you call AutoModel.from_pretrained(\"model_name\") the library does several things:\n",
    "1. **Configuration Download (`config.json`):** When you call AutoModel.from_pretrained(\"model_name\"), the library first looks for and downloads the `config.json` file from the Hugging Face Hub for that model_name. This JSON file contains all the architectural blueprints and hyperparameters of the model (e.g., number of layers, hidden dimension, attention heads, vocabulary size, the task it's meant for).\n",
    "2. **Class Instantiation:** Based on the model_type specified in config.json (e.g., \"bert\"), AutoModel dynamically determines the correct Python class to instantiate (e.g., transformers.BertModel).\n",
    "3. **Weight Download:** Once the architecture is known, it downloads the actual pre-trained model weights (usually large binary files like **`pytorch_model.bin`** or **`tf_model.h5`**). These weights contain the knowledge the model acquired during its extensive pre-training on vast amounts of data.\n",
    "4. **Local Caching:** All downloaded files (config, weights) are stored in your local Hugging Face cache directory (usually `~/.cache/huggingface/transformers`), so subsequent loads of the same model are much faster.\n",
    "5. **Model Loading:** The model instance is created, and the downloaded weights are loaded into its layers.\n",
    "\n",
    "### **Configuration Files**\n",
    "- model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12204be5-7626-4edd-a413-a8344ea741fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d3b38-60e6-4d76-a051-bd5b28aefa23",
   "metadata": {},
   "source": [
    "### **What are these warnings?**\n",
    "- These warnings are coming from the PyTorch backend, typically when a model is converted from a TensorFlow-style checkpoint (e.g., from .ckpt or .h5) into PyTorch, or during custom loading of model weights where the layer names do not fully align with PyTorch naming conventions.\n",
    "- In TensorFlow / Keras, the common names used in Batch Normalization or Layer Normalization layers are:\n",
    "    - gamma → scale → maps to weight in PyTorch\n",
    "    - beta → offset → maps to bias in PyTorch\n",
    "- When Hugging Face tries to load these weights into a PyTorch model, it renames them internally for compatibility.\n",
    "- No need to worry about these warnings. Your model will still load and run correctly. It won’t affect performance, outputs, or fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "919235df-9359-420b-97cf-3fd9ad6df83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2054, 2097, 2022, 1996, 6434, 1997, 2944, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the input text\n",
    "inputs = tokenizer(\"What will be the output of model?\", return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad84b6c5-0c76-4dab-9d88-706aef77b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "# Now pass your preprocessed batch of inputs directly to the model. \n",
    "# You just have to unpack the dictionary by adding **\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a054a39-a28e-4dd7-b3c2-5be70e774bd7",
   "metadata": {},
   "source": [
    "### **What is last_hidden_state and pooler_output?**\n",
    "\n",
    "**last_hidden_state (Embeddings for each token)**\n",
    "- It's the final output from the last encoder layer of the transformer. It contains token-level contextual embeddings.\n",
    "- Tensor of shape **(batch_size, seq_len, hidden_size)**\n",
    "- Used in NER, attention, sentence embeddings\n",
    "\n",
    "**pooler_output (Sentence-Level Embedding)**\n",
    "- It is a summary representation of the entire sentence, derived from the [CLS] token.\n",
    "- Specifically:\n",
    "    - Take the embedding of [CLS] from last_hidden_state\n",
    "    - Pass it through a dense (Linear) layer + Tanh activation\n",
    "- This gives you a fixed-size sentence vector.\n",
    "- Tensor of shape **(batch_size, hidden_size)**\n",
    "- Some models like DistilBERT, RoBERTa may not include pooler_output by default.\n",
    "\n",
    "**Example**  \n",
    "\n",
    "Let's say input is: \"I love GenAI!\"\n",
    "\n",
    "For the above input, last_hidden_state and pooler_output will be as follows:\n",
    "\n",
    "- last_hidden_state contains:\n",
    "    - [CLS] → Vector 1\n",
    "    - I → Vector 2\n",
    "    - love → Vector 3\n",
    "    - GenAI → Vector 4\n",
    "    - ! → Vector 5\n",
    "    - [SEP] → Vector 6\n",
    "- pooler_output = Tanh(Dense(Vector 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcdf701b-81f9-4287-9e04-8041da74c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20c6ac8f-5444-49e0-b605-86347e7532da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1746e0-9c06-4f49-860c-778feb0f6e8e",
   "metadata": {},
   "source": [
    "### **Essential Model Configuration and Architecture**\n",
    "- You can access the model's configuration via its .config attribute. The config object contains all the architectural details and hyperparameters.\n",
    "- **model.config.model_type:** This defines the type of model architecture used, such as: \"bert\", \"roberta\", \"gpt2\", \"t5\", \"distilbert\", \"bloom\", etc. It helps Hugging Face determine:\n",
    "    - What tokenizer class to use\n",
    "    - Which model architecture to load\n",
    "    - How to handle special tokens like [CLS], [SEP], etc.\n",
    "- **model.config.vocab_size:** The number of unique tokens (words, subwords, or characters) the model knows. For eg:\n",
    "    - BERT: 30522\n",
    "    - GPT-2: 50257\n",
    "    - RoBERTa: 50265\n",
    "    - T5: 32128\n",
    "- **model.config.num_attention_heads:** Number of self-attention heads in each Transformer layer. For eg:\n",
    "    - BERT-base: 12 heads\n",
    "    - BERT-large: 16 heads\n",
    "    - GPT-2: 12, 24, or 32 depending on size\n",
    "- **model.config.num_hidden_layers:** The number of Transformer encoder (or decoder) layers in the model. Each layer contains Multi-head self-attention, Feed-forward neural network and Layer norm + residual connections. For eg:\n",
    "    - BERT-base: 12 layers\n",
    "    - BERT-large: 24 layers\n",
    "    - GPT-2 medium: 24 layers\n",
    "- **model.config.hidden_size:** The size of each hidden layer’s output vector and the embedding dimension. It controls: Size of token embeddings, Size of [CLS] vector, and Input/output shape of attention blocks. Bigger hidden_size = better learning capacity, but slower inference and training. For eg:\n",
    "    - BERT-base: 768\n",
    "    - BERT-large: 1024\n",
    "    - GPT-2: 768, 1024, or 1600\n",
    "- **model.config.max_position_embeddings:** This defines the maximum input sequence length the model can handle. Each token in the input gets a positional embedding based on its position (1st token, 2nd token, etc.). If your sequence is longer than this → it will be truncated or need special handling like chunking or sliding window. Typical values:\n",
    "    - BERT: 512\n",
    "    - RoBERTa: 514\n",
    "    - GPT-2: 1024\n",
    "- **model.training:** This is a PyTorch flag that indicates whether the model is in training mode (True) or evaluation mode (False).\n",
    "    - Training mode: Dropout is enabled\n",
    "    - Evaluation mode: Dropout is disabled\n",
    "```python\n",
    "model.train()  # enables training mode\n",
    "model.eval()   # sets evaluation mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "750d027f-a470-47d7-9e45-246096cc50da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Configuration ---\n",
      "  Model Type: bert\n",
      "  Vocabulary Size: 30522\n",
      "  Number of Attention Heads: 12\n",
      "  Number of Layers: 12\n",
      "  Hidden Size (Embedding Dimension): 768\n",
      "  Max Position Embeddings (max sequence length it can handle): 512\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Configuration ---\")\n",
    "print(f\"  Model Type: {model.config.model_type}\")\n",
    "print(f\"  Vocabulary Size: {model.config.vocab_size}\")\n",
    "print(f\"  Number of Attention Heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Number of Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden Size (Embedding Dimension): {model.config.hidden_size}\")\n",
    "print(f\"  Max Position Embeddings (max sequence length it can handle): {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3532a75-1aed-4bac-8c68-00d3c04729e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is in training mode (default): False\n",
      "Model is in evaluation mode: False\n"
     ]
    }
   ],
   "source": [
    "# You can also see if it's currently in training or evaluation mode\n",
    "print(f\"Model is in training mode (default): {model.training}\")\n",
    "\n",
    "# It's good practice to set the model to evaluation mode for inference\n",
    "model.eval()\n",
    "print(f\"Model is in evaluation mode: {model.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f8c0e34-52c6-4d74-902e-70a4f7fa875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Model Architecture----\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"----Model Architecture----\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52958978-1271-47c7-b4eb-bc23408b7384",
   "metadata": {},
   "source": [
    "## **Core AutoModel vs. Task-Specific AutoModelFor...**\n",
    "\n",
    "### **AutoModel Generates Embeddings**\n",
    "- Loads only the core Transformer architecture (encoder or decoder layers) without any task-specific output layers (like a classification head or a language modeling head).\n",
    "- **Output:** The raw hidden states (or embeddings) produced by the model's final layer for each input token. These are rich, contextualized numerical representations of your text.\n",
    "- Use Cases:\n",
    "    - **Feature Extraction:** You want to get embeddings for your text that you can then feed into a traditional machine learning model (e.g., SVM, Logistic Regression).\n",
    "    - **Building Custom Heads:** You want to design your own custom layers on top of the Transformer backbone for a highly specific task.\n",
    "    - **Semantic Search:** Using sentence embeddings to find similar documents.\n",
    "\n",
    "### **Task-Specific AutoModelFor... (Ready for Downstream Tasks)**\n",
    "These models include the pre-trained Transformer backbone plus a specific \"head\" (a small neural network layer) trained or fine-tuned for a particular task. Their output is directly usable for that task.\n",
    "\n",
    "This is a critical distinction! The transformers library provides various AutoModelFor... classes, each tailored for a specific downstream task. They all share the same pre-trained backbone but differ in the \"head\" (the final layers) added on top.\n",
    "\n",
    "| Class                                | Use Case                         | Architecture                      |\n",
    "| ------------------------------------ | -------------------------------- | --------------------------------- |\n",
    "| `AutoModel`                          | Raw model for feature extraction | BERT, RoBERTa, GPT, etc.          |\n",
    "| `AutoModelForSequenceClassification` | Text classification              | Adds a classification head        |\n",
    "| `AutoModelForTokenClassification`    | NER, POS tagging                 | Token-wise classification         |\n",
    "| `AutoModelForQuestionAnswering`      | QnA tasks like SQuAD             | Outputs start/end logits          |\n",
    "| `AutoModelForCausalLM`               | Text generation (GPT)            | Decoder-only LM                   |\n",
    "| `AutoModelForMaskedLM`               | Fill-in-the-blank (BERT-style)   | Masked token prediction           |\n",
    "| `AutoModelForSeq2SeqLM`              | Translation, Summarization       | Encoder-decoder models (T5, BART) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f7910-5fd2-4498-9a61-29598ca007b4",
   "metadata": {},
   "source": [
    "## **AutoModelForSequenceClassification**\n",
    "\n",
    "- For tasks where you classify an entire sequence (sentence/document) into one or more categories.\n",
    "- **Head:** A linear layer applied to the hidden state of the special [CLS] token (or the pooled output of the sequence) to predict class logits.\n",
    "- **Output:** logits (raw scores for each class) → shape (batch_size, num_labels). You usually apply a softmax activation (i.e. softmax(logits)) to these to get probabilities  → classification probabilities\n",
    "- **Use Cases:** Sentiment analysis, spam detection, topic classification, intent recognition.\n",
    "\n",
    "### **Sentiment Analysis (Step by Step)**\n",
    "- Step 1: Import and initialize the model and tokenizer\n",
    "- Step 2: Preprocess the data batch\n",
    "- Step 3: Pass your preprocessed batch of inputs directly to the model\n",
    "- Step 4: Applying softmax activation on the output of model\n",
    "- Step 5: Print the predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08bd6f-3536-4b83-b7cb-b44781235d2d",
   "metadata": {},
   "source": [
    "### **Step 1: Import and initialize the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a78955da-2229-44f5-a62c-586cb924bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d386b2d-24a6-4e1c-b0a9-0fecd31a3cac",
   "metadata": {},
   "source": [
    "### **Step 2: Preprocess the data batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa03c68-a227-4017-8b5d-b8ecd3099aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   250,   182,  1099,   676,    23,     5,  3062,     4,     2,\n",
      "             1,     1],\n",
      "        [    0, 41710,   633,   626,    30,     5,   168,    19,    49,  5287,\n",
      "             4,     2],\n",
      "        [    0,   713,    16,    95,    10,  9624,  6755,     4,     2,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"A very bad experience at the airport.\", \n",
    "          \"Amazing job done by the government with their initiatives.\", \n",
    "          \"This is just a random string.\"]\n",
    "\n",
    "token_ids = tokenizer(tweets, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad7410-0f81-4960-b25b-71b61b06a07f",
   "metadata": {},
   "source": [
    "### **Step 3: Pass your preprocessed batch of inputs directly to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1285053-1ceb-40a7-8105-0d84586f618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5498, -0.3649, -2.5233],\n",
      "        [-2.2652, -1.1891,  3.2718],\n",
      "        [ 0.5962,  1.1877, -1.9644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Now pass your preprocessed batch of inputs directly to the model. \n",
    "# You just have to unpack the dictionary by adding **\n",
    "outputs = model(**token_ids)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca1ee4-2477-491e-a16a-85896304ab9e",
   "metadata": {},
   "source": [
    "### **Step 4: Applying softmax activation on the output of model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f0a8d8f-03ac-4639-b4e9-8104e0c31333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9430, 0.0511, 0.0059],\n",
      "        [0.0039, 0.0114, 0.9847],\n",
      "        [0.3468, 0.6265, 0.0268]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying softmax activation on the output of model\n",
    "from torch import nn\n",
    "\n",
    "predictions = nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fefae-18c4-4dea-936f-b991e8ff4f3c",
   "metadata": {},
   "source": [
    "### **Step 5: Print the predicted labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b22dd1f3-18ab-4610-afaf-08a564f35e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: A very bad experience at the airport.\n",
      "Prediction: negative\n",
      "\n",
      "Tweet: Amazing job done by the government with their initiatives.\n",
      "Prediction: positive\n",
      "\n",
      "Tweet: This is just a random string.\n",
      "Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the maximum value (the predicted class)\n",
    "predicted_class_idx = predictions.argmax(dim=-1)\n",
    "\n",
    "# Define the mapping from indices to class labels\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Convert the predicted indices to the corresponding labels\n",
    "predicted_labels = [class_labels[idx] for idx in predicted_class_idx]\n",
    "\n",
    "# Print the predicted labels\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(f\"Tweet: {tweets[i]}\")\n",
    "    print(f\"Prediction: {predicted_labels[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543074a-2044-4450-9bf6-2ce12adaa2da",
   "metadata": {},
   "source": [
    "### **Sentiment Analysis with pipeline()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28ee908d-f532-4b1a-9cc8-846977461e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9429681897163391}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"text-classification\", \n",
    "                      model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tweet = \"A very bad experience at the airport.\"\n",
    "classifier(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95e985-7cd9-4306-bf57-0d4d1878d9a8",
   "metadata": {},
   "source": [
    "## **AutoModelForTokenClassification**\n",
    "- For tasks where you classify each token in a sequence.\n",
    "- **Head:** A linear layer applied to the hidden state of every token in the sequence.\n",
    "- **Output:** logits for each token, for each possible class.\n",
    "- **Use Cases:** Named Entity Recognition (NER), Part-of-Speech (POS) tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d719c096-b1d6-4594-948e-c05c9eed4264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"My name is ThatAIGuy\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)  # [1, seq_len, num_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23775e8a-eae1-4fc9-9618-84cef2fa646d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20916a-1437-4158-b031-b64944ed860a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e5145-a38d-4dd6-81e9-efe30c4639e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb8e74-cfbe-4318-8600-d93864852ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99482442-7bbf-4f85-9ad0-dc245f829b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb93c6-c0e4-411a-ba86-16dfac0967ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f68bb-cf99-4a2e-b80b-52cabceefd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b895673-9a6f-420a-9167-ea8542f2d883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aee221-d737-4408-ac31-2f4f5d7d926d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cf757-028c-44c6-a9f1-b1bc25456745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975c67-a698-4b0d-9ad7-c8161216fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7ea1-914e-408f-88e9-c942d1fb7838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a24f43-a45b-45fb-a070-3c3072c827ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f957183-e6c1-4901-9910-d809b23547e3",
   "metadata": {},
   "source": [
    "## **AutoModelFor`*`, TFAutoModelFor`*` and FlaxAutoModelFor`*`**\n",
    "\n",
    "We will show how to use those briefly, following this pattern:\n",
    "\n",
    "* Given input articles.\n",
    "* Tokenize them (converting to token indices).\n",
    "* Apply the model on the tokenized data to generate summaries (represented as token indices).\n",
    "* Decode the summaries into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6f4c6a-a36b-401f-98f3-6cde971b39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the pre-trained model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f641bf9-c106-4574-b10a-35710f259fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xsum_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For summarization, T5-small expects a prefix \"summarize: \", \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# so we prepend that to each article as a prompt.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m article: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m article, \u001b[43mxsum_sample\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(articles, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xsum_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# For summarization, T5-small expects a prefix \"summarize: \", \n",
    "# so we prepend that to each article as a prompt.\n",
    "\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "\n",
    "pd.DataFrame(articles, columns=[\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1717468c-e748-45fd-802d-2909c844dd02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the input\u001b[39;00m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m----> 4\u001b[0m     \u001b[43marticles\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "\n",
    "inputs = tokenizer(\n",
    "    articles, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    ")\n",
    "\n",
    "print(\"input_ids:\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\")\n",
    "print(inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246659aa-2e2a-4001-9e4a-1eb1f478cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1609,  1997,  2944,  1029,   102,  2944,  1029,   102,  1029,\n",
      "           102, 14839,  1029,   102,  1029,   102, 14839,  1029,   102, 14839,\n",
      "          1029,   102, 14839,  1029,   102, 14839,  1029,   102, 14839,  1029,\n",
      "           102, 14839,  1029,   102, 14839,  1029,   102, 14839,  1029,   102]])\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries\n",
    "\n",
    "summary_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                num_beams=2,\n",
    "                min_length=0,\n",
    "                max_length=40,\n",
    ")\n",
    "\n",
    "print(summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f760e16b-29ce-4c64-9431-22bf4cba224d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoded_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get sun bought Fromp bought Fromp Fromp Purcha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   decoded_summaries\n",
       "0  Get sun bought Fromp bought Fromp Fromp Purcha..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the generated summaries\n",
    "\n",
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "pd.DataFrame(decoded_summaries, columns=[\"decoded_summaries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069969c2-096a-405a-99eb-21b32a337309",
   "metadata": {},
   "source": [
    "## **Fine-Tunning**\n",
    "\n",
    "https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc61c7-22a1-461c-b8f6-50fa58f538bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
