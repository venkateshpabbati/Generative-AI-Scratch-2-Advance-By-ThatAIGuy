{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567161f7-37c8-4f6a-9dd4-e6c41442a728",
   "metadata": {},
   "source": [
    "# **AutoModel - The Universal Model Loader**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to AutoModel\n",
    "    - What is AutoModel?\n",
    "    - Using .from_pretrained(\"model_name\")\n",
    "    - Configuration Files\n",
    "    - What are these warnings?\n",
    "    - What is last_hidden_state and pooler_output?\n",
    "    - Essential Model Configuration and Architecture\n",
    "2. Core AutoModel vs. Task-Specific AutoModelFor...\n",
    "    - AutoModel Generate Embeddings\n",
    "    - Task-Specific AutoModelFor... (Ready for Downstream Tasks)\n",
    "3. AutoModelForSequenceClassification\n",
    "    - Sentiment Analysis (Step by Step)\n",
    "    - Step 1: Import and initialize the model and tokenizer\n",
    "    - Step 2: Preprocess the data batch\n",
    "    - Step 3: Pass your preprocessed batch of inputs directly to the model\n",
    "    - Step 4: Applying softmax activation on the output of model\n",
    "    - Step 5: Print the predicted labels\n",
    "    - Sentiment Analysis with pipeline()\n",
    "4. AutoModelForTokenClassification\n",
    "5. A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ff5fb-b2fd-469e-9901-b9426ca9c395",
   "metadata": {},
   "source": [
    "## **Introduction to AutoModel**\n",
    "\n",
    "### **What is AutoModel?**\n",
    "Just as AutoTokenizer simplifies text preprocessing, AutoModel is your gateway to interacting with the pre-trained brain of a Transformer network.\n",
    "\n",
    "AutoModel is a class from the transformers library designed to automatically load the correct pre-trained model architecture and its weights based on a given checkpoint name (e.g., \"bert-base-uncased\", \"gpt2\", \"facebook/bart-large\").\n",
    "\n",
    "It loads:\n",
    "- The weights\n",
    "- The architecture class (like BertModel)\n",
    "- The configuration (like hidden_size, num_layers, etc.)\n",
    "\n",
    "But note that, AutoModel gives you the raw model without task-specific heads. It is used for feature extraction from text data. (Discussed in detail later in this notebook)\n",
    "\n",
    "\n",
    "### **Using .from_pretrained(\"model_name\")**\n",
    "\n",
    "When you call AutoModel.from_pretrained(\"model_name\") the library does several things:\n",
    "1. **Configuration Download (`config.json`):** When you call AutoModel.from_pretrained(\"model_name\"), the library first looks for and downloads the `config.json` file from the Hugging Face Hub for that model_name. This JSON file contains all the architectural blueprints and hyperparameters of the model (e.g., number of layers, hidden dimension, attention heads, vocabulary size, the task it's meant for).\n",
    "2. **Class Instantiation:** Based on the model_type specified in config.json (e.g., \"bert\"), AutoModel dynamically determines the correct Python class to instantiate (e.g., transformers.BertModel).\n",
    "3. **Weight Download:** Once the architecture is known, it downloads the actual pre-trained model weights (usually large binary files like **`pytorch_model.bin`** or **`tf_model.h5`**). These weights contain the knowledge the model acquired during its extensive pre-training on vast amounts of data.\n",
    "4. **Local Caching:** All downloaded files (config, weights) are stored in your local Hugging Face cache directory (usually `~/.cache/huggingface/transformers`), so subsequent loads of the same model are much faster.\n",
    "5. **Model Loading:** The model instance is created, and the downloaded weights are loaded into its layers.\n",
    "\n",
    "### **Configuration Files**\n",
    "- model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04274ab3-4ee8-437a-9272-bafba1eca7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d3b38-60e6-4d76-a051-bd5b28aefa23",
   "metadata": {},
   "source": [
    "### **What are these warnings?**\n",
    "- In case if you see lots of warnings after running the above cell, note the following.\n",
    "- These warnings are coming from the PyTorch backend, typically when a model is converted from a TensorFlow-style checkpoint (e.g., from .ckpt or .h5) into PyTorch, or during custom loading of model weights where the layer names do not fully align with PyTorch naming conventions.\n",
    "- In TensorFlow / Keras, the common names used in Batch Normalization or Layer Normalization layers are:\n",
    "    - gamma → scale → maps to weight in PyTorch\n",
    "    - beta → offset → maps to bias in PyTorch\n",
    "- When Hugging Face tries to load these weights into a PyTorch model, it renames them internally for compatibility.\n",
    "- No need to worry about these warnings. Your model will still load and run correctly. It won’t affect performance, outputs, or fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919235df-9359-420b-97cf-3fd9ad6df83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2054, 2097, 2022, 1996, 6434, 1997, 2944, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the input text\n",
    "inputs = tokenizer(\"What will be the output of model?\", return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad84b6c5-0c76-4dab-9d88-706aef77b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "# Now pass your preprocessed batch of inputs directly to the model. \n",
    "# You just have to unpack the dictionary by adding **\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a054a39-a28e-4dd7-b3c2-5be70e774bd7",
   "metadata": {},
   "source": [
    "### **What is last_hidden_state and pooler_output?**\n",
    "\n",
    "**last_hidden_state (Embeddings for each token)**\n",
    "- It's the final output from the last encoder layer of the transformer. It contains token-level contextual embeddings.\n",
    "- Tensor of shape **(batch_size, seq_len, hidden_size)**\n",
    "- Used in NER, attention, sentence embeddings\n",
    "\n",
    "**pooler_output (Sentence-Level Embedding)**\n",
    "- It is a summary representation of the entire sentence, derived from the [CLS] token.\n",
    "- Specifically:\n",
    "    - Take the embedding of [CLS] from last_hidden_state\n",
    "    - Pass it through a dense (Linear) layer + Tanh activation\n",
    "- This gives you a fixed-size sentence vector.\n",
    "- Tensor of shape **(batch_size, hidden_size)**\n",
    "- Some models like DistilBERT, RoBERTa may not include pooler_output by default.\n",
    "\n",
    "**Example**  \n",
    "\n",
    "Let's say input is: \"I love GenAI!\"\n",
    "\n",
    "For the above input, last_hidden_state and pooler_output will be as follows:\n",
    "\n",
    "- last_hidden_state contains:\n",
    "    - [CLS] → Vector 1\n",
    "    - I → Vector 2\n",
    "    - love → Vector 3\n",
    "    - GenAI → Vector 4\n",
    "    - ! → Vector 5\n",
    "    - [SEP] → Vector 6\n",
    "- pooler_output = Tanh(Dense(Vector 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdf701b-81f9-4287-9e04-8041da74c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c6ac8f-5444-49e0-b605-86347e7532da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1746e0-9c06-4f49-860c-778feb0f6e8e",
   "metadata": {},
   "source": [
    "### **Essential Model Configuration and Architecture**\n",
    "- You can access the model's configuration via its .config attribute. The config object contains all the architectural details and hyperparameters.\n",
    "- **model.config.model_type:** This defines the type of model architecture used, such as: \"bert\", \"roberta\", \"gpt2\", \"t5\", \"distilbert\", \"bloom\", etc. It helps Hugging Face determine:\n",
    "    - What tokenizer class to use\n",
    "    - Which model architecture to load\n",
    "    - How to handle special tokens like [CLS], [SEP], etc.\n",
    "- **model.config.vocab_size:** The number of unique tokens (words, subwords, or characters) the model knows. For eg:\n",
    "    - BERT: 30522\n",
    "    - GPT-2: 50257\n",
    "    - RoBERTa: 50265\n",
    "    - T5: 32128\n",
    "- **model.config.num_attention_heads:** Number of self-attention heads in each Transformer layer. For eg:\n",
    "    - BERT-base: 12 heads\n",
    "    - BERT-large: 16 heads\n",
    "    - GPT-2: 12, 24, or 32 depending on size\n",
    "- **model.config.num_hidden_layers:** The number of Transformer encoder (or decoder) layers in the model. Each layer contains Multi-head self-attention, Feed-forward neural network and Layer norm + residual connections. For eg:\n",
    "    - BERT-base: 12 layers\n",
    "    - BERT-large: 24 layers\n",
    "    - GPT-2 medium: 24 layers\n",
    "- **model.config.hidden_size:** The size of each hidden layer’s output vector and the embedding dimension. It controls: Size of token embeddings, Size of [CLS] vector, and Input/output shape of attention blocks. Bigger hidden_size = better learning capacity, but slower inference and training. For eg:\n",
    "    - BERT-base: 768\n",
    "    - BERT-large: 1024\n",
    "    - GPT-2: 768, 1024, or 1600\n",
    "- **model.config.max_position_embeddings:** This defines the maximum input sequence length the model can handle. Each token in the input gets a positional embedding based on its position (1st token, 2nd token, etc.). If your sequence is longer than this → it will be truncated or need special handling like chunking or sliding window. Typical values:\n",
    "    - BERT: 512\n",
    "    - RoBERTa: 514\n",
    "    - GPT-2: 1024\n",
    "- **model.training:** This is a PyTorch flag that indicates whether the model is in training mode (True) or evaluation mode (False).\n",
    "    - Training mode: Dropout is enabled\n",
    "    - Evaluation mode: Dropout is disabled\n",
    "```python\n",
    "model.train()  # enables training mode\n",
    "model.eval()   # sets evaluation mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750d027f-a470-47d7-9e45-246096cc50da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Configuration ---\n",
      "  Model Type: bert\n",
      "  Vocabulary Size: 30522\n",
      "  Number of Attention Heads: 12\n",
      "  Number of Layers: 12\n",
      "  Hidden Size (Embedding Dimension): 768\n",
      "  Max Position Embeddings (max sequence length it can handle): 512\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Configuration ---\")\n",
    "print(f\"  Model Type: {model.config.model_type}\")\n",
    "print(f\"  Vocabulary Size: {model.config.vocab_size}\")\n",
    "print(f\"  Number of Attention Heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Number of Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden Size (Embedding Dimension): {model.config.hidden_size}\")\n",
    "print(f\"  Max Position Embeddings (max sequence length it can handle): {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3532a75-1aed-4bac-8c68-00d3c04729e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is in training mode (default): False\n",
      "Model is in evaluation mode: False\n"
     ]
    }
   ],
   "source": [
    "# You can also see if it's currently in training or evaluation mode\n",
    "print(f\"Model is in training mode (default): {model.training}\")\n",
    "\n",
    "# It's good practice to set the model to evaluation mode for inference\n",
    "model.eval()\n",
    "print(f\"Model is in evaluation mode: {model.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8c0e34-52c6-4d74-902e-70a4f7fa875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Model Architecture----\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"----Model Architecture----\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52958978-1271-47c7-b4eb-bc23408b7384",
   "metadata": {},
   "source": [
    "## **Core AutoModel vs. Task-Specific AutoModelFor...**\n",
    "\n",
    "### **AutoModel Generates Embeddings**\n",
    "- Loads only the core Transformer architecture (encoder or decoder layers) without any task-specific output layers (like a classification head or a language modeling head).\n",
    "- **Output:** The raw hidden states (or embeddings) produced by the model's final layer for each input token. These are rich, contextualized numerical representations of your text.\n",
    "- Use Cases:\n",
    "    - **Feature Extraction:** You want to get embeddings for your text that you can then feed into a traditional machine learning model (e.g., SVM, Logistic Regression).\n",
    "    - **Building Custom Heads:** You want to design your own custom layers on top of the Transformer backbone for a highly specific task.\n",
    "    - **Semantic Search:** Using sentence embeddings to find similar documents.\n",
    "\n",
    "### **Task-Specific AutoModelFor... (Ready for Downstream Tasks)**\n",
    "These models include the pre-trained Transformer backbone plus a specific \"head\" (a small neural network layer) trained or fine-tuned for a particular task. Their output is directly usable for that task.\n",
    "\n",
    "This is a critical distinction! The transformers library provides various AutoModelFor... classes, each tailored for a specific downstream task. They all share the same pre-trained backbone but differ in the \"head\" (the final layers) added on top.\n",
    "\n",
    "| Class                                | Use Case                         | Architecture                      |\n",
    "| ------------------------------------ | -------------------------------- | --------------------------------- |\n",
    "| `AutoModel`                          | Raw model for feature extraction | BERT, RoBERTa, GPT, etc.          |\n",
    "| `AutoModelForSequenceClassification` | Text classification              | Adds a classification head        |\n",
    "| `AutoModelForTokenClassification`    | NER, POS tagging                 | Token-wise classification         |\n",
    "| `AutoModelForQuestionAnswering`      | QnA tasks like SQuAD             | Outputs start/end logits          |\n",
    "| `AutoModelForCausalLM`               | Text generation (GPT)            | Decoder-only LM                   |\n",
    "| `AutoModelForMaskedLM`               | Fill-in-the-blank (BERT-style)   | Masked token prediction           |\n",
    "| `AutoModelForSeq2SeqLM`              | Translation, Summarization       | Encoder-decoder models (T5, BART) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f7910-5fd2-4498-9a61-29598ca007b4",
   "metadata": {},
   "source": [
    "## **AutoModelForSequenceClassification**\n",
    "\n",
    "- For tasks where you classify an entire sequence (sentence/document) into one or more categories.\n",
    "- **Head:** A linear layer applied to the hidden state of the special [CLS] token (or the pooled output of the sequence) to predict class logits.\n",
    "- **Output:** logits (raw scores for each class) → shape (batch_size, num_labels). You usually apply a softmax activation (i.e. softmax(logits)) to these to get probabilities  → classification probabilities\n",
    "- **Use Cases:** Sentiment analysis, spam detection, topic classification, intent recognition.\n",
    "\n",
    "### **Sentiment Analysis (Step by Step)**\n",
    "- Step 1: Import and initialize the model and tokenizer\n",
    "- Step 2: Preprocess the data batch\n",
    "- Step 3: Pass your preprocessed batch of inputs directly to the model\n",
    "- Step 4: Applying softmax activation on the output of model\n",
    "- Step 5: Print the predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08bd6f-3536-4b83-b7cb-b44781235d2d",
   "metadata": {},
   "source": [
    "### **Step 1: Import and initialize the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78955da-2229-44f5-a62c-586cb924bdd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer\n\u001b[1;32m      3\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-roberta-base-sentiment-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/modeling_utils.py:4574\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4565\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4567\u001b[0m     (\n\u001b[1;32m   4568\u001b[0m         model,\n\u001b[1;32m   4569\u001b[0m         missing_keys,\n\u001b[1;32m   4570\u001b[0m         unexpected_keys,\n\u001b[1;32m   4571\u001b[0m         mismatched_keys,\n\u001b[1;32m   4572\u001b[0m         offload_index,\n\u001b[1;32m   4573\u001b[0m         error_msgs,\n\u001b[0;32m-> 4574\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4580\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/modeling_utils.py:4833\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4830\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4832\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 4833\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   4834\u001b[0m     )\n\u001b[1;32m   4836\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   4837\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/modeling_utils.py:554\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m--> 554\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/utils/import_utils.py:1417\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m():\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1417\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1418\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1420\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1421\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1422\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d386b2d-24a6-4e1c-b0a9-0fecd31a3cac",
   "metadata": {},
   "source": [
    "### **Step 2: Preprocess the data batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa03c68-a227-4017-8b5d-b8ecd3099aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   250,   182,  1099,   676,    23,     5,  3062,     4,     2,\n",
      "             1,     1],\n",
      "        [    0, 41710,   633,   626,    30,     5,   168,    19,    49,  5287,\n",
      "             4,     2],\n",
      "        [    0,   713,    16,    95,    10,  9624,  6755,     4,     2,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "tweets = [\"A very bad experience at the airport.\", \n",
    "          \"Amazing job done by the government with their initiatives.\", \n",
    "          \"This is just a random string.\"]\n",
    "\n",
    "token_ids = tokenizer(tweets, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad7410-0f81-4960-b25b-71b61b06a07f",
   "metadata": {},
   "source": [
    "### **Step 3: Pass your preprocessed batch of inputs directly to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1285053-1ceb-40a7-8105-0d84586f618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5498, -0.3649, -2.5233],\n",
      "        [-2.2652, -1.1891,  3.2718],\n",
      "        [ 0.5962,  1.1877, -1.9644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Now pass your preprocessed batch of inputs directly to the model. \n",
    "# You just have to unpack the dictionary by adding **\n",
    "outputs = model(**token_ids)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca1ee4-2477-491e-a16a-85896304ab9e",
   "metadata": {},
   "source": [
    "### **Step 4: Applying softmax activation on the output of model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f0a8d8f-03ac-4639-b4e9-8104e0c31333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9430, 0.0511, 0.0059],\n",
      "        [0.0039, 0.0114, 0.9847],\n",
      "        [0.3468, 0.6265, 0.0268]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying softmax activation on the output of model\n",
    "from torch import nn\n",
    "\n",
    "predictions = nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fefae-18c4-4dea-936f-b991e8ff4f3c",
   "metadata": {},
   "source": [
    "### **Step 5: Print the predicted labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b22dd1f3-18ab-4610-afaf-08a564f35e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: A very bad experience at the airport.\n",
      "Prediction: negative\n",
      "\n",
      "Tweet: Amazing job done by the government with their initiatives.\n",
      "Prediction: positive\n",
      "\n",
      "Tweet: This is just a random string.\n",
      "Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the maximum value (the predicted class)\n",
    "predicted_class_idx = predictions.argmax(dim=-1)\n",
    "\n",
    "# Define the mapping from indices to class labels\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Convert the predicted indices to the corresponding labels\n",
    "predicted_labels = [class_labels[idx] for idx in predicted_class_idx]\n",
    "\n",
    "# Print the predicted labels\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(f\"Tweet: {tweets[i]}\")\n",
    "    print(f\"Prediction: {predicted_labels[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543074a-2044-4450-9bf6-2ce12adaa2da",
   "metadata": {},
   "source": [
    "### **Sentiment Analysis with pipeline()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28ee908d-f532-4b1a-9cc8-846977461e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9429681897163391}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task=\"text-classification\", \n",
    "                      model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tweet = \"A very bad experience at the airport.\"\n",
    "classifier(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95e985-7cd9-4306-bf57-0d4d1878d9a8",
   "metadata": {},
   "source": [
    "## **AutoModelForTokenClassification**\n",
    "- For tasks where you classify each token in a sequence.\n",
    "- **Head:** A linear layer applied to the hidden state of every token in the sequence.\n",
    "- **Output:** logits for each token, for each possible class.\n",
    "- **Use Cases:** Named Entity Recognition (NER), Part-of-Speech (POS) tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d719c096-b1d6-4594-948e-c05c9eed4264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"My name is ThatAIGuy\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)  # [1, seq_len, num_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23775e8a-eae1-4fc9-9618-84cef2fa646d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20916a-1437-4158-b031-b64944ed860a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e5145-a38d-4dd6-81e9-efe30c4639e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb8e74-cfbe-4318-8600-d93864852ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99482442-7bbf-4f85-9ad0-dc245f829b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb93c6-c0e4-411a-ba86-16dfac0967ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f68bb-cf99-4a2e-b80b-52cabceefd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b895673-9a6f-420a-9167-ea8542f2d883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aee221-d737-4408-ac31-2f4f5d7d926d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cf757-028c-44c6-a9f1-b1bc25456745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975c67-a698-4b0d-9ad7-c8161216fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7ea1-914e-408f-88e9-c942d1fb7838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a24f43-a45b-45fb-a070-3c3072c827ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f957183-e6c1-4901-9910-d809b23547e3",
   "metadata": {},
   "source": [
    "## **AutoModelFor`*`, TFAutoModelFor`*` and FlaxAutoModelFor`*`**\n",
    "\n",
    "We will show how to use those briefly, following this pattern:\n",
    "\n",
    "* Given input articles.\n",
    "* Tokenize them (converting to token indices).\n",
    "* Apply the model on the tokenized data to generate summaries (represented as token indices).\n",
    "* Decode the summaries into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6f4c6a-a36b-401f-98f3-6cde971b39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the pre-trained model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f641bf9-c106-4574-b10a-35710f259fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xsum_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For summarization, T5-small expects a prefix \"summarize: \", \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# so we prepend that to each article as a prompt.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m article: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m article, \u001b[43mxsum_sample\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(articles, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xsum_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# For summarization, T5-small expects a prefix \"summarize: \", \n",
    "# so we prepend that to each article as a prompt.\n",
    "\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "\n",
    "pd.DataFrame(articles, columns=[\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1717468c-e748-45fd-802d-2909c844dd02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize the input\u001b[39;00m\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m----> 4\u001b[0m     \u001b[43marticles\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "\n",
    "inputs = tokenizer(\n",
    "    articles, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    ")\n",
    "\n",
    "print(\"input_ids:\")\n",
    "print(inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\")\n",
    "print(inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246659aa-2e2a-4001-9e4a-1eb1f478cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1609,  1997,  2944,  1029,   102,  2944,  1029,   102,  1029,\n",
      "           102, 14839,  1029,   102,  1029,   102, 14839,  1029,   102, 14839,\n",
      "          1029,   102, 14839,  1029,   102, 14839,  1029,   102, 14839,  1029,\n",
      "           102, 14839,  1029,   102, 14839,  1029,   102, 14839,  1029,   102]])\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries\n",
    "\n",
    "summary_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                num_beams=2,\n",
    "                min_length=0,\n",
    "                max_length=40,\n",
    ")\n",
    "\n",
    "print(summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f760e16b-29ce-4c64-9431-22bf4cba224d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoded_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get sun bought Fromp bought Fromp Fromp Purcha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   decoded_summaries\n",
       "0  Get sun bought Fromp bought Fromp Fromp Purcha..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the generated summaries\n",
    "\n",
    "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "pd.DataFrame(decoded_summaries, columns=[\"decoded_summaries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069969c2-096a-405a-99eb-21b32a337309",
   "metadata": {},
   "source": [
    "## **Fine-Tunning**\n",
    "\n",
    "https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc61c7-22a1-461c-b8f6-50fa58f538bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
