{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a8c407-31a1-4bf9-9c1d-76ecb381379d",
   "metadata": {},
   "source": [
    "# **AutoModel - The Universal Model Loader**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to AutoModel\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601227c9-a851-47d3-bae5-9e7cadd75be6",
   "metadata": {},
   "source": [
    "## **Introduction to AutoModel**\n",
    "\n",
    "### **What is AutoModel?**\n",
    "Just as AutoTokenizer simplifies text preprocessing, AutoModel is your gateway to interacting with the pre-trained brain of a Transformer network.\n",
    "\n",
    "AutoModel is a class from the transformers library designed to automatically load the correct pre-trained model architecture and its weights based on a given checkpoint name (e.g., \"bert-base-uncased\", \"gpt2\", \"facebook/bart-large\").\n",
    "\n",
    "It loads:\n",
    "- The weights\n",
    "- The architecture class (like BertModel)\n",
    "- The configuration (like hidden_size, num_layers, etc.)\n",
    "\n",
    "But note that, AutoModel gives you the raw model without task-specific heads. It is used for feature extraction from text data. (Discussed in detail later in this notebook)\n",
    "\n",
    "\n",
    "### **Using .from_pretrained(\"model_name\")**\n",
    "\n",
    "When you call AutoModel.from_pretrained(\"model_name\") the library does several things:\n",
    "1. **Configuration Download (`config.json`):** When you call AutoModel.from_pretrained(\"model_name\"), the library first looks for and downloads the `config.json` file from the Hugging Face Hub for that model_name. This JSON file contains all the architectural blueprints and hyperparameters of the model (e.g., number of layers, hidden dimension, attention heads, vocabulary size, the task it's meant for).\n",
    "2. **Class Instantiation:** Based on the model_type specified in config.json (e.g., \"bert\"), AutoModel dynamically determines the correct Python class to instantiate (e.g., transformers.BertModel).\n",
    "3. **Weight Download:** Once the architecture is known, it downloads the actual pre-trained model weights (usually large binary files like **`pytorch_model.bin`** or **`tf_model.h5`**). These weights contain the knowledge the model acquired during its extensive pre-training on vast amounts of data.\n",
    "4. **Local Caching:** All downloaded files (config, weights) are stored in your local Hugging Face cache directory (usually `~/.cache/huggingface/transformers`), so subsequent loads of the same model are much faster.\n",
    "5. **Model Loading:** The model instance is created, and the downloaded weights are loaded into its layers.\n",
    "\n",
    "### **Configuration Files**\n",
    "- model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12204be5-7626-4edd-a413-a8344ea741fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d3b38-60e6-4d76-a051-bd5b28aefa23",
   "metadata": {},
   "source": [
    "### **What are these warnings?**\n",
    "- These warnings are coming from the PyTorch backend, typically when a model is converted from a TensorFlow-style checkpoint (e.g., from .ckpt or .h5) into PyTorch, or during custom loading of model weights where the layer names do not fully align with PyTorch naming conventions.\n",
    "- In TensorFlow / Keras, the common names used in Batch Normalization or Layer Normalization layers are:\n",
    "    - gamma → scale → maps to weight in PyTorch\n",
    "    - beta → offset → maps to bias in PyTorch\n",
    "- When Hugging Face tries to load these weights into a PyTorch model, it renames them internally for compatibility.\n",
    "- No need to worry about these warnings. Your model will still load and run correctly. It won’t affect performance, outputs, or fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919235df-9359-420b-97cf-3fd9ad6df83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"What will be the output of model?\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a054a39-a28e-4dd7-b3c2-5be70e774bd7",
   "metadata": {},
   "source": [
    "### **What is last_hidden_state and pooler_output?**\n",
    "\n",
    "**last_hidden_state (Embeddings for each token)**\n",
    "- It's the final output from the last encoder layer of the transformer. It contains token-level contextual embeddings.\n",
    "- Tensor of shape **(batch_size, seq_len, hidden_size)**\n",
    "- Used in NER, attention, sentence embeddings\n",
    "\n",
    "**pooler_output (Sentence-Level Embedding)**\n",
    "- It is a summary representation of the entire sentence, derived from the [CLS] token.\n",
    "- Specifically:\n",
    "    - Take the embedding of [CLS] from last_hidden_state\n",
    "    - Pass it through a dense (Linear) layer + Tanh activation\n",
    "- This gives you a fixed-size sentence vector.\n",
    "- Tensor of shape **(batch_size, hidden_size)**\n",
    "- Some models like DistilBERT, RoBERTa may not include pooler_output by default.\n",
    "\n",
    "**Example**  \n",
    "\n",
    "Let's say input is: \"I love GenAI!\"\n",
    "\n",
    "For the above input, last_hidden_state and pooler_output will be as follows:\n",
    "\n",
    "- last_hidden_state contains:\n",
    "    - [CLS] → Vector 1\n",
    "    - I → Vector 2\n",
    "    - love → Vector 3\n",
    "    - GenAI → Vector 4\n",
    "    - ! → Vector 5\n",
    "    - [SEP] → Vector 6\n",
    "- pooler_output = Tanh(Dense(Vector 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1746e0-9c06-4f49-860c-778feb0f6e8e",
   "metadata": {},
   "source": [
    "### **Essential Model Configuration and Architecture**\n",
    "- You can access the model's configuration via its .config attribute. The config object contains all the architectural details and hyperparameters.\n",
    "- **model.config.model_type:** This defines the type of model architecture used, such as: \"bert\", \"roberta\", \"gpt2\", \"t5\", \"distilbert\", \"bloom\", etc. It helps Hugging Face determine:\n",
    "    - What tokenizer class to use\n",
    "    - Which model architecture to load\n",
    "    - How to handle special tokens like [CLS], [SEP], etc.\n",
    "- **model.config.vocab_size:** The number of unique tokens (words, subwords, or characters) the model knows. For eg:\n",
    "    - BERT: 30522\n",
    "    - GPT-2: 50257\n",
    "    - RoBERTa: 50265\n",
    "    - T5: 32128\n",
    "- **model.config.num_attention_heads:** Number of self-attention heads in each Transformer layer. For eg:\n",
    "    - BERT-base: 12 heads\n",
    "    - BERT-large: 16 heads\n",
    "    - GPT-2: 12, 24, or 32 depending on size\n",
    "- **model.config.num_hidden_layers:** The number of Transformer encoder (or decoder) layers in the model. Each layer contains Multi-head self-attention, Feed-forward neural network and Layer norm + residual connections. For eg:\n",
    "    - BERT-base: 12 layers\n",
    "    - BERT-large: 24 layers\n",
    "    - GPT-2 medium: 24 layers\n",
    "- **model.config.hidden_size:** The size of each hidden layer’s output vector and the embedding dimension. It controls: Size of token embeddings, Size of [CLS] vector, and Input/output shape of attention blocks. Bigger hidden_size = better learning capacity, but slower inference and training. For eg:\n",
    "    - BERT-base: 768\n",
    "    - BERT-large: 1024\n",
    "    - GPT-2: 768, 1024, or 1600\n",
    "- **model.config.max_position_embeddings:** This defines the maximum input sequence length the model can handle. Each token in the input gets a positional embedding based on its position (1st token, 2nd token, etc.). If your sequence is longer than this → it will be truncated or need special handling like chunking or sliding window. Typical values:\n",
    "    - BERT: 512\n",
    "    - RoBERTa: 514\n",
    "    - GPT-2: 1024\n",
    "- **model.training:** This is a PyTorch flag that indicates whether the model is in training mode (True) or evaluation mode (False).\n",
    "    - Training mode: Dropout is enabled\n",
    "    - Evaluation mode: Dropout is disabled\n",
    "```python\n",
    "model.train()  # enables training mode\n",
    "model.eval()   # sets evaluation mode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750d027f-a470-47d7-9e45-246096cc50da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Configuration ---\n",
      "  Model Type: bert\n",
      "  Vocabulary Size: 30522\n",
      "  Number of Attention Heads: 12\n",
      "  Number of Layers: 12\n",
      "  Hidden Size (Embedding Dimension): 768\n",
      "  Max Position Embeddings (max sequence length it can handle): 512\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Configuration ---\")\n",
    "print(f\"  Model Type: {model.config.model_type}\")\n",
    "print(f\"  Vocabulary Size: {model.config.vocab_size}\")\n",
    "print(f\"  Number of Attention Heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Number of Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden Size (Embedding Dimension): {model.config.hidden_size}\")\n",
    "print(f\"  Max Position Embeddings (max sequence length it can handle): {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3532a75-1aed-4bac-8c68-00d3c04729e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is in training mode (default): False\n",
      "Model is in evaluation mode: False\n"
     ]
    }
   ],
   "source": [
    "# You can also see if it's currently in training or evaluation mode\n",
    "print(f\"Model is in training mode (default): {model.training}\")\n",
    "\n",
    "# It's good practice to set the model to evaluation mode for inference\n",
    "model.eval()\n",
    "print(f\"Model is in evaluation mode: {model.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8c0e34-52c6-4d74-902e-70a4f7fa875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Model Architecture----\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"----Model Architecture----\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52958978-1271-47c7-b4eb-bc23408b7384",
   "metadata": {},
   "source": [
    "## **Core AutoModel vs. Task-Specific AutoModelFor...**\n",
    "\n",
    "This is a critical distinction! The transformers library provides various AutoModelFor... classes, each tailored for a specific downstream task. They all share the same pre-trained backbone but differ in the \"head\" (the final layers) added on top.\n",
    "\n",
    "| Class                                | Use Case                         | Architecture                      |\n",
    "| ------------------------------------ | -------------------------------- | --------------------------------- |\n",
    "| `AutoModel`                          | Raw model for feature extraction | BERT, RoBERTa, GPT, etc.          |\n",
    "| `AutoModelForSequenceClassification` | Text classification              | Adds a classification head        |\n",
    "| `AutoModelForTokenClassification`    | NER, POS tagging                 | Token-wise classification         |\n",
    "| `AutoModelForQuestionAnswering`      | QnA tasks like SQuAD             | Outputs start/end logits          |\n",
    "| `AutoModelForCausalLM`               | Text generation (GPT)            | Decoder-only LM                   |\n",
    "| `AutoModelForMaskedLM`               | Fill-in-the-blank (BERT-style)   | Masked token prediction           |\n",
    "| `AutoModelForSeq2SeqLM`              | Translation, Summarization       | Encoder-decoder models (T5, BART) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989fea1-6d99-4f21-b92d-a36295d665ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719c096-b1d6-4594-948e-c05c9eed4264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23775e8a-eae1-4fc9-9618-84cef2fa646d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20916a-1437-4158-b031-b64944ed860a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e5145-a38d-4dd6-81e9-efe30c4639e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb8e74-cfbe-4318-8600-d93864852ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99482442-7bbf-4f85-9ad0-dc245f829b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb93c6-c0e4-411a-ba86-16dfac0967ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f68bb-cf99-4a2e-b80b-52cabceefd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b895673-9a6f-420a-9167-ea8542f2d883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aee221-d737-4408-ac31-2f4f5d7d926d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cf757-028c-44c6-a9f1-b1bc25456745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975c67-a698-4b0d-9ad7-c8161216fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab7ea1-914e-408f-88e9-c942d1fb7838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a24f43-a45b-45fb-a070-3c3072c827ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f957183-e6c1-4901-9910-d809b23547e3",
   "metadata": {},
   "source": [
    "## **AutoModelFor`*`, TFAutoModelFor`*` and FlaxAutoModelFor`*`**\n",
    "\n",
    "We will show how to use those briefly, following this pattern:\n",
    "\n",
    "* Given input articles.\n",
    "* Tokenize them (converting to token indices).\n",
    "* Apply the model on the tokenized data to generate summaries (represented as token indices).\n",
    "* Decode the summaries into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6f4c6a-a36b-401f-98f3-6cde971b39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the pre-trained tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the pre-trained model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f641bf9-c106-4574-b10a-35710f259fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xsum_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For summarization, T5-small expects a prefix \"summarize: \", \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# so we prepend that to each article as a prompt.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m article: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m article, \u001b[43mxsum_sample\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(articles, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xsum_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# For summarization, T5-small expects a prefix \"summarize: \", \n",
    "# so we prepend that to each article as a prompt.\n",
    "\n",
    "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n",
    "\n",
    "pd.DataFrame(articles, columns=[\"prompts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717468c-e748-45fd-802d-2909c844dd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
