{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cff9939-7af6-4f28-ba45-ce618cbb1c5e",
   "metadata": {},
   "source": [
    "# **Huggingface ü§ó** \n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to HuggingFace\n",
    "2. ü§ó Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329fbb8-6182-44ab-888c-8f8341d28a58",
   "metadata": {},
   "source": [
    "## **Introduction to HuggingFace**\n",
    "\n",
    "### **What is HuggingFace?**\n",
    "\n",
    "Hugging Face is a company and a community of opensource ML projects, most famous for NLP.  \n",
    "\n",
    "Hugging Face makes advanced AI, especially with large language models, accessible and practical for everyone. It provides the building blocks (pre-trained models, datasets) and the tools (libraries, platform) to quickly build, experiment with, and deploy AI applications without reinventing the wheel.\n",
    "\n",
    "### **HuggingFace Hub**\n",
    "**HuggingFace Hub** is a web-based platform which hosts the following:\n",
    "1. **Models Hub:** Thousands of pre-trained models (BERT, GPT, T5, etc.) for tasks like sentiment analysis, translation, summarization, and more. Go to: [https://huggingface.co/models](https://huggingface.co/models) and click on any model. Pick a task you're interested in (like \"text-generation\"), find a popular model, and read its model card.  When you click on the model name, you'll find:\n",
    "    - Model card (what a model does)\n",
    "    - Usage code\n",
    "    - License\n",
    "    - Downloads\n",
    "    - Metrics\n",
    "    - Files (like pytorch_model.bin, config.json, tokenizer.json)\n",
    "2. **Datasets Hub:** Access and share a vast collection of datasets. High-quality data is the fuel for AI models, and Hugging Face offers a convenient way to find and utilize ready-to-use datasets for various tasks.\n",
    "3. **Spaces:** Create and host interactive demos of your machine learning models or applications directly in your browser. This allows for easy sharing and showcasing of your work.\n",
    "4. **Other Tools:** It also provides various other tools and services for things like automatic model training (AutoTrain) and **Inference APIs** for easy deployment.\n",
    "5. **Hub for Vision, Audio, and Multimodal Models:** Not limited to text anymore.\n",
    "\n",
    "\n",
    "### **Key libraries**\n",
    "1. `ü§ó Transformers:` This is the flagship library. It provides a unified API for working with state-of-the-art \"Transformer\" models (the \"T\" in GPT, BERT, etc.). These models are incredibly powerful for understanding and generating human language. With `transformers`, you can load, use, and fine-tune these models with just a few lines of code, regardless of whether you're using PyTorch or TensorFlow.\n",
    "2. `ü§ó Datasets:` This library simplifies the process of loading, processing, and sharing datasets for machine learning. It's highly efficient for handling large datasets.\n",
    "3. `ü§ó Evaluate:`\n",
    "4. `ü§ó Tokenizers:` Before a language model can understand text, it needs to break it down into smaller pieces called \"tokens.\" The tokenizers library provides highly optimized tokenizers for various models, ensuring your data is in the right format for the models to consume.\n",
    "5. `ü§ó Accelerate:` A library that helps you train your models on different hardware setups (e.g., multiple GPUs, distributed training) with minimal code changes.\n",
    "\n",
    "### **Open Source and Framework Agnostic**\n",
    "- By providing pre-trained models and easy-to-use APIs, it dramatically speeds up the development cycle.\n",
    "- The open-source nature fosters a collaborative environment, leading to faster advancements and more robust solutions.\n",
    "- It supports popular deep learning frameworks like PyTorch and TensorFlow, giving developers flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fc587-d930-4356-84cf-49adb25f1eee",
   "metadata": {},
   "source": [
    "## **ü§ó Transformers**\n",
    "\n",
    "### **What is ü§ó Transformers?**\n",
    "ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n",
    "\n",
    "**üìù Natural Language Processing:** text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.  \n",
    "**üñºÔ∏è Computer Vision:** image classification, object detection, and segmentation.  \n",
    "**üó£Ô∏è Audio:** automatic speech recognition and audio classification.  \n",
    "**üêô Multimodal:** table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.  \n",
    "\n",
    "ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n",
    "\n",
    "### **Installation**\n",
    "1. `transformers` can run on top of either PyTorch or TensorFlow. You need at least one of them installed.\n",
    "2. For PyTorch\n",
    "```\n",
    "! pip install torch\n",
    "```\n",
    "3. For TensorFlow\n",
    "```\n",
    "! pip install tensorflow\n",
    "! pip install tf-keras\n",
    "```\n",
    "4. Installing transformers\n",
    "```\n",
    "! pip install transformers\n",
    "```\n",
    "5. Installing tokenizers for fast tokenization\n",
    "```\n",
    "! pip install tokenizers\n",
    "```\n",
    "6. Installing datasets for efficient data loading and processing\n",
    "```\n",
    "! pip install datasets\n",
    "```\n",
    "7. Installing accelerate for easy distributed training\n",
    "```\n",
    "! pip install accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9c91cc-565b-4081-bf88-dd359066da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow\n",
    "# ! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b496c3-9374-45ff-9d81-122727458509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers\n",
    "# ! pip install tokenizers\n",
    "# ! pip install datasets\n",
    "# ! pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e83aae-19c3-4f7a-9227-e51a03a2c0d0",
   "metadata": {},
   "source": [
    "### **Verifying Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93281c0-832c-481b-817e-8d65a3b67fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046ec74-83f2-4a5d-9ff5-f116c1564a2f",
   "metadata": {},
   "source": [
    "## **Pipelines**\n",
    "\n",
    "### **What is pipeline()?**\n",
    "The `pipeline()` makes it simple to use any model from the `Hub` for inference on any language, computer vision, speech, and multimodal tasks. Even if you don‚Äôt have experience with a specific modality or aren‚Äôt familiar with the underlying code behind the models, you can still use them for inference with the `pipeline()`! \n",
    "\n",
    "It is the most powerful way to start using pre-trained Hugging Face models. \n",
    "\n",
    "It's a high level API that abstracts away all the complexity of tokenization, model loading, and post-processing, allowing you to perform common tasks with just a few lines of code.\n",
    "\n",
    "### **Common Tasks Supported**\n",
    "- sentiment-analysis\n",
    "- text-generation\n",
    "- ner\n",
    "- summarization\n",
    "- translation\n",
    "- question-answering\n",
    "- fill-mask (predicting missing words)\n",
    "- zero-shot-classification (classifying text without specific training examples)\n",
    "- ... and many more!\n",
    "\n",
    "Explore more on:  \n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "### **Pipeline syntax**\n",
    "1. Start by importing `pipeline`\n",
    "```python\n",
    "from transformers import pipeline\n",
    "```\n",
    "2. Specify the inference task\n",
    "```python\n",
    "classifier = pipeline(task=\"text-classification\")\n",
    "```\n",
    "3. Pass the input to the `pipeline()`\n",
    "```python\n",
    "classifier(input_text)\n",
    "```\n",
    "\n",
    "### **Behind the Scenes**\n",
    "- Loads tokenizer\n",
    "- Loads model\n",
    "- Handles pre/post-processing\n",
    "- Gives results\n",
    "\n",
    "### **Key Points**\n",
    "- The first time you run a pipeline for a specific model, it will download the model weights (which can be several hundred MB to GBs).\n",
    "- Subsequent runs will use the cached version.\n",
    "- You can specify a particular model within the pipeline if you don't want the default.\n",
    "- The output format of the pipeline varies depending on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62bcf5-bc1f-4ac9-aab2-77138166ad24",
   "metadata": {},
   "source": [
    "## **Default Model List**\n",
    "\n",
    "### **Natural Language Processing**\n",
    "For Natural Language Processing, the following are the default models for respective tasks.\n",
    "\n",
    "- Text classification:¬†`distilbert-base-uncased-finetuned-sst-2-english`\n",
    "- Token classification:¬†`dslim/bert-base-NER`\n",
    "- Text summarization:¬†`sshleifer/distilbart-cnn-12-6`\n",
    "- Question answering:¬†`distilbert-base-cased-distilled-squad`\n",
    "- Text generation:¬†`gpt2`\n",
    "- Text similarity:¬†`sentence-transformers/all-mpnet-base-v2`\n",
    "- Translation:¬†`t5-base`\n",
    "- Fill mask:¬†`distilroberta-base`\n",
    "\n",
    "### **Computer Vision**\n",
    "The default models for computer vision tasks are as follows:\n",
    "\n",
    "- Image classification:¬†`google/vit-base-patch16-224`\n",
    "- Object detection:¬†`facebook/detr-resnet-50`\n",
    "- Segmentation:¬†`facebook/detr-resnet-50-panoptic`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955688c-1d43-4cad-94c2-64bac28ac60a",
   "metadata": {},
   "source": [
    "## **Practical Natural Language Processing Use Cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950283e5-3e50-47cf-ad57-f6b575691048",
   "metadata": {},
   "source": [
    "### **Loading the text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd9b67d-35a1-42b3-93ca-24dba126a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations Alice ‚Äì Welcome to the GenAI Internship Program!\n",
      "\n",
      "Dear Alice,\n",
      "\n",
      "Congratulations! üéâ\n",
      "\n",
      "We are thrilled to inform you that you have been selected for the GenAI Internship Program, starting on 25th of this month. Your application stood out among thousands, and we‚Äôre excited to have you on board as part of this prestigious program.\n",
      "\n",
      "The official offer letters will be shared with all selected candidates on 20th of this month. Please keep an eye on your inbox and reach out in case you do not receive it by the end of that day.\n",
      "\n",
      "We look forward to your active participation and can‚Äôt wait to see the incredible work you‚Äôll do during this internship!\n",
      "\n",
      "Best regards,\n",
      "Program Coordinator\n",
      "GenAI Internship Team\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"text/email.txt\") as f:\n",
    "    email = f.read()\n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d742f7f-68a5-48fb-90c2-f9d6c4e58388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought this product from Flipkart website.\n",
      "This product is very worst and replacement policy is very bad. Even I went to their New Delhi support center.\n",
      "I used this laptop only for 30 minute and suddenly it turn off and it will never turn on.\n",
      "And Flipkart website does not replace this product. I should have gone for better brands like Apple or Alienware.\n"
     ]
    }
   ],
   "source": [
    "with open(\"text/product_review.txt\") as f:\n",
    "    product_review = f.read()\n",
    "\n",
    "print(product_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf12e84-b5a6-4c52-a2b5-fea97daa0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972aec54-d7a1-41bc-b64b-b82b4511cd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a14b5-e1e0-4d8d-9093-05a0fe8bad2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
