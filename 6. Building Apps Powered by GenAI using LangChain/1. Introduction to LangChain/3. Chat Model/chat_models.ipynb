{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9e88c5-c154-4834-8175-6862ebc2227b",
   "metadata": {},
   "source": [
    "# **Chat Models**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Chat Models\n",
    "    - What are Chat Models?\n",
    "    - Capabilities\n",
    "    - Integrations\n",
    "2. Building Chat Model - GoogleAI & OpenAI\n",
    "    - Installing the libraries\n",
    "    - Setting up the API Key\n",
    "    - Instantiating the Chat Model and Standard Parameters\n",
    "    - Key Methods\n",
    "    - invoke() Method\n",
    "    - stream() Method\n",
    "    - batch(list_of_message_lists) Method\n",
    "3. HuggingFace Chat Models\n",
    "    - What is HuggingFace?\n",
    "    - Why use HuggingFace Chat Models with LangChain?\n",
    "    - Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753337d-8fa1-4590-81a3-bedc01202905",
   "metadata": {},
   "source": [
    "## **Introduction to Chat Models**\n",
    "\n",
    "LangChain provides a unified interface for interacting with various chat models from different providers (OpenAI, Google, Anthropic, Cohere, etc.).\n",
    "\n",
    "LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\n",
    "\n",
    "\n",
    "### **What are Chat Models?**\n",
    "At their core, chat models are a specialized type of Large Language Model (LLM) designed and fine-tuned to engage in conversational interactions. Unlike older \"text completion\" models (which simply predict the next word given a string of text), chat models understand and operate on a concept of \"messages\" with associated \"roles.\"\n",
    "\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns an AI message as output. Chat Models are customized for conversational usage. **[Click Here](https://python.langchain.com/docs/integrations/chat/)** to check the complete list of LLMs which can be used with LangChain.\n",
    "\n",
    "**Chat Models Input: A list of BaseMessage objects (typically SystemMessage, HumanMessage, AIMessage)**\n",
    "```\n",
    "[SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=\"What is the capital of France?\")]\n",
    "```\n",
    "**Chat Models Output: A single AIMessage object**\n",
    "```\n",
    "AIMessage(content=\"The capital of France is Paris.\")\n",
    "```\n",
    "\n",
    "### **Capabilities**\n",
    "\n",
    "The newest generation of chat models offer additional capabilities:\n",
    "1. [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\n",
    "2. [Structured output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
    "3. [Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.\n",
    "\n",
    "### **Integrations**\n",
    "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types:\n",
    "\n",
    "1. **Official models:** These are models that are officially supported by LangChain and/or model provider. You can find these models in the **`langchain-<provider>`** packages.\n",
    "2. **Community models:** There are models that are mostly contributed and supported by the community. You can find these models in the **`langchain-community`** package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81099d-8426-4f8e-8e9d-0cccd4915da3",
   "metadata": {},
   "source": [
    "## **Building Chat Model - GoogleAI & OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b45c64-704c-45cd-b193-ed403ebbae43",
   "metadata": {},
   "source": [
    "### **Installing the libraries**\n",
    "\n",
    "```python\n",
    "! pip install langchain-google-genai -U\n",
    "! pip install langchain-openai -U\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1698394c-12ae-4f5a-bf2f-f731f10a61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-google-genai -U\n",
    "# ! pip install langchain-openai -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337e240-46e5-4667-a9dd-dd969d5771b2",
   "metadata": {},
   "source": [
    "### **Setting up the API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e380313c-365e-4a48-b96b-9828f6ae9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.gemini.txt')\n",
    "\n",
    "GOOGLE_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ddbf00-1071-45a7-88d4-0885150798f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952edee7-87d3-4c74-a6e9-f57859fe38fa",
   "metadata": {},
   "source": [
    "### **Instantiating the Chat Model and Standard Parameters**\n",
    "\n",
    "Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.\n",
    "\n",
    "| Parameter      | Description |\n",
    "|--------------|-------------|\n",
    "| model        | The name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\"). |\n",
    "| temperature  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused. |\n",
    "| timeout      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely. |\n",
    "| max_tokens   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be. |\n",
    "| stop         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response. |\n",
    "| max_retries  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits. |\n",
    "| api_key      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model. |\n",
    "| base_url     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests. |\n",
    "| rate_limiter | An optional BaseRateLimiter to space out requests to avoid exceeding rate limits. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079087d1-4adc-437a-bd8e-899e8a255294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749576822.017479 8585104 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "# Import ChatModel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Pass the standard parameters during initialization\n",
    "google_chat_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, \n",
    "                                           model=\"gemini-2.0-flash-exp\", \n",
    "                                           temperature=1)\n",
    "\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bc476-f0f8-415c-a506-f90cbe30c15d",
   "metadata": {},
   "source": [
    "### **Key Methods**\n",
    "The key methods of a chat model are:\n",
    "\n",
    "1. **invoke:** The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "2. **stream:** A method that allows you to stream the output of a chat model as it is generated, token by token. This is crucial for building responsive user interfaces.\n",
    "3. **batch:** A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "4. **bind_tools:** A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "5. **with_structured_output:** A wrapper around the invoke method for models that natively support structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e2c13-246e-46b9-84d3-3edd31bcdbae",
   "metadata": {},
   "source": [
    "### **invoke() Method**\n",
    "\n",
    "The primary method to send a list of messages to the model and get a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba51f646-ef6e-4f0d-9ce5-bf6c9936c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a polite assistant.\"),\n",
    "    HumanMessage(content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "response = google_chat_model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617c07a-9e81-46f2-a280-ddb4221f017e",
   "metadata": {},
   "source": [
    "### **stream() Method** \n",
    "\n",
    "Allows you to receive the model's response incrementally, token by token. This is crucial for building responsive user interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85f2188-fa50-441d-a186-a7d8c66e4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Streaming Response ---\n",
      "Hello! How can I assist you today?\n",
      "--- End Streaming ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Streaming Response ---\")\n",
    "\n",
    "for chunk in openai_chat_model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "print(\"\\n--- End Streaming ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e8f97-1f46-4627-9649-d21de0124b78",
   "metadata": {},
   "source": [
    "### **batch(list_of_message_lists) Method** \n",
    "\n",
    "For sending multiple sets of messages in a single API call (if the provider supports it), which can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243e47c1-d055-4e99-93d4-caecf9012272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2\n",
      "The capital of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [HumanMessage(content=\"What is 1+1?\")],\n",
    "    [HumanMessage(content=\"What is the capital of India?\")],\n",
    "]\n",
    "responses = google_chat_model.batch(batch_messages)\n",
    "\n",
    "for res in responses:\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dee74-1dac-47de-9f6d-6472435e4b5a",
   "metadata": {},
   "source": [
    "## **HuggingFace Chat Models**\n",
    "\n",
    "HuggingFace is an incredibly popular platform and community that has democratized access to state-of-the-art machine learning models, especially in Natural Language Processing (NLP). When it comes to chat models, HuggingFace hosts a vast array of models, many of which can be used for conversational AI.\n",
    "\n",
    "### **What is HuggingFace?**\n",
    "\n",
    "HuggingFace is a giant online library and community for machine learning models. \n",
    "\n",
    "They provide:\n",
    "1. **Transformers Library:** A powerful Python library that makes it easy to download, train, and use pre-trained NLP models (including chat models).\n",
    "2. **HuggingFace Hub:** A platform where anyone can share and discover models, datasets, and demos. It's like GitHub, but for ML models.\n",
    "3. **Tools & Ecosystem:** A rich set of tools for fine-tuning, deploying, and evaluating models.\n",
    "\n",
    "Many LLMs, including those capable of chat, are available on the HuggingFace Hub. These can range from smaller, open-source models that you can run locally to larger models that might require more significant computational resources.\n",
    "\n",
    "### **Why use HuggingFace Chat Models with LangChain?**\n",
    "\n",
    "While cloud-based LLMs like OpenAI's GPT models or Google's Gemini are powerful, HuggingFace offers distinct advantages:\n",
    "\n",
    "1. **Open Source & Flexibility:** Many models on HuggingFace are open-source, giving you more control, transparency, and the ability to fine-tune them for very specific tasks.\n",
    "2. **Cost-Effectiveness (Potentially):** If you can run models locally or on your own infrastructure, you can potentially reduce API costs associated with commercial LLM providers.\n",
    "3. **Privacy/Security:** For sensitive data, running models locally or on your private cloud can offer better privacy and security controls.\n",
    "4. **Experimentation:** A vast playground for trying out different model architectures and sizes.\n",
    "5. **Community Support:** A very active and helpful community.\n",
    "\n",
    "LangChain acts as a crucial bridge here. It provides a consistent interface (`ChatHuggingFace` class) that allows you to easily integrate models from the HuggingFace ecosystem into your LangChain applications, abstracting away much of the underlying complexity of the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f53c92e-c423-478d-8a4c-befa1dd93c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5c234e-8502-4b20-af67-434d0d01057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What should I study to become a data scientist?\\n\\nAs for studying, I am sure that with enough time to study it. After the year 6, study 1 will help create some interesting data. It is very important for me to learn about things like the structure of data and to see some data as it changes in the course of the study.\\n\\nYou have a very interesting idea of how you want to go about learning data science. What will you do?\\n\\nIf I have good skills in the field I can come up with the way I want to learn in less time. It will not be a perfect and I can try very hard to understand what I want it to be like. But I hope that it doesn't make it impossible for me.\\n\\nIf I fail in the learning I can try to take my time but at the same time I will have a lot more time. Even then it isn't as good as working on data in school or study 1 with other people. Besides that, in my experience many people are unhappy with data. Even if I succeed I can still be successful, even if I fail I am still working on it.\\n\\nAt the end of our interview, she talked to me about her experience with data analysis. She explained that after getting into data science she still had lots of problems with it. She said that she tried to study this stuff and made good use of it.\\n\\nDo you have any plans to teach any course outside data science in the past year?\\n\\nI would like to continue to try and teach data and related subjects at my own end of this work. Before I do you might ask. For example, if there is a book/video and no one likes it just a few paragraphs of what you do. If that happens I have to tell them and I won't. The whole question will depend.\\n\\nHow long does it take for you to become a data scientist in Canada?\\n\\nIt depends on how much we spend on research. It depends. It depends how much we are willing to pay because what is new is not new. I feel that the longer it is the better. I am also confident I will succeed in going to a higher level of data science and possibly even become a data scientist.\\n\\nPlease describe how much does it cost you to obtain study 1 data for your study?\\n\\nWe offer it for free to the public. We don't take payment or credit for data collected in study 1. We offer it for free for a long time. We are asking\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "hf.invoke(\"What should I study to become a data scientist?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
