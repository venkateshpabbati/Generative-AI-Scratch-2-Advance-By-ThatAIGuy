{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef552efe-1870-4494-81d3-732e2cef783e",
   "metadata": {},
   "source": [
    "# **LangChain - A Framework for developing applications powered by LLMs**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to LangChain\n",
    "    - What is LangChain?\n",
    "    - Why use LangChain?\n",
    "    - Architecture\n",
    "    - LangChain Packages\n",
    "2. Building Blocks\n",
    "    - Prompt Template\n",
    "    - Chat Model\n",
    "    - Output Parser\n",
    "    - Chains\n",
    "3. Case Study\n",
    "    - Building a Chat Prompt Template\n",
    "    - Building a Chat Model\n",
    "    - Building an Output Parser\n",
    "    - Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc16ab-4256-4832-9fd8-9d4b2949a4dd",
   "metadata": {},
   "source": [
    "## **Introduction to LangChain**\n",
    "\n",
    "### **What is LangChain?**\n",
    "LangChain is an open-source framework that acts like a toolkit and orchestration layer for building applications with Large Language Models.\n",
    "\n",
    "It simplifies the process of chaining together different components (like LLMs, data sources, and tools) to create complex, intelligent applications. It's designed to make prompt engineering more efficient and allow developers to adapt language models to specific business contexts.\n",
    "\n",
    "Imagine you want to build a truly smart application powered by a Large Language Model (LLM), like OpenAI's GPT or Google's Gemini. Simply sending a prompt to an LLM isn't usually enough for real-world scenarios. You often need to:\n",
    "\n",
    "1. **Get relevant information:** Your LLM might need to access external data (your documents, a database, the internet) to answer a user's question accurately.\n",
    "2. **Remember past conversations:** For a coherent dialogue, the LLM needs a memory of what was said before.\n",
    "3. **Perform actions:** Sometimes, the LLM needs to \"do\" something, like search the web, call an API, or run a calculator.\n",
    "4. **Structure its output:** You might want the LLM's response to be in a specific format (e.g., JSON, a bulleted list).\n",
    "\n",
    "This is where LangChain comes in.\n",
    "\n",
    "### **Why use LangChain?**\n",
    "- **Abstraction:** It provides high-level abstractions over common LLM functionalities, so you don't have to worry about the low-level API calls for every LLM.\n",
    "- **Modularity:** It offers modular components that you can mix and match to build various LLM-powered applications (chatbots, Q&A systems, content generators, agents).\n",
    "- **Integration:** It provides numerous integrations with different LLMs, vector databases, document loaders, and other tools.\n",
    "- **Orchestration:** It excels at orchestrating sequences of operations, allowing you to build multi-step workflows.\n",
    "\n",
    "### **Architecture**\n",
    "LangChain is a framework that consists of a number of packages.\n",
    "\n",
    "<img src=\"images/langchain_stack_updated.png\" width=\"500\" height=\"600\">\n",
    "\n",
    "### **LangChain Packages**\n",
    "**langchain-core**  \n",
    "This package contains base abstractions for different components and ways to compose them together. The interfaces for core components like **chat models**, **vector stores**, **tools** and more are defined here. No third-party integrations are defined here. The dependencies are very lightweight.\n",
    "\n",
    "**langchain**  \n",
    "The main langchain package contains chains and retrieval strategies that make up an application's cognitive architecture. These are NOT third-party integrations. All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\n",
    "\n",
    "**Integration packages**  \n",
    "Popular integrations have their own packages (e.g. langchain-openai, langchain-anthropic, etc) so that they can be properly versioned and appropriately lightweight.\n",
    "\n",
    "**langchain-community**  \n",
    "This package contains third-party integrations that are maintained by the LangChain community. Key integration packages are separated out (see above). This contains integrations for various components (chat models, vector stores, tools, etc). All dependencies in this package are optional to keep the package as lightweight as possible.\n",
    "\n",
    "**langgraph**  \n",
    "langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n",
    "\n",
    "LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52685f3-d9f4-44b8-8e6a-364dd6c4c7fa",
   "metadata": {},
   "source": [
    "## **Building Blocks**\n",
    "\n",
    "1. **Prompt Template**\n",
    "2. **Chat Model**\n",
    "3. **Output Parser**\n",
    "4. **Chains**\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adaff05-0424-4e21-a163-0f295e3ee5eb",
   "metadata": {},
   "source": [
    "### **Installation**\n",
    "\n",
    "1. Install `langchain` \n",
    "```python\n",
    "! pip install langchain\n",
    "```\n",
    "> The above command installs the following packages:\n",
    "> - langchain\n",
    "> - langchain-core\n",
    "> - langchain-text-splitters\n",
    "> - langsmith\n",
    "\n",
    "2. Install `langchain-google-genai` and `langchain-openai`\n",
    "```python\n",
    "! pip install langchain-openai langchain-google-genai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b6f7c0-1249-4832-be76-2c218489f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install langchain\n",
    "\n",
    "# ! pip install langchain\n",
    "# ! pip install langchain-openai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183c365d-68f8-470b-a3dd-0d3c88bdd027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.26\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f6bed-567d-45a5-9381-e1b194caab5d",
   "metadata": {},
   "source": [
    "### **Chat Models**\n",
    "\n",
    "LLMs handle various language operations such as translation, summarization, question answering, and content creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b3d81e-56d7-428d-bbb2-690a25c7169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.gemini.txt')\n",
    "\n",
    "GOOGLE_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4fb816-ea03-415d-b184-8262c5a494a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752258168.849632 20227588 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).  It provides tools and abstractions to connect LLMs to various data sources and other components, enabling more complex and powerful applications than simply prompting an LLM directly.\\n\\nThink of it as a toolkit for building LLM-powered applications. It includes modules for things like:\\n\\n*   **Chains:** Sequences of calls to LLMs or other utilities.\\n*   **Indexes:** Ways to structure data so LLMs can effectively access and retrieve relevant information.\\n*   **Memory:** Mechanisms for LLMs to remember previous interactions.\\n*   **Agents:** Allowing LLMs to use external tools and decide the best course of action.\\n\\nEssentially, LangChain lets you orchestrate LLMs with other tools to build more sophisticated and useful AI applications, like chatbots that can answer questions based on specific documents, or automated agents that can perform tasks using external APIs. It reduces boilerplate code and helps developers focus on application logic rather than the intricacies of interacting with LLMs.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--cbf53681-c0b3-414d-b933-537a3e74e68d-0', usage_metadata={'input_tokens': 13, 'output_tokens': 217, 'total_tokens': 230, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ChatModel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Pass the standard parameters during initialization\n",
    "chat_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, \n",
    "                                    model=\"gemini-2.0-flash\", \n",
    "                                    temperature=1)\n",
    "\n",
    "# Creating a Prompt\n",
    "prompt = \"What is LangChain? Explain in 200 words.\"\n",
    "\n",
    "# Printing the output of model\n",
    "model_response = chat_model.invoke(prompt)\n",
    "\n",
    "model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45960a9e-4e0a-4633-8e4f-eec1ded1b472",
   "metadata": {},
   "source": [
    "### **Output Parser**\n",
    "\n",
    "- LLMs primarily generate free-form text. However, in many applications, you need structured data (e.g., a list of items, a JSON object, a specific format). OutputParsers bridge this gap.\n",
    "- These are the objects that take the raw string output from an LLM and transform it into a more usable Python data structure (e.g., a list, a dictionary, a Pydantic object).\n",
    "- **Input** to the output parser should be an **AIMessage**.\n",
    "\n",
    "**The output of a ChatModel (and therefore, of this chain) is a AI Message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1648a0-d87d-4f43-8fbc-47a185e60022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Parsing\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82762c01-e6e0-4a1e-b070-1f051fb96704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs).  It provides tools and abstractions to connect LLMs to various data sources and other components, enabling more complex and powerful applications than simply prompting an LLM directly.\\n\\nThink of it as a toolkit for building LLM-powered applications. It includes modules for things like:\\n\\n*   **Chains:** Sequences of calls to LLMs or other utilities.\\n*   **Indexes:** Ways to structure data so LLMs can effectively access and retrieve relevant information.\\n*   **Memory:** Mechanisms for LLMs to remember previous interactions.\\n*   **Agents:** Allowing LLMs to use external tools and decide the best course of action.\\n\\nEssentially, LangChain lets you orchestrate LLMs with other tools to build more sophisticated and useful AI applications, like chatbots that can answer questions based on specific documents, or automated agents that can perform tasks using external APIs. It reduces boilerplate code and helps developers focus on application logic rather than the intricacies of interacting with LLMs.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bafa1-110f-476f-88c1-03d6a77ed4c8",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "\n",
    "- Writing static strings as prompts quickly becomes unmanageable. We need a way to inject dynamic information. This is where Prompt Template comes in.\n",
    "- These are the objects that help you construct prompts dynamically by accepting input variables. Think of them as blueprints for your prompts.\n",
    "- There are two types: **PromptTemplate** and **ChatPromptTemplate**.\n",
    "- **Input** to the Prompt Template should be a **dictionary** containing raw user inputs.\n",
    "- **Output** of a Prompt Template will be a **string** or **list of chat messages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c510cec-a921-458a-a0ad-80fa09ccb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "                    messages = [\n",
    "                    (\"system\", \"You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. \"),\n",
    "                    (\"human\", \"What is {topic}? Explain in 200 words.\"),\n",
    "                    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5def41-723e-4b44-bf8e-378bcc647445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec953ec-93a5-4b54-8958-877d95803c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain? Explain in 200 words.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"topic\": \"LangChain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb105e9-86e0-45f3-8983-566030d4bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. \n",
      "Human: What is LangChain? Explain in 200 words.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.invoke({\"topic\": \"LangChain\"}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd03071-2c3f-4f0b-8190-96ee8aebc4b0",
   "metadata": {},
   "source": [
    "### **Chains**\n",
    "\n",
    "- A chain is simply a sequence of operations where the output of one step becomes the input of the next. This allows you to build more complex and intelligent workflows than a single LLM call could achieve.\n",
    "- The `|` symbol chains together the different components feeds the output from one component as input into the next component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ca1ec79-76a1-4265-9606-fe69439cd794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 1.Prompt Template\n",
    "prompt = prompt_template.invoke({\"topic\": \"Linear Regression\"})\n",
    "print(type(prompt))\n",
    "\n",
    "# 2. Chat Model\n",
    "model_response = chat_model.invoke(prompt)\n",
    "print(type(model_response))\n",
    "\n",
    "# 3. Ouput Parser\n",
    "final_output = output_parser.invoke(model_response)\n",
    "print(type(final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596048ab-59d5-4471-b41d-73802c503074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's break down Natural Language Processing (NLP) in a concise way.\\n\\n**NLP, or Natural Language Processing, is a branch of Artificial Intelligence focused on enabling computers to understand, interpret, and generate human language.** Think of it as bridging the gap between how humans communicate and how computers process information.\\n\\nInstead of relying on structured data, NLP algorithms are designed to work with unstructured text and speech data. This involves a range of tasks including:\\n\\n*   **Understanding meaning:** Analyzing sentiment, identifying key entities, and discerning relationships between words and concepts.\\n*   **Generating text:** Creating summaries, translating languages, writing different kinds of creative content, and answering questions in a comprehensive manner.\\n*   **Speech recognition & synthesis:** Converting spoken language into text and vice versa.\\n\\nNLP combines computer science, linguistics, and machine learning techniques to build models that can analyze language patterns, extract relevant information, and respond in a meaningful way. NLP is used in various applications like chatbots, machine translation, spam filtering, and sentiment analysis.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | chat_model | output_parser\n",
    "\n",
    "user_input = {\"topic\": \"NLP\"}\n",
    "\n",
    "chain.invoke(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930375ff-d9f7-4cf0-b4da-6fb16b39db7e",
   "metadata": {},
   "source": [
    "## **Case Study**\n",
    "\n",
    "**Create an AI Tutor App that uses Prompts and Chat internally to give Python Implementation tutorial for Data Science topics**\n",
    "\n",
    "Inorder to solve this, we will first create the following three components:\n",
    "1. Chat Prompt Template\n",
    "2. Chat Model\n",
    "3. Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28998a35-5ebe-4900-a5f2-d360271756d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\nYou are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python Implementation for topics asked by user.\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, template='Tell me a python implementation for {topic_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Building a Chat Prompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Constructing System Prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "You are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python Implementation for topics asked by user.\n",
    "\"\"\")\n",
    "\n",
    "# Constructing Human Prompt\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"Tell me a python implementation for {topic_name}.\")\n",
    "\n",
    "# Compiling Chat Prompt\n",
    "chat_prompt = ChatPromptTemplate(messages=[system_prompt, human_prompt])\n",
    "\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a471dd0b-fb92-47cf-b09c-8266405a66ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752258194.022132 20227588 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    }
   ],
   "source": [
    "## Building a Chat Model\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Pass the standard parameters during initialization\n",
    "chat_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, \n",
    "                                    model=\"gemini-2.0-flash\", \n",
    "                                    temperature=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d6c48a5-b181-40b7-a8d0-f180b0a8bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building an Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8944772-4e6a-4fb5-a44a-2ebd29564846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaining All the components together\n",
    "\n",
    "chain = chat_prompt | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e52abfe-78d9-4a2a-a83b-c00fb93a91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly, let's break down the implementation of Logistic Regression in Python step by step.\n",
      "\n",
      "**Understanding Logistic Regression**\n",
      "\n",
      "Logistic Regression is a classification algorithm used when the dependent variable is categorical. It models the probability of a binary outcome (0 or 1) using a sigmoid function.\n",
      "\n",
      "**Steps for Implementation**\n",
      "\n",
      "1.  **Import Libraries:**\n",
      "    *   `numpy` for numerical operations\n",
      "    *   `sklearn.model_selection` for train/test split\n",
      "    *   `sklearn.preprocessing` for feature scaling (optional but often recommended)\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler  # Optional\n",
      "```\n",
      "\n",
      "2.  **Define the Sigmoid Function:**\n",
      "    *   The sigmoid function maps any real value to a value between 0 and 1.\n",
      "    *   Formula: `sigmoid(z) = 1 / (1 + exp(-z))`\n",
      "\n",
      "```python\n",
      "def sigmoid(z):\n",
      "    return 1 / (1 + np.exp(-z))\n",
      "```\n",
      "\n",
      "3.  **Define the Logistic Regression Model:**\n",
      "\n",
      "```python\n",
      "class LogisticRegression:\n",
      "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
      "        self.lr = learning_rate\n",
      "        self.n_iters = n_iters\n",
      "        self.weights = None\n",
      "        self.bias = None\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        n_samples, n_features = X.shape\n",
      "\n",
      "        # Initialize weights and bias\n",
      "        self.weights = np.zeros(n_features)\n",
      "        self.bias = 0\n",
      "\n",
      "        # Gradient descent\n",
      "        for _ in range(self.n_iters):\n",
      "            # Linear model: z = Xw + b\n",
      "            linear_model = np.dot(X, self.weights) + self.bias\n",
      "            # Apply sigmoid function\n",
      "            y_predicted = sigmoid(linear_model)\n",
      "\n",
      "            # Compute gradients\n",
      "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
      "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
      "\n",
      "            # Update parameters\n",
      "            self.weights -= self.lr * dw\n",
      "            self.bias -= self.lr * db\n",
      "\n",
      "    def predict(self, X):\n",
      "        linear_model = np.dot(X, self.weights) + self.bias\n",
      "        y_predicted = sigmoid(linear_model)\n",
      "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
      "        return np.array(y_predicted_cls)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   `__init__`: Initializes the learning rate and number of iterations.\n",
      "*   `fit`: Implements gradient descent to learn the weights and bias. It calculates the predicted values using the sigmoid function, computes the gradients of the cost function with respect to the weights and bias, and updates the parameters.\n",
      "*   `predict`: Predicts the class labels for new data. It applies the linear model and sigmoid function, and then assigns a class label based on whether the predicted probability is greater than 0.5.\n",
      "\n",
      "4.  **Usage:**\n",
      "\n",
      "```python\n",
      "# Sample Data (replace with your dataset)\n",
      "from sklearn.datasets import make_classification\n",
      "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
      "\n",
      "# Split into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
      "\n",
      "# Scale features (optional)\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "\n",
      "# Train the model\n",
      "model = LogisticRegression(learning_rate=0.1, n_iters=1000)\n",
      "model.fit(X_train, y_train)\n",
      "predictions = model.predict(X_test)\n",
      "\n",
      "# Evaluate accuracy\n",
      "def accuracy(y_true, y_pred):\n",
      "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
      "    return accuracy\n",
      "\n",
      "acc = accuracy(y_test, predictions)\n",
      "print(\"Accuracy:\", acc)\n",
      "```\n",
      "\n",
      "**Key Points**\n",
      "\n",
      "*   **Gradient Descent:** The `fit` method uses gradient descent to minimize the cost function and find the optimal weights and bias.\n",
      "*   **Feature Scaling:** Feature scaling is often important, especially when features have different scales. `StandardScaler` is a common way to standardize features (zero mean, unit variance).\n",
      "*   **Evaluation:** The example code includes an `accuracy` function for evaluating the model's performance.\n",
      "*   **Regularization:** L1 or L2 regularization can be added to the cost function to prevent overfitting.\n",
      "*   **Multi-class Classification:** For problems with more than two classes, you can use techniques like One-vs-Rest (OvR) or Multinomial Logistic Regression (Softmax Regression).\n",
      "*   **Libraries:** For more complex applications, consider using established libraries like scikit-learn (`sklearn.linear_model.LogisticRegression`) which provide optimized and feature-rich implementations.\n",
      "\n",
      "Let me know if you have any specific questions or would like to explore advanced topics such as regularization or multi-class classification!\n"
     ]
    }
   ],
   "source": [
    "## Calling the chain\n",
    "\n",
    "user_input = {\"topic_name\": \"Logistic Regression\"}\n",
    "\n",
    "output = chain.invoke(user_input)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
