{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef552efe-1870-4494-81d3-732e2cef783e",
   "metadata": {},
   "source": [
    "# **LangChain - A Framework for developing applications powered by LLMs**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to LangChain\n",
    "    - What is LangChain?\n",
    "    - Why use LangChain?\n",
    "    - Architecture\n",
    "    - LangChain Packages\n",
    "2. Building Blocks\n",
    "    - Prompt Template\n",
    "    - Chat Model\n",
    "    - Output Parser\n",
    "    - Chains\n",
    "3. Case Study\n",
    "    - Building a Chat Prompt Template\n",
    "    - Building a Chat Model\n",
    "    - Building an Output Parser\n",
    "    - Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc16ab-4256-4832-9fd8-9d4b2949a4dd",
   "metadata": {},
   "source": [
    "## **Introduction to LangChain**\n",
    "\n",
    "### **What is LangChain?**\n",
    "LangChain is an open-source framework that acts like a toolkit and orchestration layer for building applications with Large Language Models.\n",
    "\n",
    "It simplifies the process of chaining together different components (like LLMs, data sources, and tools) to create complex, intelligent applications. It's designed to make prompt engineering more efficient and allow developers to adapt language models to specific business contexts.\n",
    "\n",
    "Imagine you want to build a truly smart application powered by a Large Language Model (LLM), like OpenAI's GPT or Google's Gemini. Simply sending a prompt to an LLM isn't usually enough for real-world scenarios. You often need to:\n",
    "\n",
    "1. **Get relevant information:** Your LLM might need to access external data (your documents, a database, the internet) to answer a user's question accurately.\n",
    "2. **Remember past conversations:** For a coherent dialogue, the LLM needs a memory of what was said before.\n",
    "3. **Perform actions:** Sometimes, the LLM needs to \"do\" something, like search the web, call an API, or run a calculator.\n",
    "4. **Structure its output:** You might want the LLM's response to be in a specific format (e.g., JSON, a bulleted list).\n",
    "\n",
    "This is where LangChain comes in.\n",
    "\n",
    "### **Why use LangChain?**\n",
    "- **Abstraction:** It provides high-level abstractions over common LLM functionalities, so you don't have to worry about the low-level API calls for every LLM.\n",
    "- **Modularity:** It offers modular components that you can mix and match to build various LLM-powered applications (chatbots, Q&A systems, content generators, agents).\n",
    "- **Integration:** It provides numerous integrations with different LLMs, vector databases, document loaders, and other tools.\n",
    "- **Orchestration:** It excels at orchestrating sequences of operations, allowing you to build multi-step workflows.\n",
    "\n",
    "### **Architecture**\n",
    "LangChain is a framework that consists of a number of packages.\n",
    "\n",
    "<img src=\"images/langchain_stack_updated.png\" width=\"500\" height=\"600\">\n",
    "\n",
    "### **LangChain Packages**\n",
    "**langchain-core**  \n",
    "This package contains base abstractions for different components and ways to compose them together. The interfaces for core components like **chat models**, **vector stores**, **tools** and more are defined here. No third-party integrations are defined here. The dependencies are very lightweight.\n",
    "\n",
    "**langchain**  \n",
    "The main langchain package contains chains and retrieval strategies that make up an application's cognitive architecture. These are NOT third-party integrations. All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\n",
    "\n",
    "**Integration packages**  \n",
    "Popular integrations have their own packages (e.g. langchain-openai, langchain-anthropic, etc) so that they can be properly versioned and appropriately lightweight.\n",
    "\n",
    "**langchain-community**  \n",
    "This package contains third-party integrations that are maintained by the LangChain community. Key integration packages are separated out (see above). This contains integrations for various components (chat models, vector stores, tools, etc). All dependencies in this package are optional to keep the package as lightweight as possible.\n",
    "\n",
    "**langgraph**  \n",
    "langgraph is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n",
    "\n",
    "LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52685f3-d9f4-44b8-8e6a-364dd6c4c7fa",
   "metadata": {},
   "source": [
    "## **Building Blocks**\n",
    "\n",
    "1. **Prompt Template**\n",
    "2. **Chat Model**\n",
    "3. **Output Parser**\n",
    "4. **Chains**\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b6f7c0-1249-4832-be76-2c218489f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install langchain\n",
    "\n",
    "# ! pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bafa1-110f-476f-88c1-03d6a77ed4c8",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "\n",
    "- Writing static strings as prompts quickly becomes unmanageable. We need a way to inject dynamic information. This is where Prompt Template comes in.\n",
    "- These are the objects that help you construct prompts dynamically by accepting input variables. Think of them as blueprints for your prompts.\n",
    "- There are two types: **PromptTemplate** and **ChatPromptTemplate**.\n",
    "- **Input** to the Prompt Template should be a **dictionary** containing raw user inputs.\n",
    "- **Output** of a Prompt Template will be a **string** or **list of chat messages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c510cec-a921-458a-a0ad-80fa09ccb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "                    messages = [\n",
    "                    (\"system\", \"You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. \"),\n",
    "                    (\"human\", \"What is {topic}?\"),\n",
    "                    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5def41-723e-4b44-bf8e-378bcc647445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec953ec-93a5-4b54-8958-877d95803c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"topic\": \"LangChain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb105e9-86e0-45f3-8983-566030d4bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. \n",
      "Human: What is LangChain?\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.invoke({\"topic\": \"LangChain\"}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f6bed-567d-45a5-9381-e1b194caab5d",
   "metadata": {},
   "source": [
    "### **Chat Models**\n",
    "\n",
    "LLMs handle various language operations such as translation, summarization, question answering, and content creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77b3d81e-56d7-428d-bbb2-690a25c7169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4fb816-ea03-415d-b184-8262c5a494a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is an open-source framework designed to facilitate the development of applications powered by large language models (LLMs). It provides a set of tools and abstractions that enable developers to build, manage, and optimize the workflow of LLM-based applications seamlessly. Here are some key features and components of LangChain:\\n\\n1. **Modularity**: LangChain promotes a modular approach, allowing developers to utilize different components like LLMs, prompts, and chains in a flexible and reusable manner.\\n\\n2. **Chains**: It allows users to create \"chains\" that are sequences of operations executed in a specific order. This can be useful for scenarios where multiple steps of processing are required, such as generating text based on user input, followed by additional processing or API calls.\\n\\n3. **Data Loaders**: LangChain includes functionality to load and manage data from various sources, making it easier to integrate external data into LLM applications.\\n\\n4. **Memory**: The framework supports memory management, enabling applications to maintain context or state across interactions, which is crucial for building more interactive and context-aware applications.\\n\\n5. **Agents**: LangChain allows the implementation of agents that can make decisions based on user input and environmental context, often employing external tools or services to accomplish tasks.\\n\\n6. **Interfacing with APIs**: It simplifies the integration of LLMs with various APIs, allowing for enhanced functionality and real-world application.\\n\\n7. **Customization**: Developers can customize the behavior of chains and agents to build specialized applications tailored to specific use cases.\\n\\nLangChain is particularly useful for building chatbots, automated content generation systems, question-answering frameworks, and other interactive applications that require intelligent language processing capabilities. Its flexibility and modularity make it a popular choice among developers working with LLMs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 361, 'prompt_tokens': 12, 'total_tokens': 373, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-eed1f729-30aa-4c68-9463-888ef1503de3-0', usage_metadata={'input_tokens': 12, 'output_tokens': 361, 'total_tokens': 373, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import OpenAI ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Creating a Prompt\n",
    "prompt = \"What is LangChain?\"\n",
    "\n",
    "# Printing the output of model\n",
    "model_response = chat_model.invoke(prompt)\n",
    "\n",
    "model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45960a9e-4e0a-4633-8e4f-eec1ded1b472",
   "metadata": {},
   "source": [
    "### **Output Parser**\n",
    "\n",
    "- LLMs primarily generate free-form text. However, in many applications, you need structured data (e.g., a list of items, a JSON object, a specific format). OutputParsers bridge this gap.\n",
    "- These are the objects that take the raw string output from an LLM and transform it into a more usable Python data structure (e.g., a list, a dictionary, a Pydantic object).\n",
    "- **Input** to the output parser should be an **AIMessage**.\n",
    "\n",
    "**The output of a ChatModel (and therefore, of this chain) is a AI Message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1648a0-d87d-4f43-8fbc-47a185e60022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Parsing\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82762c01-e6e0-4a1e-b070-1f051fb96704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is an open-source framework designed to facilitate the development of applications powered by large language models (LLMs). It provides a set of tools and abstractions that enable developers to build, manage, and optimize the workflow of LLM-based applications seamlessly. Here are some key features and components of LangChain:\\n\\n1. **Modularity**: LangChain promotes a modular approach, allowing developers to utilize different components like LLMs, prompts, and chains in a flexible and reusable manner.\\n\\n2. **Chains**: It allows users to create \"chains\" that are sequences of operations executed in a specific order. This can be useful for scenarios where multiple steps of processing are required, such as generating text based on user input, followed by additional processing or API calls.\\n\\n3. **Data Loaders**: LangChain includes functionality to load and manage data from various sources, making it easier to integrate external data into LLM applications.\\n\\n4. **Memory**: The framework supports memory management, enabling applications to maintain context or state across interactions, which is crucial for building more interactive and context-aware applications.\\n\\n5. **Agents**: LangChain allows the implementation of agents that can make decisions based on user input and environmental context, often employing external tools or services to accomplish tasks.\\n\\n6. **Interfacing with APIs**: It simplifies the integration of LLMs with various APIs, allowing for enhanced functionality and real-world application.\\n\\n7. **Customization**: Developers can customize the behavior of chains and agents to build specialized applications tailored to specific use cases.\\n\\nLangChain is particularly useful for building chatbots, automated content generation systems, question-answering frameworks, and other interactive applications that require intelligent language processing capabilities. Its flexibility and modularity make it a popular choice among developers working with LLMs.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd03071-2c3f-4f0b-8190-96ee8aebc4b0",
   "metadata": {},
   "source": [
    "### **Chains**\n",
    "\n",
    "- A chain is simply a sequence of operations where the output of one step becomes the input of the next. This allows you to build more complex and intelligent workflows than a single LLM call could achieve.\n",
    "- The `|` symbol chains together the different components feeds the output from one component as input into the next component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ca1ec79-76a1-4265-9606-fe69439cd794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the independent variable(s) and the dependent variable.\\n\\nIn simple linear regression, there is only one independent variable, while in multiple linear regression, there are multiple independent variables. The goal of linear regression is to find the best-fitting line or plane that describes the relationship between the independent and dependent variables.\\n\\nThe equation of a simple linear regression model can be written as:\\n\\ny = b0 + b1*x\\n\\nWhere:\\n- y is the dependent variable\\n- x is the independent variable\\n- b0 is the y-intercept (the value of y when x = 0)\\n- b1 is the slope (the change in y for a one-unit change in x)\\n\\nThe values of b0 and b1 are estimated using a method such as the least squares method, which minimizes the sum of the squared differences between the observed values and the values predicted by the model.\\n\\nLinear regression is widely used for making predictions, understanding relationships between variables, and determining the strength of those relationships. It is a fundamental tool in the field of statistics and data analysis.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.Prompt Template\n",
    "prompt = prompt_template.invoke({\"topic\": \"Linear Regression\"})\n",
    "\n",
    "# 2. Chat Model\n",
    "response = chat_model.invoke(prompt)\n",
    "\n",
    "# 3. Ouput Parser\n",
    "output_parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "596048ab-59d5-4471-b41d-73802c503074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. It involves the development of algorithms and models that allow computers to understand, interpret, generate, and respond to human language in a meaningful way.\\n\\nKey components and tasks in NLP include:\\n\\n1. **Text Processing**: This involves breaking down text into manageable components, such as sentences and words, often through techniques like tokenization, stemming, and lemmatization.\\n\\n2. **Sentiment Analysis**: Determining the emotional tone behind a series of words, which is often used to understand opinions in reviews, social media, and customer feedback.\\n\\n3. **Named Entity Recognition (NER)**: Identifying and categorizing key entities in the text, such as people, organizations, and locations.\\n\\n4. **Part-of-Speech Tagging**: Assigning parts of speech to each word (nouns, verbs, adjectives, etc.), which helps in understanding the grammatical structure of sentences.\\n\\n5. **Machine Translation**: Automatically translating text from one language to another, as seen with tools like Google Translate.\\n\\n6. **Text Summarization**: Producing a concise summary of a larger body of text while retaining its main ideas.\\n\\n7. **Question Answering**: Building systems that can answer questions posed in natural language.\\n\\n8. **Text Generation**: Producing coherent and contextually relevant text based on a prompt, as demonstrated by models like GPT (Generative Pre-trained Transformer).\\n\\nNLP combines computational linguistics, machine learning, and deep learning techniques to analyze and generate human language. It has applications in various industries, including customer service (chatbots), healthcare (analyzing clinical notes), finance (market sentiment analysis), and many more.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | chat_model | output_parser\n",
    "\n",
    "user_input = {\"topic\": \"NLP\"}\n",
    "\n",
    "chain.invoke(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930375ff-d9f7-4cf0-b4da-6fb16b39db7e",
   "metadata": {},
   "source": [
    "## **Case Study**\n",
    "\n",
    "**Create an AI Tutor App that uses Prompts and Chat internally to give Python Implementation tutorial for Data Science topics**\n",
    "\n",
    "Inorder to solve this, we will first create the following three components:\n",
    "1. Chat Prompt Template\n",
    "2. Chat Model\n",
    "3. Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28998a35-5ebe-4900-a5f2-d360271756d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\nYou are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python Implementation for topics asked by user.\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, template='Tell me a python implementation for {topic_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Building a Chat Prompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Constructing System Prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "You are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python Implementation for topics asked by user.\n",
    "\"\"\")\n",
    "\n",
    "# Constructing Human Prompt\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"Tell me a python implementation for {topic_name}.\")\n",
    "\n",
    "# Compiling Chat Prompt\n",
    "chat_prompt = ChatPromptTemplate(messages=[system_prompt, human_prompt])\n",
    "\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a471dd0b-fb92-47cf-b09c-8266405a66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a Chat Model\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6c48a5-b181-40b7-a8d0-f180b0a8bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building an Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8944772-4e6a-4fb5-a44a-2ebd29564846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaining All the components together\n",
    "\n",
    "chain = chat_prompt | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e52abfe-78d9-4a2a-a83b-c00fb93a91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is a step-by-step implementation of Logistic Regression in Python using a sample dataset:\n",
      "\n",
      "```python\n",
      "# Step 1: Import necessary libraries\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Step 2: Load the dataset\n",
      "# For this example, we'll use the famous Iris dataset from sklearn\n",
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = (iris.target == 0).astype(int)  # Converting classes to binary (1 if Iris-setosa else 0)\n",
      "\n",
      "# Step 3: Split the dataset into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Step 4: Create and train the Logistic Regression model\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Step 5: Make predictions on the test set\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Step 6: Evaluate the model\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(\"Accuracy:\", accuracy)\n",
      "\n",
      "# Step 7: Optionally, you can use the model to make predictions on new data\n",
      "new_data = np.array([[5.1, 3.5, 1.4, 0.2],  # Sample new data\n",
      "                     [4.9, 3.0, 1.4, 0.2]])\n",
      "predictions = model.predict(new_data)\n",
      "print(\"Predictions for new data:\", predictions)\n",
      "```\n",
      "\n",
      "This implementation demonstrates how to train a Logistic Regression model using the Iris dataset, make predictions, evaluate the model's accuracy, and use it for making predictions on new data points. Feel free to replace the dataset with your own data and adapt the code as needed!\n"
     ]
    }
   ],
   "source": [
    "## Calling the chain\n",
    "\n",
    "user_input = {\"topic_name\": \"Logistic Regression\"}\n",
    "\n",
    "output = chain.invoke(user_input)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
