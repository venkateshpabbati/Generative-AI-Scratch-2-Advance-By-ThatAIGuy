{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3462222a-c17e-443e-86d6-958b09067495",
   "metadata": {},
   "source": [
    "# **Templates - Making Prompts Dynamic and Reusable**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Templates\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7519e-8736-4f4f-b6d1-c565e0f377cf",
   "metadata": {},
   "source": [
    "## **Introduction to Templates**\n",
    "\n",
    "### **What are Prompt Templates?**\n",
    "- Writing **static strings as prompts quickly becomes unmanageable**. We need a way to inject dynamic information. This is where Prompt Template comes in.\n",
    "- These are the objects that help you **construct prompts dynamically by accepting input variables**. Think of them as blueprints for your prompts.\n",
    "- Templates **offer a more systematic approach to passing in variables to prompts** for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "<img src=\"images/langchain_model_io.jpg\">\n",
    "\n",
    "### **Why they are crucial?**\n",
    "1. **Reusability:** Define a template once and use it for many different inputs.\n",
    "2. **Consistency:** Ensure your prompts follow a specific structure every time, which helps LLMs perform better.\n",
    "3. **Readability:** Makes your code cleaner by separating prompt logic from other code.\n",
    "4. **Parameterization:** Easily insert varying information into the prompt.\n",
    "\n",
    "\n",
    "### **Types of Prompt Templates**\n",
    "1. **PromptTemplate:** Used for generic text completion models or when your prompt is a single string.\n",
    "2. **ChatPromptTemplate:** Specifically designed for chat models, allowing you to define sequences of `HumanMessage`, `AIMessage`, and `SystemMessage` templates. This is generally preferred for modern LLMs as most are fine-tuned for chat.\n",
    "\n",
    "### **Input and Output**\n",
    "- We can call a template with **.invoke()** method.\n",
    "- **Input** to the Prompt Template should be a **dictionary** containing raw user inputs.\n",
    "- **Output** of a Prompt Template will be a **string** or **list of chat messages**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd999c2-5a6b-49bb-a0d8-902ec9e72623",
   "metadata": {},
   "source": [
    "## **PromptTemplate**\n",
    "\n",
    "Used for generic text completion models or when your prompt is a single string.\n",
    "\n",
    "**Prompt Template**  \n",
    "- Prompt Templates are used to convert raw user input to a better input to the LLM.\n",
    "- Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
    "- A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
    "- LangChain provides tooling to create and work with prompt templates.\n",
    "- LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\n",
    "- Typically, language models expect the prompt to either be a string or else a list of chat messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fc53e-f5b9-4657-a32f-87b693d90d4d",
   "metadata": {},
   "source": [
    "### **Creating a PromptTemplate by Direct Instantiation**\n",
    "\n",
    "You can also instantiate PromptTemplate **directly by passing the template string** and **explicitly listing the input_variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b322dbc8-d2d1-42cb-8dc8-0f117bb5ae7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(template=\"Tell me a {adjective} joke about {content}.\")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22546da4-472f-4524-a04f-1e826f752c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the input variables\n",
    "\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f63bb6-dc92-4a62-bd85-7a61989b9321",
   "metadata": {},
   "source": [
    "### **Creating a PromptTemplate using .from_template()**\n",
    "\n",
    "`PromptTemplate.from_template(string_with_placeholders)` helps us create a PromptTemplate object from a string with placeholders (e.g., \"Tell me about {topic} in 200 words.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e2ef49-861a-4249-be5c-d36d06cdb69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bb94d5-c45c-4eed-9e74-43cd323cebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the input variables\n",
    "\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4357d6-0223-47ad-8d81-666d6f839958",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format()**\n",
    "\n",
    "**.format()** converts the PromptTemplate to a `String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbcb0059-7e24-4e0c-bd13-71fc8b6e6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62218d76-9b68-4af5-a812-1b85d6212ad6",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format_prompt()**\n",
    "\n",
    "**.format_prompt()** converts the PromptTempate to `StringPromptValue`. \n",
    "\n",
    "`PromptValues` can be converted to a string and list of messages with the help of `to_string()` and `to_messages()` respectively.\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58a1e07-59f4-4ff2-ab0d-2a87023dc6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "text='Tell me a funny joke about chickens.'\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a string i.e. StingPromptValue\n",
    "# PromptValue can be converted to either Strings or Messages\n",
    "\n",
    "prompt = prompt_template.format_prompt(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35cece20-d1bb-4a25-b75c-e57c1aa4ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "## We can use StingPromptValue and convert it to string using .to_string()\n",
    "\n",
    "prompt_string = prompt.to_string()\n",
    "\n",
    "print(type(prompt_string))\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aeec0c1-146a-433e-85e4-cb5ce5aa12e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "## We can use StingPromptValue and convert it to List of ChatMessages using .to_messages()\n",
    "\n",
    "prompt_messages = prompt.to_messages()\n",
    "\n",
    "print(type(prompt_messages))\n",
    "print(prompt_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc49847-fad8-4abb-a78a-c9bc15b563aa",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format_messages()**\n",
    "\n",
    "**.format_message()** converts the ChatPromptTemplate to list of `ChatMessages`. **It won't work with the PromptTemplate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd2d02b-1c5e-4845-97ab-9bcf8acb3e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: 'PromptTemplate' object has no attribute 'format_messages'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prompt = prompt_template.format_messages(adjective=\"funny\", content=\"chickens\")\n",
    "except:\n",
    "    print(\"AttributeError: 'PromptTemplate' object has no attribute 'format_messages'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f785e7-0125-4199-8c6c-92325fe64a4e",
   "metadata": {},
   "source": [
    "### **partial_variables**\n",
    "\n",
    "**partial_variables** in prompt templates are a powerful feature that allows you to \"pre-fill\" some of the placeholders in your prompt template.\n",
    "\n",
    "Think of `partial_variables` as **default function parameters**.\n",
    "\n",
    "**This is particularly useful in scenarios where:**\n",
    "1. **Some variables are static or known well in advance:** For example, a system instruction that's always the same, or a fixed format_instructions string from an OutputParser. This is covered later in the next chapter (i.e. 4. Output Parsing).\n",
    "2. **You're building complex chains where some information becomes available earlier:** You can partial a prompt as information becomes available, reducing the number of variables you need to pass around later in the chain.\n",
    "3. **You need to inject dynamic information from a function:** Instead of a static string, you can provide a function that will be called at the time of formatting to get the value for that variable. This is great for things like current date/time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83d4699-33d6-41ba-ad85-5971f0f47912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['content'], input_types={}, partial_variables={'adjective': 'funny'}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    partial_variables={\"adjective\": \"funny\"}\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ed8e2c-6c6a-49d1-9c23-5c20b661cc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "123edfc7-885a-4d50-ad3b-d73c536d440f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a dark joke about chickens.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(content=\"chickens\", adjective=\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d188007-d936-4b5e-970c-246931e57e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c1901-07b6-4835-8429-98a26ed667ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66706e19-5cc8-4eab-8339-5875f390a307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ab60b4-68a8-4c0c-864d-4eff3e7c0c29",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate**\n",
    "\n",
    "The prompt to chat models is a list of chat messages.\n",
    "\n",
    "Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba8600-1dbe-4006-8c5e-5ab5ae077c28",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate and from_template()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a736db-8c78-439a-8c94-5d6c2b7b4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(\n",
    "    \"What is {topic}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f7c21c-93ba-4cfb-ba5b-338835325024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85412fc6-a79e-4eb7-a98a-d776930a7837",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate - .format(), .format_messages() and .format_prompt()**\n",
    "\n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "- .format(): Converts the PromptTemplate to `String`\n",
    "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`\n",
    "- .format_prompt(): Converts the PromptTempate to `ChatPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a49ce6-dc0d-4ad5-88e7-01315c61d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Human: What is machine learning?\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = chat_template.format(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e079d5a5-a54c-4dbe-9e26-329d0c81b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# fomat_messages() return list of chat messages\n",
    "\n",
    "prompt = chat_template.format_messages(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e119b0-2261-4d38-a01c-eff90fb2dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "# PromptValue can be converted to either Strings or Chat Messages\n",
    "\n",
    "prompt = chat_template.format_prompt(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c39e2c-2b73-49ad-9c05-fb778f60796b",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate and from_messages()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927ffc3f-c3c2-4ef2-9edf-1c7d77bb8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07784763-9403-4a92-83af-c9be3800e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'user_input']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b093e8-2626-49a1-9e5a-69abaaa2e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI bot. Your name is Bob.\\nHuman: Hello, how are you doing?\\nAI: I'm doing well, thanks!\\nHuman: What is your name?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1ae895-54ad-4eaa-a80c-149bb54dbecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_messages() returns chat messages \n",
    "\n",
    "chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02998b4b-db2a-4c66-a0b6-6b39c5c5132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "\n",
    "chat_template.format_prompt(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44455b32-c2ec-4b95-b9b8-4742ae5e2f95",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate, partial_variables and .invoke()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb153c83-8650-4d0b-ba84-10eea476428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={'name': 'Alice'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ], \n",
    "    partial_variables={\"name\": \"Alice\"}\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c336781-66cf-47fa-94aa-3d3292a3215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"user_input\": \"What is your name?\"}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914b508a-9f7a-474d-a674-3b7a1e04a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI bot. Your name is Alice.\n",
      "Human: Hello, how are you doing?\n",
      "AI: I'm doing well, thanks!\n",
      "Human: What is your name?\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke(raw_input).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "581cf03f-8e4e-42a1-86d0-af4d946895b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke(raw_input).to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b635a-513b-4ae9-abf4-321206397c65",
   "metadata": {},
   "source": [
    "## **Passing values to placeholders using .invoke()**\n",
    "\n",
    "`PromptTemplate` and `ChatPromptTemplate` implement the Runnable interface, the basic building block of the **LangChain Expression Language (LCEL)**. This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls.\n",
    "\n",
    "**Using `invoke()`:**  \n",
    "`PromptTemplate` **accepts a dictionary (of the prompt variables)** and returns a `StringPromptValue`.  \n",
    "\n",
    "A `ChatPromptTemplate` **accepts a dictionary** and returns a `ChatPromptValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22365e1d-f981-4490-b127-2587d251e502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1301a7-5c43-4a8e-ac95-5bec5aa5b9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce91275-199a-47c1-af5a-b3719012a436",
   "metadata": {},
   "source": [
    "## **.pretty_print() Messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dade8242-f1e0-47fc-b3bc-b3d530cd5ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful AI bot. Your name is Alice.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, how are you doing?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm doing well, thanks!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is your name?\n"
     ]
    }
   ],
   "source": [
    "for msg in chat_template.invoke(raw_input).to_messages():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f36f2-9f16-4239-9572-a9ca72eec8df",
   "metadata": {},
   "source": [
    "## **Use Case**\n",
    "\n",
    "**Designing ChatPromptTemplate using SystemMessagePromptTemplate, HumanMessagePromptTemplate and AIMessagePromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02925376-d6fb-4055-a254-4d1fb9b9384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "# System Template\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template = HumanMessagePromptTemplate.from_template(\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Template\n",
    "ai_prompt_template = AIMessagePromptTemplate.from_template(\"I'm doing well, thanks!\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template_input = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages=[ system_prompt_template, \n",
    "               human_prompt_template, \n",
    "               ai_prompt_template, \n",
    "               human_prompt_template_input ],\n",
    "    partial_variables={\"name\": \"Alice\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f116116-b4aa-4200-b5b1-d4dd240d4eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful AI bot. Your name is Alice.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, how are you doing?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm doing well, thanks!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is your name?\n"
     ]
    }
   ],
   "source": [
    "raw_input = {\"user_input\": \"What is your name?\"}\n",
    "\n",
    "for msg in chat_template.invoke(raw_input).to_messages():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d6a4a-ba99-40a2-9e94-9fc77392f67b",
   "metadata": {},
   "source": [
    "### **In above implementation, we can use HumanMessage and AIMessage as shown below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a176193c-e70d-4722-885f-acb41c633ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'user_input']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# System Template\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt = HumanMessage(\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Template\n",
    "ai_prompt = AIMessage(\"I'm doing well, thanks!\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages=[system_prompt_template, human_prompt, ai_prompt, human_prompt_template]\n",
    ")\n",
    "\n",
    "print(chat_template.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07b9cb5b-333f-4ac3-98c0-5bb05b21a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful AI bot. Your name is Bob.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, how are you doing?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm doing well, thanks!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is your name?\n"
     ]
    }
   ],
   "source": [
    "raw_input = {\"name\": \"Bob\", \"user_input\": \"What is your name?\"}\n",
    "\n",
    "for msg in chat_template.invoke(raw_input).to_messages():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439d488-ec08-4d1d-9ce3-eb5189ab362b",
   "metadata": {},
   "source": [
    "## **MessagesPlaceholder**\n",
    "\n",
    "A placeholder which can be used to pass in a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9718cfa7-6c90-4166-a5b8-1b15ee476248",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chat_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessagesPlaceholder\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m MessagesPlaceholder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# raises KeyError\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/prompts/chat.py:240\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[BaseMessage]:\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        ValueError: If variable is not a list of messages.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    238\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name, [])\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    243\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m         )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chat_history'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = MessagesPlaceholder(\"chat_history\")\n",
    "prompt.format_messages() # raises KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "554af0a0-6655-49bc-bfe8-67e9b9d03b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = MessagesPlaceholder(\"chat_history\", optional=True)\n",
    "\n",
    "prompt.format_messages() # returns empty list []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07e0b78e-0b16-4b01-9c23-760356777cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = prompt.format_messages(\n",
    "                chat_history=[\n",
    "                    (\"system\", \"You are an AI assistant.\"),\n",
    "                    (\"human\", \"Hello!\"),\n",
    "                ]\n",
    ")\n",
    "\n",
    "message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da1a00e1-2888-4277-8f56-2f311a5dfbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an AI assistant.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "for msg in message_list:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8757b1c0-1874-4a21-9d78-80a916708087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# Limiting the number of messages\n",
    "\n",
    "prompt = MessagesPlaceholder(\"chat_history\", optional=True, n_messages=1)\n",
    "\n",
    "message_list = prompt.format_messages(\n",
    "                chat_history=[\n",
    "                    (\"system\", \"You are an AI assistant.\"),\n",
    "                    (\"human\", \"Hello!\"),\n",
    "                ]\n",
    ")\n",
    "\n",
    "for msg in message_list:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f338db3d-1070-473b-8b8c-a0159d25c38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x126f1be50>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages=[ (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), \n",
    "     MessagesPlaceholder(variable_name=\"chat_history\", optional=True), \n",
    "     (\"human\", \"{user_input}\") ]\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbc6364a-4b7c-4ffe-af5c-f706e41376e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"name\": \"Bob\", \"user_input\": \"What is your name?\"}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c595134-abe0-4d09-a6c3-9a421915619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human Message\n",
    "human_prompt = HumanMessage(content=\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Message\n",
    "ai_prompt = AIMessage(content=\"I'm doing well, thanks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f0559cd-37e0-4e35-820d-27484090f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"name\": \"Bob\", \"user_input\": \"What is your name?\", \"chat_history\": [human_prompt, ai_prompt]}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bccd6e8-a2d5-435c-a80f-bdf96713a101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.invoke(raw_input).to_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
