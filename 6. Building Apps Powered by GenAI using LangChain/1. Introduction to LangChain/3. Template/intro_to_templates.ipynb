{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3462222a-c17e-443e-86d6-958b09067495",
   "metadata": {},
   "source": [
    "# **Templates - Making Prompts Dynamic and Reusable**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Templates\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7519e-8736-4f4f-b6d1-c565e0f377cf",
   "metadata": {},
   "source": [
    "## **Introduction to Templates**\n",
    "\n",
    "### **What are Prompt Templates?**\n",
    "- Writing **static strings as prompts quickly becomes unmanageable**. We need a way to inject dynamic information. This is where Prompt Template comes in.\n",
    "- These are the objects that help you **construct prompts dynamically by accepting input variables**. Think of them as blueprints for your prompts.\n",
    "- Templates **offer a more systematic approach to passing in variables to prompts** for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "<img src=\"images/langchain_model_io.jpg\">\n",
    "\n",
    "### **Why they are crucial?**\n",
    "1. **Reusability:** Define a template once and use it for many different inputs.\n",
    "2. **Consistency:** Ensure your prompts follow a specific structure every time, which helps LLMs perform better.\n",
    "3. **Readability:** Makes your code cleaner by separating prompt logic from other code.\n",
    "4. **Parameterization:** Easily insert varying information into the prompt.\n",
    "\n",
    "\n",
    "### **Types of Prompt Templates**\n",
    "1. **PromptTemplate (for single-string prompts):** Used for generic text completion models or when your prompt is a single string.\n",
    "2. **ChatPromptTemplate (for structured chat messages):** Specifically designed for chat models, allowing you to define sequences of `HumanMessage`, `AIMessage`, and `SystemMessage` templates. This is generally preferred for modern LLMs as most are fine-tuned for chat.\n",
    "\n",
    "### **Input and Output**\n",
    "- We can call a template with **.invoke()** method.\n",
    "- **Input** to the Prompt Template should be a **dictionary** containing raw user inputs.\n",
    "- **Output** of a Prompt Template will be a **string** or **list of chat messages**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd999c2-5a6b-49bb-a0d8-902ec9e72623",
   "metadata": {},
   "source": [
    "## **PromptTemplate (for single-string prompts)**\n",
    "\n",
    "Used for generic text completion models or when your prompt is a single string.\n",
    "\n",
    "**Prompt Template**  \n",
    "- Prompt Templates are used to convert raw user input to a better input to the LLM.\n",
    "- Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
    "- A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
    "- LangChain provides tooling to create and work with prompt templates.\n",
    "- LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\n",
    "- Typically, language models expect the prompt to either be a string or else a list of chat messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f63bb6-dc92-4a62-bd85-7a61989b9321",
   "metadata": {},
   "source": [
    "### **Creating a PromptTemplate using .from_template()**\n",
    "\n",
    "`PromptTemplate.from_template(string_with_placeholders)` helps us create a PromptTemplate object from a string with placeholders (e.g., \"Tell me about {topic} in 200 words.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e2ef49-861a-4249-be5c-d36d06cdb69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bb94d5-c45c-4eed-9e74-43cd323cebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the input variables\n",
    "\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fc53e-f5b9-4657-a32f-87b693d90d4d",
   "metadata": {},
   "source": [
    "### **Creating a PromptTemplate by Direct Instantiation**\n",
    "\n",
    "You can also instantiate PromptTemplate **directly by passing the template string** and **explicitly listing the input_variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b322dbc8-d2d1-42cb-8dc8-0f117bb5ae7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(template=\"Tell me a {adjective} joke about {content}.\")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "22546da4-472f-4524-a04f-1e826f752c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the input variables\n",
    "\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4357d6-0223-47ad-8d81-666d6f839958",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format()**\n",
    "\n",
    "**.format()** converts the PromptTemplate to a `String`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbcb0059-7e24-4e0c-bd13-71fc8b6e6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62218d76-9b68-4af5-a812-1b85d6212ad6",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format_prompt()**\n",
    "\n",
    "**.format_prompt()** converts the PromptTempate to `StringPromptValue`. \n",
    "\n",
    "`PromptValues` can be converted to a string and list of messages with the help of `to_string()` and `to_messages()` respectively.\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c58a1e07-59f4-4ff2-ab0d-2a87023dc6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "text='Tell me a funny joke about chickens.'\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a string i.e. StingPromptValue\n",
    "# PromptValue can be converted to either Strings or Messages\n",
    "\n",
    "prompt = prompt_template.format_prompt(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35cece20-d1bb-4a25-b75c-e57c1aa4ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "## We can use StingPromptValue and convert it to string using .to_string()\n",
    "\n",
    "prompt_string = prompt.to_string()\n",
    "\n",
    "print(type(prompt_string))\n",
    "print(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6aeec0c1-146a-433e-85e4-cb5ce5aa12e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "## We can use StingPromptValue and convert it to List of ChatMessages using .to_messages()\n",
    "\n",
    "prompt_messages = prompt.to_messages()\n",
    "\n",
    "print(type(prompt_messages))\n",
    "print(prompt_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc49847-fad8-4abb-a78a-c9bc15b563aa",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format_messages()**\n",
    "\n",
    "**.format_message()** converts the ChatPromptTemplate to list of `ChatMessages`. **It won't work with the PromptTemplate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9fd2d02b-1c5e-4845-97ab-9bcf8acb3e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: 'PromptTemplate' object has no attribute 'format_messages'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prompt = prompt_template.format_messages(adjective=\"funny\", content=\"chickens\")\n",
    "except:\n",
    "    print(\"AttributeError: 'PromptTemplate' object has no attribute 'format_messages'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f785e7-0125-4199-8c6c-92325fe64a4e",
   "metadata": {},
   "source": [
    "### **Passing partial_variables**\n",
    "\n",
    "**partial_variables** in prompt templates are a powerful feature that allows you to \"pre-fill\" some of the placeholders in your prompt template.\n",
    "\n",
    "Think of `partial_variables` as **default function parameters**.\n",
    "\n",
    "**This is particularly useful in scenarios where:**\n",
    "1. **Some variables are static or known well in advance:** For example, a system instruction that's always the same, or a fixed format_instructions string from an OutputParser. This is covered later in the next chapter (i.e. 4. Output Parsing).\n",
    "2. **You're building complex chains where some information becomes available earlier:** You can partial a prompt as information becomes available, reducing the number of variables you need to pass around later in the chain.\n",
    "3. **You need to inject dynamic information from a function:** Instead of a static string, you can provide a function that will be called at the time of formatting to get the value for that variable. This is great for things like current date/time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f83d4699-33d6-41ba-ad85-5971f0f47912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['content'], input_types={}, partial_variables={'adjective': 'funny'}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Method A: Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    partial_variables={\"adjective\": \"funny\"}\n",
    ")\n",
    "\n",
    "# Method B: Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\",\n",
    "    partial_variables={\"adjective\": \"funny\"}\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56ed8e2c-6c6a-49d1-9c23-5c20b661cc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "123edfc7-885a-4d50-ad3b-d73c536d440f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a dark joke about chickens.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(content=\"chickens\", adjective=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae40df-eb59-43a6-b7ed-772383f200b8",
   "metadata": {},
   "source": [
    "### **Passing partial_variables in an existing template using .partial()**\n",
    "\n",
    "Use the .partial() method on an existing template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90d8e9ca-83dd-4529-8785-6e4f11be7fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-06-10 18:04:44'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Function to get the current date and time\n",
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "get_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d88bb19d-97b7-4a00-aa0b-21c56501a71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['current_date', 'topic'], input_types={}, partial_variables={}, template=\"Today's date is {current_date}. Tell me something interesting about {topic}.\")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Today's date is {current_date}. Tell me something interesting about {topic}.\",\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a2f14203-a168-4d4a-bd27-25e1815d3b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={'current_date': <function get_current_datetime at 0x114680790>}, template=\"Today's date is {current_date}. Tell me something interesting about {topic}.\")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declaring date as a partial variable using a function\n",
    "# NOTE: passing the function without using paranthesis will help in dynamically generating 'current_date'\n",
    "\n",
    "date_prompt = prompt_template.partial(current_date=get_current_datetime)\n",
    "\n",
    "date_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8f29607e-86f1-43ed-8c9b-04c2f8fd78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date is 2025-06-10 18:11:43. Tell me something interesting about outer space.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the prompt\n",
    "formatted_prompt = date_prompt.format(topic=\"outer space\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44861e01-8f13-4d31-8298-b83babe4d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date is 2025-06-10 18:11:50. Tell me something interesting about oceans.\n"
     ]
    }
   ],
   "source": [
    "# Run it again, the date will update\n",
    "formatted_prompt_again = date_prompt.format(topic=\"oceans\")\n",
    "print(formatted_prompt_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b7d47-7ae9-47d6-adfa-62c308189f4b",
   "metadata": {},
   "source": [
    "## **The \"Templates\" vs The \"Concrete Messages\"**\n",
    "\n",
    "`*MessagePromptTemplate` is like a form with empty fields (e.g., \"Customer Name: ______, Product: ______\"). You define the fields and their labels.\n",
    "\n",
    "`*Message` is like a filled-out form (e.g., \"Customer Name: John Doe, Product: Laptop\"). It has all the information specific to that one instance.\n",
    "\n",
    "### **Templates**\n",
    "These are classes that represent templates for specific types of messages (System, Human, AI). They define the structure and content of a message, often including placeholders ({variable_name}) that need to be filled in later. Example:\n",
    "- SystemMessagePromptTemplate\n",
    "- HumanMessagePromptTemplate\n",
    "- AIMessagePromptTemplate\n",
    "\n",
    "They are not directly sent to the LLM. They first need to be \"formatted\" or \"invoked\" to produce concrete `*Message` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f71fdee7-56ed-4478-a61e-1ae69bd76cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='You are a helpful assistant specialized in {topic}.'), additional_kwargs={})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant specialized in {topic}.\"\n",
    ")\n",
    "\n",
    "system_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "05735a5a-5e0a-4aae-b3aa-8b1a3abb8668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['concept'], input_types={}, partial_variables={'user_name': 'ThatAIGuy'}, template='My name is {user_name}. Can you explain {concept}?'), additional_kwargs={})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"My name is {user_name}. Can you explain {concept}?\", \n",
    "    partial_variables={\"user_name\":\"ThatAIGuy\"}\n",
    ")\n",
    "\n",
    "human_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5faf9e3-cce6-4e3f-af4a-7465be78cc21",
   "metadata": {},
   "source": [
    "### **Concrete Messages**\n",
    "These are actual, concrete message objects that contain the final, filled-in text content and their respective roles (system, human, AI). They represent a single turn or instruction in a conversation. Example:\n",
    "- SystemMessage\n",
    "- HumanMessage\n",
    "- AIMessage\n",
    "\n",
    "They are the \"data\" that the LLM processes.\n",
    "\n",
    "They do not contain placeholders ({}). Their content is fully resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7070b4da-3078-421c-89c9-97459eac54b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='You are a polite and friendly chatbot.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_instruction = SystemMessage(content=\"You are a polite and friendly chatbot.\")\n",
    "\n",
    "system_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "87e1e247-79c9-401e-9bf5-3d2ec2c02697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='What is the weather like today?', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "user_query = HumanMessage(content=\"What is the weather like today?\")\n",
    "\n",
    "user_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7b840a4a-3026-446d-ba3f-9c8cebde40f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, I don't have access to real-time weather information.\", additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "ai_response = AIMessage(content=\"I'm sorry, I don't have access to real-time weather information.\")\n",
    "\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab60b4-68a8-4c0c-864d-4eff3e7c0c29",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate (for structured chat messages)**\n",
    "\n",
    "This class is the preferred way to create prompts for modern chat-based LLMs. \n",
    "\n",
    "You provide a list of message \"templates,\" where each template represents a turn in the conversation and can have a specific role (system, human, ai) and placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba8600-1dbe-4006-8c5e-5ab5ae077c28",
   "metadata": {},
   "source": [
    "### **Creating a ChatPromptTemplate using .from_template()**\n",
    "\n",
    "Even though this method is available for ChatPromptTemplate, but it's important to understand its limitation: it creates a single human message template. It's useful if you only have one dynamic part from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b4a736db-8c78-439a-8c94-5d6c2b7b4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(\n",
    "    \"Tell me about {topic_name}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51f7c21c-93ba-4cfb-ba5b-338835325024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_name']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c39e2c-2b73-49ad-9c05-fb778f60796b",
   "metadata": {},
   "source": [
    "### **Creating a ChatPromptTemplate using .from_messages()**\n",
    "\n",
    "This is the most versatile and recommended way to create a ChatPromptTemplate. You provide a list of message \"templates,\" where each template represents a turn in the conversation and can have a specific role (system, human, ai) and placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "927ffc3f-c3c2-4ef2-9edf-1c7d77bb8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Option A: Using message objects explicitly\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {bot_name}.\", \n",
    "                                                  partial_variables={\"bot_name\": \"ThatAIGuy\"}),\n",
    "        HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "        AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"Tell me about {topic_name}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "07784763-9403-4a92-83af-c9be3800e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_name']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "32e44000-da61-4928-890c-57019cf1c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Option B: Using tuples (shorthand) - (role_string, template_string)\n",
    "# This is often more concise and very common.\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {bot_name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"Tell me about {topic_name}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e47e22ef-0e42-46c8-875d-fb089ce5d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={'bot_name': 'ThatAIGuy'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {bot_name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, template='Tell me about {topic_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_chat_template = chat_template.partial(bot_name=\"ThatAIGuy\")\n",
    "\n",
    "partial_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9872b12f-48b5-4c95-9ed6-9630a72198d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_name']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_chat_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85412fc6-a79e-4eb7-a98a-d776930a7837",
   "metadata": {},
   "source": [
    "### **Passing values to placeholder using .format(), .format_messages() and .format_prompt()**\n",
    "\n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "- .format(): Converts the PromptTemplate to `String`\n",
    "- .format_prompt(): Converts the PromptTempate to `ChatPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`.\n",
    "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26b093e8-2626-49a1-9e5a-69abaaa2e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI bot. Your name is Bob.\\nHuman: Hello, how are you doing?\\nAI: I'm doing well, thanks!\\nHuman: Tell me about LangChain.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format(bot_name=\"Bob\", topic_name=\"LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02998b4b-db2a-4c66-a0b6-6b39c5c5132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about LangChain.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_prompt() returns a ChatPromptValue()\n",
    "\n",
    "chat_template.format_prompt(bot_name=\"Bob\", topic_name=\"LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad1ae895-54ad-4eaa-a80c-149bb54dbecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Tell me about LangChain.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_messages() returns chat messages \n",
    "\n",
    "chat_template.format_messages(bot_name=\"Bob\", topic_name=\"LangChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a560e34-243f-40d1-aed4-60dbee4c3e08",
   "metadata": {},
   "source": [
    "### **Creating a ChatPromptTemplate by Direct Instantiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "59c1bd23-df72-489c-92c6-9a8bf1382280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={'bot_name': 'Alice'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {bot_name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], input_types={}, partial_variables={}, template='Tell me about {topic_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {bot_name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"Tell me about {topic_name}.\"),\n",
    "    ],\n",
    "    partial_variables={\"bot_name\": \"Alice\"}\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22a49ce6-dc0d-4ad5-88e7-01315c61d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: You are a helpful AI bot. Your name is Alice.\n",
      "Human: Hello, how are you doing?\n",
      "AI: I'm doing well, thanks!\n",
      "Human: Tell me about machine learning.\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = chat_template.format(topic_name=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "98e119b0-2261-4d38-a01c-eff90fb2dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about machine learning.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "# PromptValue can be converted to either Strings or Chat Messages\n",
    "\n",
    "prompt = chat_template.format_prompt(topic_name=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e079d5a5-a54c-4dbe-9e26-329d0c81b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about machine learning.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# fomat_messages() return list of chat messages\n",
    "\n",
    "prompt = chat_template.format_messages(topic_name=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b635a-513b-4ae9-abf4-321206397c65",
   "metadata": {},
   "source": [
    "## **Using LCEL methods**\n",
    "\n",
    "### **.invoke()**\n",
    "\n",
    "`PromptTemplate` and `ChatPromptTemplate` implement the Runnable interface, the basic building block of the **LangChain Expression Language (LCEL)**. This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls.\n",
    "\n",
    "**Using `invoke()`:**  \n",
    "`PromptTemplate` **accepts a dictionary (of the prompt variables)** and returns a `StringPromptValue`.  \n",
    "\n",
    "A `ChatPromptTemplate` **accepts a dictionary (of the prompt variables)** and returns a `ChatPromptValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6c336781-66cf-47fa-94aa-3d3292a3215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about LangChain.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.invoke({\"topic_name\": \"LangChain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "914b508a-9f7a-474d-a674-3b7a1e04a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI bot. Your name is Alice.\n",
      "Human: Hello, how are you doing?\n",
      "AI: I'm doing well, thanks!\n",
      "Human: Tell me about LangChain.\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke({\"topic_name\": \"LangChain\"}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "581cf03f-8e4e-42a1-86d0-af4d946895b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about LangChain.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke({\"topic_name\": \"LangChain\"}).to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce91275-199a-47c1-af5a-b3719012a436",
   "metadata": {},
   "source": [
    "### **.pretty_print()**\n",
    "\n",
    "The purpose of `.pretty_print()` is to display a human-readable, graphical representation of your chain's structure and flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e8829506-7710-4a2c-9dca-124f83650742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful AI bot. Your name is \u001b[33;1m\u001b[1;3m{bot_name}\u001b[0m.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, how are you doing?\n",
      "\n",
      "==================================\u001b[1m AI Message \u001b[0m==================================\n",
      "\n",
      "I'm doing well, thanks!\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me about \u001b[33;1m\u001b[1;3m{topic_name}\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "chat_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dade8242-f1e0-47fc-b3bc-b3d530cd5ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful AI bot. Your name is Alice.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, how are you doing?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm doing well, thanks!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me about LangChain.\n"
     ]
    }
   ],
   "source": [
    "for msg in chat_template.invoke({\"topic_name\": \"LangChain\"}).to_messages():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439d488-ec08-4d1d-9ce3-eb5189ab362b",
   "metadata": {},
   "source": [
    "## **MessagesPlaceholder**\n",
    "\n",
    "### **What is MessagesPlaceholder?**\n",
    "At its core, `MessagesPlaceholder` is a special type of \"placeholder\" within a `ChatPromptTemplate` that is designed to accept a list of `BaseMessage` objects.\n",
    "\n",
    "`MessagesPlaceholder` provides the exact slot in your `ChatPromptTemplate` where this evolving list of messages can be inserted.\n",
    "\n",
    "### **Regular Placeholder vs MessagePlaceholder**\n",
    "Unlike a regular placeholder like {variable_name} which expects a single string or simple value, `MessagesPlaceholder` expects a sequence of messages (like HumanMessage, AIMessage, SystemMessage).\n",
    "\n",
    "### **How it Works?**\n",
    "\n",
    "When you include MessagesPlaceholder in your ChatPromptTemplate.from_messages() or ChatPromptTemplate() definition:\n",
    "1. You give it a variable_name (e.g., \"chat_history\").\n",
    "2. When you invoke() the ChatPromptTemplate (or a chain containing it), you must pass a key in your input dictionary that matches this variable_name. The value for this key must be a list of BaseMessage objects.\n",
    "3. LangChain then takes this list of messages and inserts them directly into the position specified by MessagesPlaceholder within the final list of messages sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "438a560f-ce4b-4e9c-b4ca-e0ee2415c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Simulate some pre-existing chat history\n",
    "current_chat_history = [\n",
    "    HumanMessage(content=\"What's your favorite color?\"),\n",
    "    AIMessage(content=\"As an AI, I don't have feelings or preferences, so I don't have a favorite color.\"),\n",
    "    HumanMessage(content=\"Oh, I see. What's your favorite animal then?\"),\n",
    "    AIMessage(content=\"Similarly, I don't have personal experiences to develop preferences for animals.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "48e76440-213e-4a00-8677-d2c46e78a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful and knowledgeable assistant.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{new_question}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "\n",
    "# Define a ChatPromptTemplate that includes MessagesPlaceholder\n",
    "chat_prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful and knowledgeable assistant.\"),\n",
    "        # This is where the magic happens: insert the chat history\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        # The new human message for the current turn\n",
    "        HumanMessagePromptTemplate.from_template(\"{new_question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt_with_history.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b8a205a0-3c53-497c-bf3f-0b25eabfd8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful and knowledgeable assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's your favorite color?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't have feelings or preferences, so I don't have a favorite color.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Oh, I see. What's your favorite animal then?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Similarly, I don't have personal experiences to develop preferences for animals.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you summarize our conversation so far?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When invoking, pass the actual chat_history list and the new question\n",
    "\n",
    "formatted_messages = chat_prompt_with_history.invoke({\n",
    "    \"chat_history\": current_chat_history,\n",
    "    \"new_question\": \"Can you summarize our conversation so far?\"\n",
    "})\n",
    "\n",
    "formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cc24fd58-b02c-45d8-a56d-b1b56a8c7109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful and knowledgeable assistant.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's your favorite color?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As an AI, I don't have feelings or preferences, so I don't have a favorite color.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Oh, I see. What's your favorite animal then?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Similarly, I don't have personal experiences to develop preferences for animals.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you summarize our conversation so far?\n"
     ]
    }
   ],
   "source": [
    "for msg in formatted_messages.to_messages():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a9590-1291-47f6-aa9b-10435d124111",
   "metadata": {},
   "source": [
    "### **Making MessagesPlaceholder Optional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ffaac5e9-f9c7-4085-9ce5-997b7a7bd503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: Input to ChatPromptTemplate is missing variables {'chat_history'}.\n"
     ]
    }
   ],
   "source": [
    "user_input = {\"new_question\": \"Hi, My name is ThatAIGuy.\"}\n",
    "\n",
    "try:\n",
    "    chat_prompt_with_history.invoke(user_input)\n",
    "except:\n",
    "    print(\"KeyError: Input to ChatPromptTemplate is missing variables {'chat_history'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7d1f96d5-bef6-40ef-b2d6-2b3a9cd513a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful and knowledgeable assistant.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{new_question}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define a ChatPromptTemplate that includes MessagesPlaceholder\n",
    "chat_prompt_with_history = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful and knowledgeable assistant.\"),\n",
    "        # This is where the magic happens: insert the chat history\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "        # The new human message for the current turn\n",
    "        HumanMessagePromptTemplate.from_template(\"{new_question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt_with_history.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b383c7ed-f50c-4119-b5a4-939f784c34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful and knowledgeable assistant.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, My name is ThatAIGuy.\n"
     ]
    }
   ],
   "source": [
    "user_input = {\"new_question\": \"Hi, My name is ThatAIGuy.\"}\n",
    "\n",
    "for msg in chat_prompt_with_history.invoke(user_input).to_messages():\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a6447-5342-4606-8ccd-65c2f935dc3b",
   "metadata": {},
   "source": [
    "### **Limiting the number of messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "629363fe-811d-464b-90b5-71a97ee2b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Simulate some pre-existing chat history\n",
    "current_chat_history = [\n",
    "    HumanMessage(content=\"What's your favorite color?\"),\n",
    "    AIMessage(content=\"As an AI, I don't have feelings or preferences, so I don't have a favorite color.\"),\n",
    "    HumanMessage(content=\"Oh, I see. What's your favorite animal then?\"),\n",
    "    AIMessage(content=\"Similarly, I don't have personal experiences to develop preferences for animals.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8757b1c0-1874-4a21-9d78-80a916708087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful and knowledgeable assistant.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{new_question}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define a ChatPromptTemplate that includes MessagesPlaceholder\n",
    "chat_prompt_with_history = ChatPromptTemplate(\n",
    "    messages = [ SystemMessage(content=\"You are a helpful and knowledgeable assistant.\"),\n",
    "                # This is where the magic happens: insert the chat history\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\", optional=True, n_messages=2),\n",
    "                # The new human message for the current turn\n",
    "                HumanMessagePromptTemplate.from_template(\"{new_question}\"),\n",
    "                ] \n",
    ")\n",
    "\n",
    "chat_prompt_with_history.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0a98f3e6-7c63-43c2-a082-b2c492d069c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful and knowledgeable assistant.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Oh, I see. What's your favorite animal then?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Similarly, I don't have personal experiences to develop preferences for animals.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you summarize our conversation so far?\n"
     ]
    }
   ],
   "source": [
    "# When invoking, pass the actual chat_history list and the new question\n",
    "\n",
    "formatted_messages = chat_prompt_with_history.invoke({\n",
    "    \"chat_history\": current_chat_history,\n",
    "    \"new_question\": \"Can you summarize our conversation so far?\"\n",
    "})\n",
    "\n",
    "for msg in formatted_messages.to_messages():\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
