{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07cf546-9c70-461d-934f-316a31db96f0",
   "metadata": {},
   "source": [
    "# **Introduction to VectorDB**\n",
    "\n",
    "## **What's Covered?**\n",
    "\n",
    "1. What is a Vector Database?\n",
    "2. Why Do We Need Vector Databases?\n",
    "3. Applications\n",
    "4. How are Vectors Stored?\n",
    "5. What is indexing in VectorDBs?\n",
    "6. Two Major Categories of Indexing\n",
    "7. Flat (brute-force) index\n",
    "8. Inverted File Index (IVF)\n",
    "9. Hierarchical Navigable Small World (HNSW)\n",
    "10. Major Vector Database Comparison\n",
    "\n",
    "## **What is a Vector Database?**\n",
    "A Vector Database (VectorDB) is a specialized type of database designed to store, manage, and search vector embeddings—high-dimensional representations of data that capture the semantic meaning of unstructured inputs like text, images, audio, or even code.\n",
    "\n",
    "These embeddings are typically generated using machine learning models (e.g., transformers, sentence encoders, vision models), and they enable semantic similarity search—finding items that are \"meaningfully\" similar rather than just syntactically similar.\n",
    "\n",
    "## **Why Do We Need Vector Databases?**\n",
    "Traditional databases are optimized for exact matches or range queries on structured data (numbers, dates, text fields). However, with the rise of AI and Generative AI, we now deal heavily with unstructured data (e.g., user messages, support tickets, images), where meaning matters more than matching exact words or values.\n",
    "\n",
    "Let’s consider this:\n",
    "> You search “how to get my money back?”  \n",
    "> A traditional search engine looks for documents with those exact words.  \n",
    "> A semantic search engine powered by a VectorDB understands this is about refund policies, even if that phrase isn’t mentioned.\n",
    "\n",
    "This leap from syntactic to semantic understanding is only possible with vector-based representations—and thus, vector databases.\n",
    "\n",
    "## **Applications**\n",
    "- **Semantic search engines -** Semantic search goes beyond keyword matching to understand the meaning of a query and return contextually relevant results. Example: Searching for “healthy snacks” returns “organic protein bars” even if the word \"snack\" isn't present.\n",
    "- **Chatbot memory & knowledge retrieval -** Chatbots need to access relevant past conversations or knowledge to generate informed and contextual responses. Example: When a user asks, \"What’s your refund policy?\", the bot can fetch the relevant document snippet even if the question is phrased differently.\n",
    "- **Recommendation systems -** Recommending content or products similar to what the user has interacted with. Example: Spotify uses vector similarity to recommend songs that match your recent listening pattern in mood and genre.\n",
    "- **Content discovery -** Helping users explore new, relevant content that matches their interests or current context. Example: On a news platform, reading an article about climate change might surface other articles about global warming, carbon emissions, etc.\n",
    "- **Document deduplication -** Identifying and removing duplicate or near-duplicate documents in large datasets. Example: Detecting that two job descriptions with slightly different formatting are actually the same.\n",
    "\n",
    "\n",
    "| Application             | Role of VectorDB                                              |\n",
    "| ----------------------- | ------------------------------------------------------------- |\n",
    "| Semantic Search Engines | Finds contextually relevant results beyond keyword matches    |\n",
    "| Chatbot Memory / RAG    | Retrieves most relevant knowledge chunks for better responses |\n",
    "| Recommendation Systems  | Suggests similar items by comparing vectors                   |\n",
    "| Content Discovery       | Enables intuitive exploration based on vector proximity       |\n",
    "| Document Deduplication  | Identifies near-duplicate content using embedding similarity  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06283428-713c-4156-8c5a-9921d2557cbf",
   "metadata": {},
   "source": [
    "## **How are Vectors Stored?**\n",
    "Vectors are stored in specialized data structures optimized for similarity search:\n",
    "- HNSW (Hierarchical Navigable Small World) - graph-based\n",
    "index\n",
    "- IVF (Inverted File) - clustering-based index\n",
    "- Flat - brute force comparison (simple but slow)\n",
    "\n",
    "Metadata is stored separately in a key-value store or document DB.  \n",
    "IDs link the vector index with the metadata store\n",
    "\n",
    "## **What is indexing in VectorDBs?**\n",
    "Indexing is a fundamental technique used to organize vector data for efficient similarity search operations. In the context of vector databases, indexing creates specialized data structures that make nearest neighbor search operations faster and more efficient, especially when dealing with high-dimensional embeddings from text, images, audio, or other data types.\n",
    "\n",
    "## **Two Major Categories of Indexing**\n",
    "1. **Exact Indexing**\n",
    "- Guarantees 100% accurate results\n",
    "- Generally slower but provides perfect recall\n",
    "- Example: Flat (brute-force) index\n",
    "2. **Approximate Indexing (ANN - Approximate Nearest Neighbors)**\n",
    "- Trades a small amount of accuracy for significant speed gains\n",
    "- Uses various techniques to efficiently narrow down the search space\n",
    "- Examples: IVF, HNSW, PQ, ANNOY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b93072-45bf-4b44-94e8-6c1bf0c523bf",
   "metadata": {},
   "source": [
    "## **Flat (brute-force) index**\n",
    "\n",
    "**What is the Flat Index?**\n",
    "> A flat index is the simplest indexing method that performs an exhaustive search by comparing the query vector with every vector in the database.\n",
    "\n",
    "**When to Use Flat Index?**\n",
    "-  Small to medium datasets (typically ≤100K vectors)\n",
    "-  When 100% accuracy is critical (e.g., security applications, financial matching)\n",
    "-  During development and testing to establish baseline performance\n",
    "-  When the dimensionality of vectors is relatively low\n",
    "\n",
    "**How Flat Index Works?**\n",
    "-  No preprocessing or clustering is performed\n",
    "-  All vectors are stored in their original form\n",
    "-  Each query requires direct distance computation with every stored vector\n",
    "-  Typically uses Euclidean (L2) distance or cosine similarity\n",
    "\n",
    "**Disadvantages**\n",
    "- Search time grows linearly with dataset size (O(n) complexity)\n",
    "- Memory intensive as all vectors must be stored in full precision\n",
    "- Becomes impractical for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eedccf3-ec45-4c44-bc7d-c5e04cdb2640",
   "metadata": {},
   "source": [
    "## **Inverted File Index (IVF)**\n",
    "\n",
    "\n",
    "**What is IVF?**\n",
    "IVF organizes vectors into clusters (using algorithms like k-means) and narrows search to only the most relevant clusters, dramatically speeding up search on large datasets.\n",
    "\n",
    "**When to Use IVF?**\n",
    "- Medium to large datasets (1M+ vectors)\n",
    "- When you need a balance between speed and accuracy\n",
    "- When you can accept approximate results (small accuracy trade-off)\n",
    "- When you have a representative sample for training\n",
    "\n",
    "**How IVF Works?**\n",
    "1. Training Phase: Vectors are clustered into nlist clusters (centroids). nlist represents number of clusters.\n",
    "2. Assignment Phase: Each vector is assigned to its nearest centroid\n",
    "3. Search phase:\n",
    "    - Query vector is compared to all centroids\n",
    "    - Only vectors in the nprobe nearest clusters are examined\n",
    "    - nprobe is a tunable parameter that balances speed vs accuracy. nprobe represents clusters to search.\n",
    "\n",
    "**Distance Metrics**\n",
    "- Uses the same distance metrics as flat indexes (typically L2 or cosine)\n",
    "- The choice affects both clustering and search phases\n",
    "\n",
    "**Tuning IVF Parameters**\n",
    "- nlist (number of clusters): Higher values = faster search but lower accuracy\n",
    "    - Rule of thumb: nlist = sqrt(N) where N is dataset size\n",
    "- nprobe (clusters to search): Higher values = higher accuracy but slower search\n",
    "    - Typical values: 1-10% of nlist\n",
    "\n",
    "**Advantages**\n",
    "- Much faster than flat index for large datasets\n",
    "- Scalable to millions of vectors\n",
    "- Adaptable trade-off between speed and accuracy via nprobe\n",
    "\n",
    "**Disadvantages**\n",
    "- Requires a training phase with representative data\n",
    "- Not 100% accurate (may miss some relevant vectors)\n",
    "- Performance depends on data distribution and clustering quality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9bff71-534d-4356-a407-7879d079f765",
   "metadata": {},
   "source": [
    "## **Hierarchical Navigable Small World (HNSW)**\n",
    "\n",
    "**What is HNSW?**\n",
    "Hierarchical Navigable Small World (HNSW) is a graph-based indexing method that creates a multi-layered graph structure for efficient navigation to nearest neighbors.\n",
    "\n",
    "**When to Use HNSW?**\n",
    "- Medium to large datasets where search speed is critical\n",
    "- Real-time applications requiring low latency\n",
    "- When high accuracy is important, but you can't use flat index\n",
    "- When you have sufficient memory to store the graph structure\n",
    "\n",
    "**How HNSW Works?**\n",
    "1. Graph construction: Each vector becomes a node connected to its neighbors\n",
    "2. Hierarchical structure: Multiple layers with varying connection densities\n",
    "3. Search process:\n",
    "    - Start at entry point in the top layer\n",
    "    - Greedily move to closer neighbors\n",
    "    - Descend through layers, refining the search\n",
    "    - Stop at the bottom layer with the best candidates\n",
    "\n",
    "**Distance Metrics in HNSW**\n",
    "- Works with any distance metric (L2, cosine, etc.)\n",
    "- Distance calculations are exact, but search path is approximate\n",
    "- The choice of metric affects both graph construction and search\n",
    "\n",
    "**Tuning HNSW Parameters**\n",
    "- M (connections per node): Higher values = better accuracy but more memory\n",
    "    - Typical values: 16-64\n",
    "- efConstruction (build-time accuracy): Higher values = better index quality but slower build\n",
    "    - Typical values: 100-500\n",
    "- efSearch (search-time accuracy): Higher values = better search accuracy but slower search\n",
    "    - Can be adjusted at query time\n",
    "\n",
    "**Advantages**\n",
    "- Excellent balance of speed and accuracy\n",
    "- No training phase required\n",
    "- Dynamic updates possible (can add vectors after construction)\n",
    "- Works well with filtered queries\n",
    "\n",
    "**Disadvantages**\n",
    "- Memory intensive (stores both vectors and graph links)\n",
    "- Index construction can be slow\n",
    "- More complex implementation than other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a5483-58ae-4d98-9268-588a3dee61c8",
   "metadata": {},
   "source": [
    "## **Major Vector Database Comparison**\n",
    "\n",
    "| Vector Database | Supported Index Types                    | Distance Metrics                    | Auto-Indexing? | Key Features                                                   |\n",
    "|------------------|------------------------------------------|-------------------------------------|----------------|----------------------------------------------------------------|\n",
    "| **FAISS**         | Flat, IVF, PQ, HNSW, SQ, LSH, combinations | L2, Cosine, Dot Product             | No             | Highly customizable, GPU support                              |\n",
    "| **Pinecone**      | Proprietary (HNSW-based)                | Cosine, Dot Product, Euclidean      | Yes            | Fully managed, serverless, auto-scaling                        |\n",
    "| **Weaviate**      | HNSW, Flat                              | Cosine                              | Yes            | Schema-based, multi-modal, GraphQL API                         |\n",
    "| **Qdrant**        | HNSW, Scalar Quantization               | Cosine, Dot Product, Euclidean      | Yes            | Payload storage                                                |\n",
    "| **Milvus**        | IVF, HNSW, PQ, ANNOY, combinations      | Euclidean, IP, Jaccard, others      | Yes            | Hybrid search, cloud/self-hosted                               |\n",
    "| **Chroma**        | HNSW                                    | Cosine, L2, IP                      | Yes            | Simple API, embedding function integration                     |\n",
    "| **Elasticsearch** | HNSW                                    | Cosine, Dot Product, L2             | Yes            | Text+vector search, mature ecosystem                           |\n",
    "| **pgvector**      | IVF, HNSW                               | Cosine, L2, IP                      | Yes            | Postgres extension, familiar SQL interface                     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
